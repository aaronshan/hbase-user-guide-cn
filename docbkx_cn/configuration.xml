<?xml version="1.0"?>
<chapter
  xml:id="configuration"
  version="5.0"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xlink="http://www.w3.org/1999/xlink"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:svg="http://www.w3.org/2000/svg"
  xmlns:m="http://www.w3.org/1998/Math/MathML"
  xmlns:html="http://www.w3.org/1999/xhtml"
  xmlns:db="http://docbook.org/ns/docbook">
  <!--
/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
-->
  <title>Apache HBase配置</title>
  <para>本章扩展自 <xref linkend="getting_started" /> 章节，阐述更多的Apache HBase配置。 请仔细阅读本章节，尤其是 <xref
      linkend="basic.prerequisites" /> 部分，以便确保你的HBase测试和部署能够平滑，还可以防止数据丢失。</para>

  <para> Apache HBase使用与Apache Hadoop相同的配置系统。所有的配置文件位于 <filename>conf/</filename> 目录，你需要同步该配置到你集群的所有节点上。</para>
  
  <variablelist>
    <title>HBase配置文件</title>
    <varlistentry>
      <term><filename>backup-masters</filename></term>
      <listitem>
        <para>不存在默认情况。是一个列出了主机名的纯文本文件。文件里的主机名代表了需要启动的备份(backup)Master的主机，文件的每一行代表一个主机。</para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><filename>hadoop-metrics2-hbase.properties</filename></term>
      <listitem>
        <para>用来连接HBase Hadoop's Metrics2框架。参考 <link
            xlink:href="http://wiki.apache.org/hadoop/HADOOP-6728-MetricsV2">Hadoop Wiki
            入口</link> 查看更多Metrics2的信息。. 默认仅仅包含了一些被注释的例子。</para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><filename>hbase-env.cmd</filename> 和 <filename>hbase-env.sh</filename></term>
      <listitem>
        <para>是用于Windows 和 Linux / Unix 环境的HBase启动脚本。包括Java的位置和选项，以及其他一些环境变量。这个文件包含许多注释掉的指导示例。</para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><filename>hbase-policy.xml</filename></term>
      <listitem>
        <para>RPC服务器使用默认的策略配置来对客户端的请求进行授权。仅仅用于当HBase安全(<xref
                linkend="security" />)被启用的时候。</para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><filename>hbase-site.xml</filename></term>
      <listitem>
        <para>这是HBase主要的配置文件。这个文件指定的配置将会覆盖HBase的默认配置。你可以在<filename>docs/hbase-default.xml</filename>查看（但不能编辑）默认的配置文件，你也可以通过HBase Web UI的<guilabel>HBase Configuration</guilabel>标签来查看你集群所有有效的配置(默认和重载的)。</para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><filename>log4j.properties</filename></term>
      <listitem>
        <para>通过<code>log4j</code>配置HBase的日志输出。</para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><filename>regionservers</filename></term>
      <listitem>
        <para>一个包含了你集群中将要运行RegionServer的所有节点的主机名的纯文本文件。默认该文件仅仅包含了
          <literal>localhost</literal>。它应该包含一个主机名或者IP地址的列表，每一行是一个主机名或者IP地址。如果你的集群中的每一个节点将在<literal>localhost</literal>运行RegionServer，那么该文件应该仅仅包含<literal>localhost</literal>。</para>
      </listitem>
    </varlistentry>
  </variablelist>
  
  <tip>
    <title>检查XML的有效性</title>
    <para>当你编辑XML时，最好的方法是使用一个XML编辑器，它能提醒你你的语法是否正确。你也可以使用 <command>xmllint</command>工具来检查你的XML格式是否正确。默认情况下，<command>xmllint</command> 将打印出标准输出。为了检查XML格式是否正确并且输出错误信息，需要使用命令 <command>xmllint -noout
        <replaceable>filename.xml</replaceable></command>。</para>
  </tip>

  <warning>
    <title>在集群上使用同步操作来使配置一致</title>
    <para>挡在分布式模式下运行HBase时，在你编辑了HBase配置，请确定你已经将<filename>conf/</filename> 目录下的所有文件拷贝到集群上的其他节点上的同样的目录中，HBase默认不会进行这些操作。你可以使用<command>rsync</command>命令、<command>scp</command>命令或者其他的安全拷贝机制将配置文件拷贝到其他节点上。对于大多数配置来说，需要重新启动机器以便配置能够生效。一些异常情况在稍后会进行描述。</para>
  </warning>

  <section
    xml:id="basic.prerequisites">
    <title>基本的先决条件</title>
    <para>本节将列出需要的服务和一些系统配置。 </para>

    <table
      xml:id="java">
      <title>Java</title>
      <textobject>
        <para>HBase至少需要Java 6版本<link
            xlink:href="http://www.java.com/download/">Oracle</link>. 下表列出了JDK的版本和每个HBase版本的兼容性。</para>
      </textobject>
      <tgroup
        cols="4">
        <thead>
          <row>
            <entry>HBase版本</entry>
            <entry>JDK 6</entry>
            <entry>JDK 7</entry>
            <entry>JDK 8</entry>
          </row>
        </thead>
        <tbody>
          <row>
            <entry>1.0</entry>
            <entry><link
                xlink:href="http://search-hadoop.com/m/DHED4Zlz0R1">不支持</link></entry>
            <entry>支持</entry>
            <entry><para>可以运行在JDK 8，但是测试不够充分。</para></entry>
          </row>
          <row>
            <entry>0.98</entry>
            <entry>支持</entry>
            <entry>支持</entry>
            <entry><para>可以运行在JDK 8，但是测试不够充分。使用JDK 8需要移除PoolMap类的过时的remove()方法，目前正在考虑。查看 <link
                  xlink:href="https://issues.apache.org/jira/browse/HBASE-7608">HBASE-7608</link>
                可以查看更多的关于JDK 8支持的信息。</para></entry>
          </row>
          <row>
            <entry>0.96</entry>
            <entry>支持</entry>
            <entry>支持</entry>
            <entry />
          </row>
          <row>
            <entry>0.94</entry>
            <entry>支持</entry>
            <entry>支持</entry>
            <entry />
          </row>
        </tbody>
      </tgroup>
    </table>

    <variablelist
      xml:id="os">
      <title>操作系统的实用程序</title>
      <varlistentry
        xml:id="ssh">
        <term>ssh</term>
        <listitem>
          <para>HBase使用安全的Shell(ssh)命令和工具来进行集群节点间的通信。集群中的每一个服务器必须运行<command>ssh</command>来管理Hadoop和HBase的守护进程。你必须能通过无密码的SSH而不是需要输入密码SSH来从Master和备份(backup)Master连接所有的节点，包括本地节点。你可以参考<xref
                  linkend="passwordless.ssh.quickstart" />来查看如何在Linux或Unix系统来进行设置。如果你的集群节点使用OS X，请查看<link
              xlink:href="http://wiki.apache.org/hadoop/Running_Hadoop_On_OS_X_10.5_64-bit_%28Single-Node_Cluster%29">SSH:
              Setting up Remote Desktop and Enabling Self-Login</link>。</para>
        </listitem>
      </varlistentry>
      <varlistentry
        xml:id="dns">
        <term>DNS</term>
        <listitem>
          <para>HBase使用本地主机名以便自己报告它的IP地址。正向或反向解析必须工作在HBase 0.92.0之前的版本。<footnote>
              <para><link
                  xlink:href="https://github.com/sujee/hadoop-dns-checker">hadoop-dns-checker</link>
                工具能够被用来确定集群上的DNS是否在正确运行。这个工程的README文件提供了详细的使用指导。 </para>
            </footnote></para>

          <para>如果你的服务器已经有多个网络接口，HBase默认会使用主机名来进行解析。为了覆盖这种行为，请设置
              <code>hbase.regionserver.dns.interface</code> 属性为一个不同的接口。此时如果你的集群使用相同的网络接口配置，集群才能正常工作。</para>

          <para>为了使用不同的DNS域名服务器而不是系统默认的域名服务器，需要设定
              <varname>hbase.regionserver.dns.nameserver</varname> 属性为那个域名服务器的IP地址。</para>
        </listitem>
      </varlistentry>
      <varlistentry
        xml:id="loopback.ip">
        <term>环回(Loopback) IP</term>
        <listitem>
          <para>在hbase-0.96.0版本之前，HBase仅仅使用IP地址
              <systemitem>127.0.0.1</systemitem> 来代表 <code>localhost</code>，并且它可能没有被配置。请查看 <xref
              linkend="loopback.ip" />。</para>
        </listitem>
      </varlistentry>
      <varlistentry
        xml:id="ntp">
        <term>NTP</term>
        <listitem>
          <para>集群上的所有节点的时钟必须被同步。少量的差异可以被接受，但是如果差异过大会导致不稳定和异常行为。如果你发现你的集群出现了不明原因的问题，时钟同步的是第一个需要检查的事项。建议你运行一个网络时间协议(NTP)的服务，或者另外一种时钟同步的机制。在集群上，相同的服务在所有节点的时钟应该是同步的。查看<link
              xlink:href="http://www.tldp.org/LDP/sag/html/basic-ntp-config.html">Basic NTP
              Configuration</link> 的 <citetitle>The Linux Documentation Project (TLDP)</citetitle>
            来设置NTP。</para>
        </listitem>
      </varlistentry>

      <varlistentry
        xml:id="ulimit">
        <term>对文件和进程数的限制(<command>ulimit</command>)
          <indexterm>
            <primary>ulimit</primary>
          </indexterm><indexterm>
            <primary>nproc</primary>
          </indexterm>
        </term>

        <listitem>
          <para>Apache HBase是一个数据库。它需要在同一时刻打开大量的文件。许多Linux的发行版限定了单一用户允许打开的最大文件数目为 <literal>1024</literal> (在旧的OS X系统上是 <literal>256</literal> )。
            你可以通过命令<command>ulimit -n</command>检查你的服务器上运行HBase的用户能够打开的最大文件数目。查看 <xref
              linkend="trouble.rs.runtime.filehandles" /> 可以看到你因为这个数目太低而可能遇到的一些问题。你可能会看到类似下面的错误：</para>
          <screen>
2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Exception increateBlockOutputStream java.io.EOFException
2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Abandoning block blk_-6935524980745310745_1391901
          </screen>
          <para>推荐你将ulimit最少设置为10,000，但是设置为10,240会更好些，因为该值通常表示为1024的倍数。 每一个列簇至少都有一个StoreFile文件，并且如果region被加载，可能会有超过6个StoreFiles。打开的文件数目依赖于列簇和region数目。下面是一个RegionServer可能打开的文件数目的粗略的计算公式。</para>
          <example>
            <title>计算可能打开的文件数目</title>
            <screen>(每个列簇的StoreFile数目) x (每个RegionServer的region数目 )</screen>
          </example>
          <para>例如，假设一个模型的每个Region有3个列簇，每个列簇有3个StoreFiles文件，并且每个RegionServer有100个regions。那么，JVM将打开3 * 3 * 100 = 900个文件描述符，这还不包括打开的JAR文件、配置文件以及其他文件的数目。 打开一个文件不会花费很多资源，允许用户打开很多文件的风险是很小的。</para>
          <para>另外一个相关的设置是一个用户允许一次运行的进程数目。 在Linux和Unix中，可以使用<command>ulimit -u</command>命令来进行该数目的设定。这个命令不会与<command>nproc</command>命令混淆。<command>nproc</command>命令用来设定一个用户能使用的CPU的数量。在负载情况下，一个<varname>nproc</varname>太低可能会引起OutOfMemoryError异常。请查阅从2011年开始的hbase-users邮件列表，找出Jack Levin写的 <link
              xlink:href="http://thread.gmane.org/gmane.comp.java.hadoop.hbase.user/16374">major
              hdfs issues</link> 。</para>
          <para>配置一个运行HBase的用户的ILE描述符和进程的fmaximum数目是操作系统的配置，而不是HBase的配置。同样重要的是得确定修改是针对运行HBase的用户。为了查看启动HBase的用户和该用户的ulimit配置，请查看HBase日志的第一行信息。<footnote>
              <para>一个在你的hadoop集群上很有用的读设置配置请看Aaron Kimballs的 <link
                  xlink:href="http://www.cloudera.com/blog/2009/03/configuration-parameters-what-can-you-just-ignore/">Configuration
                  Parameters: What can you just ignore?</link>文章。</para>
            </footnote></para>
          <formalpara xml:id="ulimit_ubuntu">
            <title>Ubuntu上设定<command>ulimit</command></title>
            <para>为了在Ubuntu上配置 <command>ulimit</command>，需要编辑
                <filename>/etc/security/limits.conf</filename>文件, 这是一个使用空格分割的有4列的文件。参考 <link
                xlink:href="http://manpages.ubuntu.com/manpages/lucid/man5/limits.conf.5.html">man
                page for limits.conf</link> 查看关于该文件的格式信息。 在下面的例子中，第一行设定了用户hadoop打开文件的软硬限定的数目(<literal>nofile</literal>)为<literal>32768</literal>。第二行设定了该用户打开的进程数目为32000。</para>
          </formalpara>
          <screen>
hadoop  -       nofile  32768
hadoop  -       nproc   32000
          </screen>
          <para>如果插入式验证模块(PAM)环境直接使用它们，这个设置会被应用。为了配置PAM来使用这些限制，请确保 <filename>/etc/pam.d/common-session</filename> 文件包含下面一行：</para>
          <screen>session required  pam_limits.so</screen>
        </listitem>
      </varlistentry>

      <varlistentry
        xml:id="windows">
        <term>Windows</term>

        <listitem>
          <para>在HBase 0.96和之前的版本，在微软的Windows系统上测试运行HBase是有限制的。在生产环境下，不推荐在Windows上运行HBase。</para>

        <para>为了在微软的Windows系统上运行HBase 0.96之前的版本，你需要安装 <link
            xlink:href="http://cygwin.com/">Cygwin</link> 并且在Cygwin环境中运行HBase。Cygwin提供了对Linux/Unix命令和脚本的完整支持。详细的细节说明请看 <link
            xlink:href="http://hbase.apache.org/cygwin.html">Windows安装</link> 指南和 <link
            xlink:href="http://search-hadoop.com/?q=hbase+windows&amp;fc_project=HBase&amp;fc_type=mail+_hash_+dev">查找用户邮件列表</link> 以便查看针对Windows用户的一些修复工作。</para>
        <para>在hbase-0.96.0之后的版本，, 通过使用hbase提供的
            <command>*.cmd</command> 脚本，可以在windows直接运行HBase。 </para></listitem>
      </varlistentry>

    </variablelist>
    <!--  OS -->

    <section
      xml:id="hadoop">
      <title><link
          xlink:href="http://hadoop.apache.org">Hadoop</link><indexterm>
          <primary>Hadoop</primary>
        </indexterm></title>
        <para>下面的表总结了对于每一个版本的HBase，Hadoop版本的支持情况。基于HBase的版本，你应该选择最合适的Hadoop版本。你可以选择使用Apache Hadoop，或者其他供应商的Hadoop的发行版。查阅 <link
                xlink:href="http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support" />
            可以得到更多的关于Hadoop的供应商的信息。</para>
        <tip>
            <title>推荐使用Hadoop 2.x</title>
            <para>Hadoop 2.x运行的更快并且包含了更多特性。例如短路(short-circuit)读取，这将有助于提高HBase的随机读取能力。Hadoop 2.x修复了一些重要的bug，这将从整体上提高HBase的体验。HBase 0.98已经弃用Hadoop 1.x，并且HBase 1.0将不再支持Hadoop 1.x。</para>
        </tip>
        <para>使用下面的图例来解释这个表格：</para>
        <simplelist
                type="vert"
                columns="1">
            <member>S = 支持并且测试充分，</member>
            <member>X = 不支持，</member>
            <member>NT = 可以运行，但测试不够充分。</member>
        </simplelist>

        <table>
            <title>Hadoop版本支持矩阵</title>
            <tgroup
                    cols="6"
                    align="left"
                    colsep="1"
                    rowsep="1">
                <colspec
                        colname="c1"
                        align="left" />
                <colspec
                        colname="c2"
                        align="center" />
                <colspec
                        colname="c3"
                        align="center" />
                <colspec
                        colname="c4"
                        align="center" />
                <colspec
                        colname="c5"
                        align="center" />
                <colspec
                        colname="c6"
                        align="center" />
                <thead>
                    <row>
                        <entry> </entry>
                        <entry>HBase-0.92.x</entry>
                        <entry>HBase-0.94.x</entry>
                        <entry>HBase-0.96.x</entry>
                        <entry>HBase-0.98.x<footnote>
                            <para>已经弃用对Hadoop 1.x 的支持。</para>
                        </footnote></entry>
                        <entry>HBase-1.0.x<footnote>
                            <para>不再支持Hadoop 1.x。</para>
                        </footnote></entry>
                    </row>
                </thead>
                <tbody>
                    <row>
                        <entry>Hadoop-0.20.205</entry>
                        <entry>S</entry>
                        <entry>X</entry>
                        <entry>X</entry>
                        <entry>X</entry>
                        <entry>X</entry>
                    </row>
                    <row>
                        <entry>Hadoop-0.22.x </entry>
                        <entry>S</entry>
                        <entry>X</entry>
                        <entry>X</entry>
                        <entry>X</entry>
                        <entry>X</entry>
                    </row>
                    <row>
                        <entry>Hadoop-1.0.0-1.0.2<footnote>
                            <para>HBase至少需要hadoop 1.0.3版本，这里存在一个问题，即我们找不到编译早期Hadoop的KerberosUtil。</para>
                        </footnote>
                        </entry>
                        <entry>X</entry>
                        <entry>X</entry>
                        <entry>X</entry>
                        <entry>X</entry>
                        <entry>X</entry>
                    </row>
                    <row>
                        <entry>Hadoop-1.0.3+</entry>
                        <entry>S</entry>
                        <entry>S</entry>
                        <entry>S</entry>
                        <entry>X</entry>
                        <entry>X</entry>
                    </row>
                    <row>
                        <entry>Hadoop-1.1.x </entry>
                        <entry>NT</entry>
                        <entry>S</entry>
                        <entry>S</entry>
                        <entry>X</entry>
                        <entry>X</entry>
                    </row>
                    <row>
                        <entry>Hadoop-0.23.x </entry>
                        <entry>X</entry>
                        <entry>S</entry>
                        <entry>NT</entry>
                        <entry>X</entry>
                        <entry>X</entry>
                    </row>
                    <row>
                        <entry>Hadoop-2.0.x-alpha </entry>
                        <entry>X</entry>
                        <entry>NT</entry>
                        <entry>X</entry>
                        <entry>X</entry>
                        <entry>X</entry>
                    </row>
                    <row>
                        <entry>Hadoop-2.1.0-beta </entry>
                        <entry>X</entry>
                        <entry>NT</entry>
                        <entry>S</entry>
                        <entry>X</entry>
                        <entry>X</entry>
                    </row>
                    <row>
                        <entry>Hadoop-2.2.0 </entry>
                        <entry>X</entry>
                        <entry>NT <footnote>
                            <para>为了使0.94.x运行在hadoop 2.2.0上，你需要改变<filename>pom.xml</filename>文件中hadoop 2和protobuf的版本，这里展示了关于这两个pom.xml文件的不同： </para>
                            <programlisting><![CDATA[$ svn diff pom.xml
Index: pom.xml
===================================================================
--- pom.xml     (revision 1545157)
+++ pom.xml     (working copy)
@@ -1034,7 +1034,7 @@
     <slf4j.version>1.4.3</slf4j.version>
     <log4j.version>1.2.16</log4j.version>
     <mockito-all.version>1.8.5</mockito-all.version>
-    <protobuf.version>2.4.0a</protobuf.version>
+    <protobuf.version>2.5.0</protobuf.version>
     <stax-api.version>1.0.1</stax-api.version>
     <thrift.version>0.8.0</thrift.version>
     <zookeeper.version>3.4.5</zookeeper.version>
@@ -2241,7 +2241,7 @@
         </property>
       </activation>
       <properties>
-        <hadoop.version>2.0.0-alpha</hadoop.version>
+        <hadoop.version>2.2.0</hadoop.version>
         <slf4j.version>1.6.1</slf4j.version>
       </properties>
       <dependencies>]]>
                            </programlisting>
                            <para>假定Protobuf已经被安装，接下来的这一步将生成Protobuf文件：</para>
                            <itemizedlist>
                                <listitem>
                                    <para>进入hbase的根目录，使用如下命令行；</para>
                                </listitem>
                                <listitem>
                                    <para>输入如下命令：</para>
                                    <para>
                                        <programlisting language="bourne"><![CDATA[$ protoc -Isrc/main/protobuf --java_out=src/main/java src/main/protobuf/hbase.proto]]></programlisting>
                                    </para>
                                    <para>
                                        <programlisting language="bourne"><![CDATA[$ protoc -Isrc/main/protobuf --java_out=src/main/java src/main/protobuf/ErrorHandling.proto]]></programlisting>
                                    </para>
                                </listitem>
                            </itemizedlist>
                            <para> 通过运行如下命令基于hadoop 2的profile来构建HBase： </para>
                            <screen language="bourne">$  mvn clean install assembly:single -Dhadoop.profile=2.0 -DskipTests</screen>
                        </footnote></entry>
                        <entry>S</entry>
                        <entry>S</entry>
                        <entry>NT</entry>
                    </row>
                    <row>
                        <entry>Hadoop-2.3.x</entry>
                        <entry>X</entry>
                        <entry>NT</entry>
                        <entry>S</entry>
                        <entry>S</entry>
                        <entry>NT</entry>
                    </row>
                    <row>
                        <entry>Hadoop-2.4.x</entry>
                        <entry>X</entry>
                        <entry>NT</entry>
                        <entry>S</entry>
                        <entry>S</entry>
                        <entry>S</entry>
                    </row>
                    <row>
                        <entry>Hadoop-2.5.x</entry>
                        <entry>X</entry>
                        <entry>NT</entry>
                        <entry>S</entry>
                        <entry>S</entry>
                        <entry>S</entry>
                    </row>

                </tbody>
            </tgroup>
        </table>

        <note
                xml:id="replace.hadoop">
            <title>替换与HBase捆绑的Hadoop</title>
            <para> 因为HBase依赖于Hadoop，它绑定了一个在<filename>lib</filename>目录下的Hadoop jar的实例。捆绑的jar文件仅仅只能在单机模式下使用。在分布式模式下，<emphasis>最重要</emphasis>的是你的HBase集群外部使用的Hadoop版本。
                将HBase的lib目录下的hadoop的jar文件替换为你在集群上正在使用的Hadoop的对应版本，这样可以避免出现一些不匹配的问题。
                确定你替换了你HBase集群中所有的jar文件。版本不匹配的问题有不同的表现，但是它都会让集群看起来像是挂了。
            </para>
        </note>
        <section
                xml:id="hadoop.hbase-0.94">
            <title>Apache HBase 0.92和0.94</title>
            <para>HBase 0.92和0.94版本能工作在以下Hadoop版本：0.20.205、 0.22.x、 1.0.x和
                1.1.x。HBase-0.94还能工作在Hadoop-0.23.x和2.x上，但是你可能需要使用特定的maven profile重新编译代码。(查阅顶层的pom.xml)</para>
        </section>

        <section
                xml:id="hadoop.hbase-0.96">
            <title>Apache HBase 0.96</title>
            <para> 对于Apache HBase 0.96.x，至少需要Apache Hadoop 1.0.x。强烈推荐使用Hadoop 2(更快并且修复了很多bug)。
                HBase不适合运行在诸如0.20.205或者branch-0.20-append或者更早的Hadoop版本上。如果你不升级你的Hadoop，不要迁移到HBase 0.96.x。<footnote>
                    <para>查阅 <link
                            xlink:href="http://search-hadoop.com/m/7vFVx4EsUb2">HBase, mail # dev - DISCUSS:
                        Have hbase require at least hadoop 1.0.0 in hbase 0.96.0?</link></para>
                </footnote></para>
        </section>

        <section
                xml:id="hadoop.older.versions">
            <title>Hadoop 0.20.x - 1.x版本</title>
            <para> 除非HBase运行在实现了<code>sync</code>的HDFS上，否则它很有可能会丢失数据。
                不要使用Hadoop 0.20.2，Hadoop 0.20.203.0和Hadoop 0.20.204.0 ，它们没有这个特性。目前只有
                0.20.205.x 或者之后的Hadoop发行版本 -- 包括hadoop-1.0.0 -- 有一个工作且耐用的同步。 <footnote>
                    <para>Cloudera公司的Charles Zedlweski的博文 <link
                            xlink:href="http://www.cloudera.com/blog/2012/01/an-update-on-apache-hadoop-1-0/">An
                        update on Apache Hadoop 1.0</link> 有一个关于所有Hadoop版本相关的论述。如果你觉得你陷入了关于Hadoop版本的泥沼，那么该文就值得一读。</para>
          </footnote>. 同步需要显式的在客户端的<filename>hbase-site.xml</filename>文件和服务器端的<filename>hdfs-site.xml</filename>文件中设置<varname>dfs.support.append</varname>属性的值为true(HBase需要的同步功能在附件代码路径中)。</para>
        <programlisting language="xml"><![CDATA[  
<property>
  <name>dfs.support.append</name>
  <value>true</value>
</property>]]></programlisting>
        <para> 在你更改了配置后，你需要重新启动集群。 Ignore the
          chicken-little comment you'll find in the <filename>hdfs-default.xml</filename> in the
          description for the <varname>dfs.support.append</varname> configuration. </para>
      </section>
      <section
        xml:id="hadoop.security">
        <title>运行在安全Hadoop上的Apache HBase</title>
        <para>Apache HBase 可以运行在任何集成了Hadoop安全特性的0.20.x版本的Hadoop上，只需要你按照之前的建议将HBase中的对应的Hadoop jar文件使用安全版本的jar文件进行替换即可。如果你想要阅读更多的关于如何设置安全HBase的内容，请查看 <xref
            linkend="hbase.secure.configuration" />部分。</para>
      </section>

      <section
        xml:id="dfs.datanode.max.transfer.threads">
        <title><varname>dfs.datanode.max.transfer.threads</varname><indexterm>
            <primary>dfs.datanode.max.transfer.threads</primary>
          </indexterm></title>

        <para>HDFS的datanode对于同一时刻服务的文件数目存在一个上限。在进行任何加载操作之前，请确定你已经配置了Hadoop的 <filename>conf/hdfs-site.xml</filename>文件，将属性
          <varname>dfs.datanode.max.transfer.threads</varname> 的值至少设置为了下面示例中的值：
        </para>
        <programlisting language="xml"><![CDATA[
<property>
  <name>dfs.datanode.max.transfer.threads</name>
  <value>4096</value>
</property>
      ]]></programlisting>

        <para>确认之后重启你的HDFS以使上述配置生效。</para>

        <para>没有做上述设置会引起一些看起来很奇怪的错误。一个表现是会显示丢失了某些块(block)。例如：</para>
        <screen>10/12/08 20:10:31 INFO hdfs.DFSClient: Could not obtain block
          blk_XXXXXXXXXXXXXXXXXXXXXX_YYYYYYYY from any node: java.io.IOException: No live nodes
          contain current block. Will get new block locations from namenode and retry...</screen>
        <para>可以参考 <xref linkend="casestudies.max.transfer.threads" /> 一节并且要注意到这个属性之前叫做 <varname>dfs.datanode.max.xcievers</varname> (e.g.
          <link
            xlink:href="http://ccgtech.blogspot.com/2010/02/hadoop-hdfs-deceived-by-xciever.html">
            Hadoop HDFS: Deceived by Xciever</link>).
        </para>


      </section>
    </section>
    <!--  hadoop -->
  </section>

  <section
    xml:id="standalone_dist">
    <title>HBase的运行模式: 单机和分布式</title>

    <para>HBase 有两种运行模式: <xref
        linkend="standalone" /> 和 <xref
        linkend="distributed" />。 HBase可以直接在单机模式下运行。无论你选择了哪种模式，你都需要对HBase <filename>conf</filename>目录下的文件进行修改。起码你需要修改<code>conf/hbase-env.sh</code>以告知HBase将要用的<command>java</command>命令的目录。在这个文件中你可以设置一些HBase的环境变量，比如堆大小和其他一些JVM的选项，以及你选择的存储日志文件的位置。将<varname>JAVA_HOME</varname>设置为你安装的<command>java</command>的根路径。</para>

    <section
      xml:id="standalone">
      <title>单机HBase</title>

      <para>这是默认的模式。单机模式在 <xref
          linkend="quickstart" /> 一节中进行了描述。在单机模式中，HBase使用本地文件系统取代了HDFS。它在同一个JVM中运行所有的HBase守护进程和一个本地ZooKeeper。Zookeeper也使用了默认的端口与HBase进行通信。</para>
    </section>

    <section
      xml:id="distributed">
      <title>分布式</title>

      <para>分布式模式可以分为所有守护进程运行在单个节点上的<emphasis>伪分布式</emphasis>和守护进程分布在集群上所有节点的<emphasis>完全分布式</emphasis>。<footnote>
          <para>伪分布式和完全分布式的命名源自于Hadoop。</para>
        </footnote>.</para>

      <para>伪分布式模式可以运行在本地文件系统中或<emphasis>Hadoop分布式文件系统</emphasis> (Hadoop Distributed File System, HDFS)的一个实例中。完全分布式模式只能运行在HDFS上。
        查看Hadoop <link
          xlink:href="http://hadoop.apache.org/common/docs/r1.1.1/api/overview-summary.html#overview_description">
          requirements and instructions</link> 文献以便学习如何在Hadoop 1.x上设置HDFS。这里有一个关于如何在Hadoop 2上设置HDFS的指南： <link
          xlink:href="http://www.alexjf.net/blog/distributed-systems/hadoop-yarn-installation-definitive-guide">http://www.alexjf.net/blog/distributed-systems/hadoop-yarn-installation-definitive-guide</link>。</para>

      <para>下面我们将介绍不同的分布式设置。首先，确认和验证你安装的是<xref
              linkend="confirm" />中论述的<emphasis>伪分布式</emphasis>还是<emphasis>完全分布式</emphasis>。这两种类型的校验脚本是一样的。</para>
      <section
        xml:id="pseudo">
        <title>伪分布式</title>
        <note>
          <title>伪分布式快速入门</title>
          <para>快速入门已经在 <xref
              linkend="quickstart" /> 中有所论述。 请查看 <xref
              linkend="quickstart-pseudo" />，一些原本在本节论述的内容本移到了那里。</para>
        </note>

        <para>伪分布式模式就是在单机上运行一个完全分布式模式。使用这样的配置可以测试和体验HBase。但是不要用这样的配置来用于生产环境，或者评估HBase的性能。</para>

      </section>

    </section>

    <section
      xml:id="fully_dist">
      <title>完全分布式</title>
      <para>默认情况下，HBase运行在单机模式。单机模式和伪分布式模式都用来提供一个小规模的测试。分布式模式更适合生产环境。在分布式模式下，多个HBase守护进程的实例运行在集群的多个服务器上。</para>
      <para>和伪分布式模式一样，完全分布式需要你将配置中的<code>hbase-cluster.distributed</code>属性值设置为 <literal>true</literal>。通常，<code>hbase.rootdir</code>属性应该被设置指向HDFS系统。 </para>
      <para>另外，集群的节点分为了
        RegionServers、ZooKeeper QuorumPeers和backup HMaster servers。这些配置基本都在 <xref
          linkend="quickstart-fully-distributed" />。</para>

      <formalpara
        xml:id="regionserver">
        <title>分布式的RegionServers</title>
        <para>通常，你的集群包含运行在所有不同服务器上的多个RegionServer，也包括主Master和备份Master和Zookeeper守护进程。主服务器的<filename>conf/regionservers</filename>文件包括了一个该集群被用作RegionServer的主机列表。文件的每一行代表一个主机，当主服务器启动或停止的时候，文件中的所有主机的RegionServer进程也会跟着启动或停止。</para>
      </formalpara>

      <formalpara
        xml:id="hbase.zookeeper">
        <title>ZooKeeper和HBase</title>
        <para>查看 <xref
            linkend="zookeeper" /> 来设置用于HBase的ZooKeeper。</para>
      </formalpara>

      <example>
        <title>一个分布式的HBase集群实例</title>
        <para>这是一个最基本的分布式HBase集群的 <filename>conf/hbase-site.xml</filename>文件。实际工作环境中的集群将包含更多的配置参数。除非你在<filename>hbase-site.xml</filename>文件中重新指定，否则大多数HBase的配置项都使用一个默认值。查看 <xref
            linkend="config.files" /> 可以找到更多信息。</para>
        <programlisting language="xml"><![CDATA[
<configuration>
  <property>
    <name>hbase.rootdir</name>
    <value>hdfs://namenode.example.org:8020/hbase</value>
  </property>
  <property>
    <name>hbase.cluster.distributed</name>
    <value>true</value>
  </property>
  <property>
      <name>hbase.zookeeper.quorum</name>
      <value>node-a.example.com,node-b.example.com,node-c.example.com</value>
    </property>
</configuration>
]]>
        </programlisting>
        <para>这是一个 <filename>conf/regionservers</filename> 文件的例子，它包含了集群中用来运行RegionServer的每一个节点。这些节点也需要安装HBase，并且使用与主服务器的<filename>conf/</filename>目录中相同的配置。</para>
        <programlisting>
node-a.example.com
node-b.example.com
node-c.example.com
        </programlisting>
        <para>这是一个 <filename>conf/backup-masters</filename> 文件的例子, 它包含了用来运行备份Master实例的节点。如果主Master不可用，备份Master会接管主Master的职能，否则它会一直处于闲置状态。</para>
        <programlisting>
node-b.example.com
node-c.example.com
        </programlisting>
      </example>
      <formalpara>
        <title>分布式HBase快速入门</title>
        <para>请查看 <xref
            linkend="quickstart-fully-distributed" /> ，它是一个配置了多个ZooKeeper、备份HMaster和RegionServer实例的具有三个节点的简单集群。</para>
      </formalpara>

      <procedure
        xml:id="hdfs_client_conf">
        <title>HDFS客户端配置</title>
        <step>
          <para>值得注意的是，如果你已经在你的Hadoop集群中进行了HDFS客户端的配置，，而不是服务器端，例如配置了HDFS的路径。那么你必须使用如下方式使得HBase能查看并使用这些配置：</para>
          <stepalternatives>
            <step>
              <para>
                  在<filename>hbase-env.sh</filename>文件中给<varname>HBASE_CLASSPATH</varname>环境变量增加一个指向<varname>HADOOP_CONF_DIR</varname>的路径。</para>
            </step>

            <step>
              <para>将 <filename>hdfs-site.xml</filename>文件拷贝到<filename>${HBASE_HOME}/conf</filename>。或者增加一个指向<filename>${HBASE_HOME}/conf</filename>的链接，或者</para>
            </step>

            <step>
              <para>如果HDFS客户端的配置较少，那么直接让这些配置增加到
                  <filename>hbase-site.xml</filename>文件中。</para>
            </step>
          </stepalternatives>
        </step>
      </procedure>
      <para>一个最简单的HDFS客户端配置是<varname>dfs.replication</varname>。如果你想在运行时使用复制因子为5的配置，那么你必须在HBase中进行相应配置。否则，HBase将使用的默认为3的复制因子。</para>
    </section>
  </section>

    <section
      xml:id="confirm">
      <title>运行且确认你的安装</title>
      <para>首先确认HDFS已经在运行。通过运行<varname>HADOOP_HOME</varname>目录中的<filename>bin/start-hdfs.sh</filename>来启动或停止Hadoop HDFS守护进程。为了确定是否启动成功，你可以运行<command>put</command>和<command>get</command>命令来操作Hadoop文件系统中的文件。HBase通常不使用mapreduce守护进程。它们不需要被启动。</para>
      <para><emphasis>如果</emphasis> 你正在管理你自己的ZooKeeper，启动它并且确认它正在正常运行，否则HBase将作为启动过程的一部分来启动它。</para>
      <para>使用如下命令可以启动HBase</para>
      <screen>bin/start-hbase.sh</screen>
      <para>该命令在 <varname>HBASE_HOME</varname> 目录下。</para>
      <para>你现在应该有了一个正在运行的HBase实例。你可以在<filename>logs</filename> 子目录中看到HBase的日志文件。如果HBase的启动出现问题，请检查它们。</para>

      <para>HBase有一个界面列出了重要的属性。默认在部署在Master主机上的16010端口 (HBase RegionServer 默认监听16020端口并且将信息输出到16030端口)。 如果运行Master的主机名是
          <varname>master.example.org</varname> 并且使用默认端口，那么可以在你的浏览器中进入<filename>http://master.example.org:16010</filename>来查看Master的主页。</para>

      <para>在HBase 0.98之前，默认的master界面的端口是16010，并且HBase RegionServer监听16020端口，将信息输出到16030端口。</para>

      <para>一旦HBase成功启动，可以查看 <xref
          linkend="shell_exercises" /> 来学习如何创建表、增加数据、遍历你插入的数据以及最终禁用表和删除表。</para>

      <para>为了在退出HBase shell之后停止HBase，可以使用如下命令：</para>
      <screen language="bourne">$ ./bin/stop-hbase.sh
stopping hbase...............</screen>
      <para>关闭操作将花费一段时间。如果你的集群包含了多台机器，那么它花费的时间可能会更长。如果你正在运行分布式操作，请等到HBase完全关闭之后再关闭Hadoop守护进程。</para>
    </section>

  <!--  run modes -->



  <section
    xml:id="config.files">
    <title>配置文件</title>

    <section
      xml:id="hbase.site">
      <title><filename>hbase-site.xml</filename> 和 <filename>hbase-default.xml</filename></title>
      <para>正如你在Hadoop中需要在<filename>hdfs-site.xml</filename>文件中指定HDFS的地址，在HBase中，也需要在<filename>conf/hbase-site.xml</filename>文件中指定HDFS的地址。配置属性的列表，请查看 <xref
          linkend="hbase_default_configurations" /> 或者阅读HBase源代码中的<filename>src/main/resources</filename>中原始的
          <filename>hbase-default.xml</filename> 源文件。 </para>
      <para> 不是所有的配置项都在 <filename>hbase-default.xml</filename>文件中。
        有一些配置仅仅存在于源代码中。调整这些配置的唯一方式就是阅读源代码并且修改源代码。</para>
      <para> 目前，所有对配置做的修改都需要重启集群以使配置生效。 </para>
      <!--The file hbase-default.xml is generated as part of
    the build of the hbase site.  See the hbase pom.xml.
    The generated file is a docbook section with a glossary
    in it-->
      <!--presumes the pre-site target has put the hbase-default.xml at this location-->
      <xi:include
        xmlns:xi="http://www.w3.org/2001/XInclude"
        href="../../target/docbkx/hbase-default.xml">
        <xi:fallback>
          <section
            xml:id="hbase_default_configurations">
            <title >HBase默认配置</title>
            <para>
              <emphasis>这个文件是错误提示</emphasis>。如果看到这个信息，那么证明你在构建HBase文档的过程中出错。 </para>
            <para> hbase-default.xml是构建hbase站点之后的生成结果的一部分。
              查阅 <filename>pom.xml</filename>。这是使用docbook词汇产生的文件。 </para>
            <section>
              <title>IDs自动生成，如果不存在，则会出现校验错误</title>
              <para> Each of these is a reference to a configuration file parameter which will cause
                an error if you are using the fallback content here. This is a dirty dirty hack. </para>
              <section
                xml:id="fail.fast.expired.active.master">
                <title>fail.fast.expired.active.master</title>
                <para />
              </section>
              <section
                xml:id="hbase.hregion.memstore.flush.size">
                <title>"hbase.hregion.memstore.flush.size"</title>
                <para />
              </section>
              <section
                xml:id="hbase.hstore.bytes.per.checksum">
                <title>hbase.hstore.bytes.per.checksum</title>
                <para />
              </section>
              <section
                xml:id="hbase.online.schema.update.enable">
                <title>hbase.online.schema.update.enable</title>
                <para />
              </section>
              <section
                xml:id="hbase.regionserver.global.memstore.size">
                <title>hbase.regionserver.global.memstore.size</title>
                <para />
              </section>
              <section
                xml:id="hbase.hregion.max.filesize">
                <title>hbase.hregion.max.filesize</title>
                <para />
              </section>
              <section
                xml:id="hbase.hstore.blockingStoreFiles">
                <title>hbase.hstore.BlockingStoreFiles</title>
                <para />
              </section>
              <section
                xml:id="hfile.block.cache.size">
                <title>hfile.block.cache.size</title>
                <para />
              </section>
              <section
                xml:id="copy.table">
                <title>copy.table</title>
                <para />
              </section>
              <section
                xml:id="hbase.hstore.checksum.algorithm">
                <title>hbase.hstore.checksum.algorithm</title>
                <para />
              </section>
              <section
                xml:id="hbase.zookeeper.useMulti">
                <title>hbase.zookeeper.useMulti</title>
                <para />
              </section>
              <section
                xml:id="hbase.hregion.memstore.block.multiplier">
                <title>hbase.hregion.memstore.block.multiplier</title>
                <para />
              </section>
              <section
                xml:id="hbase.regionserver.global.memstore.size.lower.limit">
                <title>hbase.regionserver.global.memstore.size.lower.limit</title>
                <para />
              </section>
            </section>
          </section>
        </xi:fallback>
      </xi:include>
    </section>

    <section
      xml:id="hbase.env.sh">
      <title><filename>hbase-env.sh</filename></title>
      <para>HBase环境变量在该文件中进行设置。例如在启动HBase守护进程时需要的JVM信息、堆大小、垃圾收集，pid文件的位置等。打开文件<filename>conf/hbase-env.sh</filename>并且仔细查看其中的内容。每一个配置项都是有据可查的。如果你想让HBase守护进程在启动时使用某些环境变量，那么就把它们添加到这儿。</para>
      <para> 对该文件所处的改变需要重启集群才能生效。 </para>
    </section>

    <section
      xml:id="log4j">
      <title><filename>log4j.properties</filename></title>
      <para>编辑该文件可以改变HBase日志信息的级别和其他日志记录的相关信息。 </para>
      <para>对该文件做出的改变需要重启集群才能生效，尽管某些守护进程可以通过HBase用户界面来改变日志记录的级别。 </para>
    </section>

    <section
      xml:id="client_dependencies">
      <title>客户端的配置和连接HBase集群的依赖项</title>
      <para>如果你在单机模式下运行HBase，你不需要对你的客户端做任何配置，只要它们都在同一台机器上。</para>
      <para>由于HBase的Master可能变化，所以客户端需要通过Zookeeper来定位HBase Master的位置。Zookeeper中保存着这些值。因此客户端需要在获取信息前需要指导Zookeeper的位置。通常该信息保存在<filename>hbase-site.xml</filename> 文件中，并且被客户端从<varname>CLASSPATH</varname>中获取。</para>
      <para>如果你正在配置一个IDE用来运行HBase客户端，你应该将<filename>conf/</filename> 目录放到你的classpath中，以便 so
          <filename>hbase-site.xml</filename> 设置能被找到 (或者在测试时添加hbase-site.xml到
          <filename>src/test/resources</filename> )。 </para>
      <para> 最起码，一个HBase客户端在连接集群时，需要在它
          <varname>CLASSPATH</varname> 需要几个类库，包括：
        <programlisting>
commons-configuration (commons-configuration-1.6.jar)
commons-lang (commons-lang-2.5.jar)
commons-logging (commons-logging-1.1.1.jar)
hadoop-core (hadoop-core-1.0.0.jar)
hbase (hbase-0.92.0.jar)
log4j (log4j-1.2.16.jar)
slf4j-api (slf4j-api-1.5.8.jar)
slf4j-log4j (slf4j-log4j12-1.5.8.jar)
zookeeper (zookeeper-3.4.2.jar)</programlisting>
      </para>
      <para> 一个基本的客户端的 <filename>hbase-site.xml</filename> 文件示例看起来是下面这个样子： <programlisting language="xml"><![CDATA[
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>example1,example2,example3</value>
    <description>The directory shared by region servers.
    </description>
  </property>
</configuration>
]]></programlisting>
      </para>

      <section
        xml:id="java.client.config">
        <title>Java客户端配置</title>
        <para>Java客户端的配置被保存在一个<link
            xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HBaseConfiguration">HBaseConfiguration</link>
          类实例中。 HBaseConfiguration的工厂方法，
            <code>HBaseConfiguration.create();</code>, 在调用时，将会读取客户端最初在<varname>CLASSPATH</varname>中找到的<filename>hbase-site.xml</filename> ，如果存在 (调用时也会寻找名为
            <filename>hbase-default.xml</filename> 的文件;hbase-default.xml文件在
            <filename>hbase.X.X.X.jar</filename>中可以被找到).。也可以直接指定配置，如果没有找到 <filename>hbase-site.xml</filename>文件。例如，设置集群ZooKeeper地址的语句如下：
          <programlisting language="java">Configuration config = HBaseConfiguration.create();
config.set("hbase.zookeeper.quorum", "localhost");  // Here we are running zookeeper locally</programlisting>
          如果ZooKeeper集群有多个ZooKeeper实例，它们应该使用逗号分割 (就像在 <filename>hbase-site.xml</filename> 文件中那样)。这个<classname>Configuration</classname>实例可以被传递给一个 <link
            xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html">HTable</link>，等等。 </para>
      </section>
    </section>

  </section>
  <!--  config files -->

  <section
    xml:id="example_config">
    <title>配置示例</title>

    <section>
      <title>基本的分布式HBase安装</title>

      <para>下面是一个有10个节点的集群的基本配置的例子。节点被命名为 <varname>example0</varname>、 <varname>example1</varname>直到节点
          <varname>example9</varname>。 HBase Master和HDFS namenode运行在节点 <varname>example0</varname>。 RegionServers 运行在节点
          <varname>example1</varname>-<varname>example9</varname>。 ZooKeeper运行在 <varname>example1</varname>, <varname>example2</varname>, and <varname>example3</varname>
        上，使用默认端口。 ZooKeeper的持久化数据被保存在
          <filename>/export/zookeeper</filename>。 下面我们展示主要的配置文件 --
          <filename>hbase-site.xml</filename>、<filename>regionservers</filename>和
          <filename>hbase-env.sh</filename> -- 它们在HBase <filename>conf</filename>
        目录中。</para>

      <section
        xml:id="hbase_site">
        <title><filename>hbase-site.xml</filename></title>

        <programlisting language="bourne">
<![CDATA[
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>example1,example2,example3</value>
    <description>The directory shared by RegionServers.
    </description>
  </property>
  <property>
    <name>hbase.zookeeper.property.dataDir</name>
    <value>/export/zookeeper</value>
    <description>Property from ZooKeeper config zoo.cfg.
    The directory where the snapshot is stored.
    </description>
  </property>
  <property>
    <name>hbase.rootdir</name>
    <value>hdfs://example0:8020/hbase</value>
    <description>The directory shared by RegionServers.
    </description>
  </property>
  <property>
    <name>hbase.cluster.distributed</name>
    <value>true</value>
    <description>The mode the cluster will be in. Possible values are
      false: standalone and pseudo-distributed setups with managed Zookeeper
      true: fully-distributed with unmanaged Zookeeper Quorum (see hbase-env.sh)
    </description>
  </property>
</configuration>
]]>
        </programlisting>
      </section>

      <section
        xml:id="regionservers">
        <title><filename>regionservers</filename></title>

        <para>In this file you list the nodes that will run RegionServers. In our case, these nodes
          are <varname>example1</varname>-<varname>example9</varname>. </para>

        <programlisting>
example1
example2
example3
example4
example5
example6
example7
example8
example9
        </programlisting>
      </section>

      <section
        xml:id="hbase_env">
        <title><filename>hbase-env.sh</filename></title>

        <para>下面我们使用 <command>diff</command> 命令来展示下面的文件和默认的
            <filename>hbase-env.sh</filename> 文件的不同。此处我们将默认的1G的堆大小设置为4G。</para>

        <screen language="bourne">

$ git diff hbase-env.sh
diff --git a/conf/hbase-env.sh b/conf/hbase-env.sh
index e70ebc6..96f8c27 100644
--- a/conf/hbase-env.sh
+++ b/conf/hbase-env.sh
@@ -31,7 +31,7 @@ export JAVA_HOME=/usr/lib//jvm/java-6-sun/
 # export HBASE_CLASSPATH=

 # The maximum amount of heap to use, in MB. Default is 1000.
-# export HBASE_HEAPSIZE=1000
+export HBASE_HEAPSIZE=4096

 # Extra Java runtime options.
 # Below are what we set by default.  May only work with SUN JVM.

        </screen>

        <para>使用 <command>rsync</command> 命令拷贝 <filename>conf</filename> 目录到集群上的其他所有节点。</para>
      </section>
    </section>
  </section>
  <!-- example config -->


  <section
    xml:id="important_configurations">
    <title>重要配置</title>
    <para>下面我们列举一些 <emphasis>重要</emphasis> 配置。 我们将这部分分为需要的配置和推荐的值得一看的配置两部分。 </para>

    <section
      xml:id="required_configuration">
      <title>需要的配置</title>
      <para>阅览 <xref
          linkend="os" /> 和 <xref
          linkend="hadoop" /> 部分。 </para>
      <section
        xml:id="big.cluster.config">
        <title>大集群配置</title>
        <para>如果一个集群有大量的region，那么这种情况是可能的。Master在启动后迅速检查所有的regionserver。然而因为其他regionserver启动滞后，导致所有的region被分配到了第一台regionserver上。如果region数目过多，那么会导致第一台regionserver的负载过高。为了阻止上述情况的发生，将<varname>hbase.master.wait.on.regionservers.mintostart</varname>的默认值1设置为其他值。查看<link
            xlink:href="https://issues.apache.org/jira/browse/HBASE-6389">HBASE-6389 Modify the
            conditions to ensure that Master waits for sufficient number of Region Servers before
            starting region assignments</link> 可以获得更多信息。 </para>
      </section>
      <section
        xml:id="backup.master.fail.fast">
        <title>如果一个备份Master使主Master很快失败</title>
        <para>如果一个主Master与Zookeeper失去了连接，那它将一直循环重试与zookeeper进行连接。如果你有多个Master，比如有一个备份Master，那禁用这个功能。如果在失败的时候还继续这个功能，那尽管另外一个Master已经承担了主Master的职责，但死掉的Master还会继续受到RPC请求。可以参照着 <xref
            linkend="fail.fast.expired.active.master" />进行配置。 </para>
      </section>
    </section>

    <section
      xml:id="recommended_configurations">
      <title>推荐的配置</title>
      <section
        xml:id="recommended_configurations.zk">
        <title>ZooKeeper配置</title>
        <section
          xml:id="sect.zookeeper.session.timeout">
          <title><varname>zookeeper.session.timeout</varname></title>
          <para>默认的时间是3分钟（单位是毫秒）。这意味着如果一个服务器宕机，3分钟后Master才能知道它已经当即并且开始恢复工作。
              你可能想将这个值调整为1分钟或者更短的时间，使得Master尽快知道服务器已经宕机。在改变这个值之前，请确定你的JVM的垃圾收集配置在控制以内，否则大量的垃圾收集将导致你的RegionServer的Zookeeper的session会话超时（你可能会受到惩罚 -- 因为启动一个进行垃圾收集的RegionServer将花费很长时间）。</para>

          <para>为了改变这个配置，编辑 <filename>hbase-site.xml</filename>，拷贝改变后的文件到集群其他节点并且重启集群。</para>

          <para>我们将该值调高以便不需要去邮件列表中询问在大规模导入时RegionServer宕机这种菜鸟问题。通常的原因是，JVM已经停止了运行，它们陷入了长时间的垃圾收集等待中。我们认为随着用户越来越熟悉HBase，他们已经了解了所有的复杂性。之后当他们建立了一定的信心后，就可以像这样玩转配置了。 </para>
        </section>
        <section
          xml:id="zookeeper.instances">
          <title>ZooKeeper实例数目</title>
          <para>查看 <xref
              linkend="zookeeper" />。 </para>
        </section>
      </section>
      <section
        xml:id="recommended.configurations.hdfs">
        <title>HDFS配置</title>
        <section
          xml:id="dfs.datanode.failed.volumes.tolerated">
          <title>dfs.datanode.failed.volumes.tolerated</title>
          <para>从<filename>hdfs-default.xml</filename>的描述中，我们知道这是“datanode停止服务之前允许失败的卷的数目，默认任何卷的失败将导致datanode宕掉”。如果你有大于3个或4个磁盘或者更多个磁盘，你可能想要将这个值设为1。 </para>
        </section>
      </section>
      <section
        xml:id="hbase.regionserver.handler.count-description">
        <title><varname>hbase.regionserver.handler.count</varname></title>
        <para> 这个设置定义了响应用户表的请求所打开的线程数目。一个经验是：当每一个请求的有效负载达到了MB级别(大的puts、使用大缓存量的scan操作)，那么应该将该值调低。
如果有效负载很小（gets、小的puts、ICVs、deletes操作），将该值调高。可以通过设置"hbase.ipc.server.max.callqueue.size"来限定处理队列的总的大小。</para>
        <para>如果新的客户端请求的有效负载都很小，那么将该值设置为这些客户端的总数目是安全的。一个典型的实例是服务站点的hbase集群的put操作没有被缓存并且大多数操作都是gets。 </para>
        <para>
            这就是为什么
            The reason why it is dangerous to keep this setting high is that the aggregate size
          of all the puts that are currently happening in a region server may impose too much
          pressure on its memory, or even trigger an OutOfMemoryError. A region server running on
          low memory will trigger its JVM's garbage collector to run more frequently up to a point
          where GC pauses become noticeable (the reason being that all the memory used to keep all
          the requests' payloads cannot be trashed, no matter how hard the garbage collector tries).
          After some time, the overall cluster throughput is affected since every request that hits
          that region server will take longer, which exacerbates the problem even more. </para>
        <para>You can get a sense of whether you have too little or too many handlers by <xref
            linkend="rpc.logging" /> on an individual RegionServer then tailing its logs (Queued
          requests consume memory). </para>
      </section>
      <section
        xml:id="big_memory">
        <title>Configuration for large memory machines</title>
        <para> HBase ships with a reasonable, conservative configuration that will work on nearly
          all machine types that people might want to test with. If you have larger machines --
          HBase has 8G and larger heap -- you might the following configuration options helpful.
          TODO. </para>

      </section>

      <section
        xml:id="config.compression">
        <title>Compression</title>
        <para>You should consider enabling ColumnFamily compression. There are several options that
          are near-frictionless and in most all cases boost performance by reducing the size of
          StoreFiles and thus reducing I/O. </para>
        <para>See <xref
            linkend="compression" /> for more information.</para>
      </section>
      <section
        xml:id="config.wals">
        <title>Configuring the size and number of WAL files</title>
        <para>HBase uses <xref
            linkend="wal" /> to recover the memstore data that has not been flushed to disk in case
          of an RS failure. These WAL files should be configured to be slightly smaller than HDFS
          block (by default, HDFS block is 64Mb and WAL file is ~60Mb).</para>
        <para>HBase also has a limit on number of WAL files, designed to ensure there's never too
          much data that needs to be replayed during recovery. This limit needs to be set according
          to memstore configuration, so that all the necessary data would fit. It is recommended to
          allocated enough WAL files to store at least that much data (when all memstores are close
          to full). For example, with 16Gb RS heap, default memstore settings (0.4), and default WAL
          file size (~60Mb), 16Gb*0.4/60, the starting point for WAL file count is ~109. However, as
          all memstores are not expected to be full all the time, less WAL files can be
          allocated.</para>
      </section>
      <section
        xml:id="disable.splitting">
        <title>Managed Splitting</title>
        <para>HBase generally handles splitting your regions, based upon the settings in your
            <filename>hbase-default.xml</filename> and <filename>hbase-site.xml</filename>
          configuration files. Important settings include
            <varname>hbase.regionserver.region.split.policy</varname>,
            <varname>hbase.hregion.max.filesize</varname>,
            <varname>hbase.regionserver.regionSplitLimit</varname>. A simplistic view of splitting
          is that when a region grows to <varname>hbase.hregion.max.filesize</varname>, it is split.
          For most use patterns, most of the time, you should use automatic splitting.</para>
        <para>Instead of allowing HBase to split your regions automatically, you can choose to
          manage the splitting yourself. This feature was added in HBase 0.90.0. Manually managing
          splits works if you know your keyspace well, otherwise let HBase figure where to split for you.
          Manual splitting can mitigate region creation and movement under load. It also makes it so
          region boundaries are known and invariant (if you disable region splitting). If you use manual
          splits, it is easier doing staggered, time-based major compactions spread out your network IO
          load.</para>

        <formalpara>
          <title>Disable Automatic Splitting</title>
          <para>To disable automatic splitting, set <varname>hbase.hregion.max.filesize</varname> to
            a very large value, such as <literal>100 GB</literal> It is not recommended to set it to
            its absolute maximum value of <literal>Long.MAX_VALUE</literal>.</para>
        </formalpara>
        <note>
          <title>Automatic Splitting Is Recommended</title>
          <para>If you disable automatic splits to diagnose a problem or during a period of fast
            data growth, it is recommended to re-enable them when your situation becomes more
            stable. The potential benefits of managing region splits yourself are not
            undisputed.</para>
        </note>
        <formalpara>
          <title>Determine the Optimal Number of Pre-Split Regions</title>
          <para>The optimal number of pre-split regions depends on your application and environment.
            A good rule of thumb is to start with 10 pre-split regions per server and watch as data
            grows over time. It is better to err on the side of too few regions and perform rolling
            splits later. The optimal number of regions depends upon the largest StoreFile in your
            region. The size of the largest StoreFile will increase with time if the amount of data
            grows. The goal is for the largest region to be just large enough that the compaction
            selection algorithm only compacts it during a timed major compaction. Otherwise, the
            cluster can be prone to compaction storms where a large number of regions under
            compaction at the same time. It is important to understand that the data growth causes
            compaction storms, and not the manual split decision.</para>
        </formalpara>
        <para>If the regions are split into too many large regions, you can increase the major
          compaction interval by configuring <varname>HConstants.MAJOR_COMPACTION_PERIOD</varname>.
          HBase 0.90 introduced <classname>org.apache.hadoop.hbase.util.RegionSplitter</classname>,
          which provides a network-IO-safe rolling split of all regions.</para>
      </section>
      <section
        xml:id="managed.compactions">
        <title>Managed Compactions</title>
        <para>By default, major compactions are scheduled to run once in a 7-day period. Prior to HBase 0.96.x, major
          compactions were scheduled to happen once per day by default.</para>
        <para>If you need to control exactly when and how often major compaction runs, you can
          disable managed major compactions. See the entry for
            <varname>hbase.hregion.majorcompaction</varname> in the <xref
            linkend="compaction.parameters" /> table for details.</para>
        <warning>
          <title>Do Not Disable Major Compactions</title>
          <para>Major compactions are absolutely necessary for StoreFile clean-up. Do not disable
            them altogether. You can run major compactions manually via the HBase shell or via the <link
              xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html#majorCompact%28java.lang.String%29">HBaseAdmin
              API</link>.</para>
        </warning>        
        <para>For more information about compactions and the compaction file selection process, see <xref
            linkend="compaction" /></para>
      </section>

      <section
        xml:id="spec.ex">
        <title>Speculative Execution</title>
        <para>Speculative Execution of MapReduce tasks is on by default, and for HBase clusters it
          is generally advised to turn off Speculative Execution at a system-level unless you need
          it for a specific case, where it can be configured per-job. Set the properties
            <varname>mapreduce.map.speculative</varname> and
            <varname>mapreduce.reduce.speculative</varname> to false. </para>
      </section>
    </section>
      <section xml:id="other_configuration"><title>Other Configurations</title>
         <section xml:id="balancer_config"><title>Balancer</title>
           <para>The balancer is a periodic operation which is run on the master to redistribute regions on the cluster.  It is configured via
           <varname>hbase.balancer.period</varname> and defaults to 300000 (5 minutes). </para>
           <para>See <xref linkend="master.processes.loadbalancer" /> for more information on the LoadBalancer.
           </para>
         </section>
        <section xml:id="disabling.blockcache"><title>Disabling Blockcache</title>
          <para>Do not turn off block cache (You'd do it by setting <varname>hbase.block.cache.size</varname> to zero).
          Currently we do not do well if you do this because the regionserver will spend all its time loading hfile
          indices over and over again.  If your working set it such that block cache does you no good, at least
          size the block cache such that hfile indices will stay up in the cache (you can get a rough idea
          on the size you need by surveying regionserver UIs; you'll see index block size accounted near the
          top of the webpage).</para>
        </section>
    <section xml:id="nagles">
      <title><link xlink:href="http://en.wikipedia.org/wiki/Nagle's_algorithm">Nagle's</link> or the small package problem</title>
      <para>If a big 40ms or so occasional delay is seen in operations against HBase,
      try the Nagles' setting.  For example, see the user mailing list thread,
      <link xlink:href="http://search-hadoop.com/m/pduLg2fydtE/Inconsistent+scan+performance+with+caching+set+&amp;subj=Re+Inconsistent+scan+performance+with+caching+set+to+1">Inconsistent scan performance with caching set to 1</link>
      and the issue cited therein where setting notcpdelay improved scan speeds.  You might also
      see the graphs on the tail of <link xlink:href="https://issues.apache.org/jira/browse/HBASE-7008">HBASE-7008 Set scanner caching to a better default</link>
      where our Lars Hofhansl tries various data sizes w/ Nagle's on and off measuring the effect.</para>
    </section>
    <section xml:id="mttr">
      <title>Better Mean Time to Recover (MTTR)</title>
      <para>This section is about configurations that will make servers come back faster after a fail.
          See the Deveraj Das an Nicolas Liochon blog post
          <link xlink:href="http://hortonworks.com/blog/introduction-to-hbase-mean-time-to-recover-mttr/">Introduction to HBase Mean Time to Recover (MTTR)</link>
          for a brief introduction.</para>
      <para>The issue <link xlink:href="https://issues.apache.org/jira/browse/HBASE-8389">HBASE-8354 forces Namenode into loop with lease recovery requests</link>
          is messy but has a bunch of good discussion toward the end on low timeouts and how to effect faster recovery including citation of fixes
          added to HDFS.  Read the Varun Sharma comments.  The below suggested configurations are Varun's suggestions distilled and tested.  Make sure you are
          running on a late-version HDFS so you have the fixes he refers too and himself adds to HDFS that help HBase MTTR
          (e.g. HDFS-3703, HDFS-3712, and HDFS-4791 -- hadoop 2 for sure has them and late hadoop 1 has some).
          Set the following in the RegionServer.</para>
      <programlisting language="xml">
<![CDATA[<property>
<property>
    <name>hbase.lease.recovery.dfs.timeout</name>
    <value>23000</value>
    <description>How much time we allow elapse between calls to recover lease.
    Should be larger than the dfs timeout.</description>
</property>
<property>
    <name>dfs.client.socket-timeout</name>
    <value>10000</value>
    <description>Down the DFS timeout from 60 to 10 seconds.</description>
</property>
]]></programlisting>

        <para>And on the namenode/datanode side, set the following to enable 'staleness' introduced
          in HDFS-3703, HDFS-3912. </para>
        <programlisting language="xml"><![CDATA[
<property>
    <name>dfs.client.socket-timeout</name>
    <value>10000</value>
    <description>Down the DFS timeout from 60 to 10 seconds.</description>
</property>
<property>
    <name>dfs.datanode.socket.write.timeout</name>
    <value>10000</value>
    <description>Down the DFS timeout from 8 * 60 to 10 seconds.</description>
</property>
<property>
    <name>ipc.client.connect.timeout</name>
    <value>3000</value>
    <description>Down from 60 seconds to 3.</description>
</property>
<property>
    <name>ipc.client.connect.max.retries.on.timeouts</name>
    <value>2</value>
    <description>Down from 45 seconds to 3 (2 == 3 retries).</description>
</property>
<property>
    <name>dfs.namenode.avoid.read.stale.datanode</name>
    <value>true</value>
    <description>Enable stale state in hdfs</description>
</property>
<property>
    <name>dfs.namenode.stale.datanode.interval</name>
    <value>20000</value>
    <description>Down from default 30 seconds</description>
</property>
<property>
    <name>dfs.namenode.avoid.write.stale.datanode</name>
    <value>true</value>
    <description>Enable stale state in hdfs</description>
</property>
]]></programlisting>
      </section>

      <section
        xml:id="JMX_config">
        <title>JMX</title>
        <para>JMX(Java Management Extensions) provides built-in instrumentation that enables you
          to monitor and manage the Java VM. To enable monitoring and management from remote
          systems, you need to set system property com.sun.management.jmxremote.port(the port
          number through which you want to enable JMX RMI connections) when you start the Java VM.
          See <link
            xlink:href="http://docs.oracle.com/javase/6/docs/technotes/guides/management/agent.html">
          official document</link> for more information. Historically, besides above port mentioned,
          JMX opens 2 additional random TCP listening ports, which could lead to port conflict
          problem.(See <link
            xlink:href="https://issues.apache.org/jira/browse/HBASE-10289">HBASE-10289</link>
          for details)
        </para>
        <para>As an alternative, You can use the coprocessor-based JMX implementation provided
          by HBase. To enable it in 0.99 or above, add below property in
          <filename>hbase-site.xml</filename>:
        <programlisting language="xml"><![CDATA[
<property>
    <name>hbase.coprocessor.regionserver.classes</name>
    <value>org.apache.hadoop.hbase.JMXListener</value>
</property>
]]></programlisting>
          NOTE: DO NOT set com.sun.management.jmxremote.port for Java VM at the same time.
        </para>
        <para>Currently it supports Master and RegionServer Java VM. The reason why you only
          configure coprocessor for 'regionserver' is that, starting from HBase 0.99,
          a Master IS also a RegionServer. (See <link
            xlink:href="https://issues.apache.org/jira/browse/HBASE-10569">HBASE-10569</link>
          for more information.)
          By default, the JMX listens on TCP port 10102, you can further configure the port
          using below properties:

        <programlisting language="xml"><![CDATA[
<property>
    <name>regionserver.rmi.registry.port</name>
    <value>61130</value>
</property>
<property>
    <name>regionserver.rmi.connector.port</name>
    <value>61140</value>
</property>
]]></programlisting>
          The registry port can be shared with connector port in most cases, so you only
          need to configure regionserver.rmi.registry.port. However if you want to use SSL
          communication, the 2 ports must be configured to different values.
        </para>

        <para>By default the password authentication and SSL communication is disabled.
          To enable password authentication, you need to update <filename>hbase-env.sh</filename>
          like below:
      <screen language="bourne">
export HBASE_JMX_BASE="-Dcom.sun.management.jmxremote.authenticate=true                  \
                       -Dcom.sun.management.jmxremote.password.file=your_password_file   \
                       -Dcom.sun.management.jmxremote.access.file=your_access_file"

export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS $HBASE_JMX_BASE "
export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS $HBASE_JMX_BASE "
      </screen>
          See example password/access file under $JRE_HOME/lib/management.
        </para>

        <para>To enable SSL communication with password authentication, follow below steps:
      <screen language="bourne">
#1. generate a key pair, stored in myKeyStore
keytool -genkey -alias jconsole -keystore myKeyStore

#2. export it to file jconsole.cert
keytool -export -alias jconsole -keystore myKeyStore -file jconsole.cert

#3. copy jconsole.cert to jconsole client machine, import it to jconsoleKeyStore
keytool -import -alias jconsole -keystore jconsoleKeyStore -file jconsole.cert
      </screen>
          And then update <filename>hbase-env.sh</filename> like below:
      <screen language="bourne">
export HBASE_JMX_BASE="-Dcom.sun.management.jmxremote.ssl=true                         \
                       -Djavax.net.ssl.keyStore=/home/tianq/myKeyStore                 \
                       -Djavax.net.ssl.keyStorePassword=your_password_in_step_1       \
                       -Dcom.sun.management.jmxremote.authenticate=true                \
                       -Dcom.sun.management.jmxremote.password.file=your_password file \
                       -Dcom.sun.management.jmxremote.access.file=your_access_file"

export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS $HBASE_JMX_BASE "
export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS $HBASE_JMX_BASE "
      </screen>

          Finally start jconsole on client using the key store:
      <screen language="bourne">
jconsole -J-Djavax.net.ssl.trustStore=/home/tianq/jconsoleKeyStore
      </screen>
        </para>
        <para>NOTE: for HBase 0.98, To enable the HBase JMX implementation on Master, you also
          need to add below property in <filename>hbase-site.xml</filename>:
        <programlisting language="xml"><![CDATA[
<property>
    <name>hbase.coprocessor.master.classes</name>
    <value>org.apache.hadoop.hbase.JMXListener</value>
</property>
]]></programlisting>
          The corresponding properties for port configuration are master.rmi.registry.port
          (by default 10101) and master.rmi.connector.port(by default the same as registry.port)
        </para>
    </section>

   </section>

  </section>
  <!--  important config -->

</chapter>
