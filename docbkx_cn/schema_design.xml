<?xml version="1.0" encoding="UTF-8"?>
<chapter
  version="5.0"
  xml:id="schema"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xlink="http://www.w3.org/1999/xlink"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:svg="http://www.w3.org/2000/svg"
  xmlns:m="http://www.w3.org/1998/Math/MathML"
  xmlns:html="http://www.w3.org/1999/xhtml"
  xmlns:db="http://docbook.org/ns/docbook">
  <!--
/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
-->
  <title>HBase和模式(Schema)设计</title>
  <para>
      Ian Varley的硕士论文<link
          xlink:href="http://ianvarley.com/UT/MR/Varley_MastersReport_Full_2009-08-07.pdf">No Relation:
      The Mixed Blessings of Non-Relational Databases</link> 中对各种非关系型数据库的优缺点从总体上做了很好的介绍。推荐阅读该论文。
      另外，可以阅读<xref
          linkend="keyvalue" /> 和<xref
          linkend="schema.casestudies" /> 章节以了解HBase内部如何存储数据。
       </para>
  <section
    xml:id="schema.creation">
    <title> 模式创建 </title>
    <para> 我们可以使用Java API <link
            xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html">HBaseAdmin</link> 来对HBase的模式(schema)进行创建和更新。 </para>
    <para>当对表的列族进行修改时，必须先将表禁用。例如：</para>
    <programlisting language="java">
Configuration config = HBaseConfiguration.create();
HBaseAdmin admin = new HBaseAdmin(conf);
String table = "myTable";

admin.disableTable(table);

HColumnDescriptor cf1 = ...;
admin.addColumn(table, cf1);      // adding new ColumnFamily
HColumnDescriptor cf2 = ...;
admin.modifyColumn(table, cf2);    // modifying existing ColumnFamily

admin.enableTable(table);
    </programlisting>
    <para>查阅 <xref
        linkend="client_dependencies" /> 可以获取关于客户端连接配置的更多信息。</para>
    <para>注意：在0.92.x版本的HBase才支持在线的模式变更，而0.90.x版本的HBase必须先对表进行禁用操作。 </para>
    <section
      xml:id="schema.updates">
      <title>模式更新</title>
      <para>
          当改变了表或者列族（例如，分区(region)大小，块大小等），这些改变将在下一次的major压缩与StoreFile被重写时才会起作用。
          </para>
      <para>查看 <xref
          linkend="store" /> 可以获得关于StoreFile的更多信息。 </para>
    </section>
  </section>
  <section
    xml:id="number.of.cfs">
    <title> 列族的数目 </title>
    <para>
        目前HBase在有两个或者三个以上的列族的情况下表现的不是很好，所以尽量让你的模式中的列族数少一些。目前，刷写和压缩操作是基于每一个分区(Region)进行的，所以如果一个列族正在进行数据量很大的刷写操作，那相邻的列族也将进行刷写操作，虽然它们刷写的数据量很少。当很多列族被刷写和压缩，那么将会造成很多本不需要的I/O负载
        （解决该问题需要让刷写和压缩操作基于列族进行）。更多关于压缩的细节，请参考<xref
            linkend="compaction" />。
         </para>
    <para>
        尽量在你的模式中使用一个列族。只有你的所有数据读写操作只针对一个列族的时候，才可以引入第二个和第三个列族。例如，有两个列族,但你查询的时候总是只访问其中的一个，从来不会两个一起访问。
 </para>
    <section
      xml:id="number.of.cfs.card">
      <title>列族的基数</title>
      <para>
          如果一个表有多个列族，那么注意数据的基数（例如，行数）。如果列族A有1百万行，列族B有10亿行，列族A的数据可能会被分散到集群很多很多区中（及RegionServer中）。那么对列族A的遍历操作将会很低效。
          </para>
    </section>
  </section>
  <section
    xml:id="rowkey.design">
    <title>行键(RowKey)设计</title>
    <section
      xml:id="timeseries">
      <title> 单调递增的行键/时间序列数据 </title>
      <para>
          在Tom White的 <link
              xlink:href="http://oreilly.com/catalog/9780596521981">Hadoop权威指南(Hadoop: The Definitive Guide)</link> (O'Reilly出版社出版)一书中，有一个章节描述了一个值得注意的问题：在一个集群中，一个导入数据的进程一动不动，所有的客户端都在等待该表的某一个region(也可以说在等待一个单独的节点)，过了一会后，会等待下一个region，以此类推。如果使用了单调递增或者时序的行键就会导致这样的问题。详情可以参见IKai Lan关于为何类BigTable数据库中使用单调递增的行键会存在问题的漫画<link
              xlink:href="http://ikaisays.com/2011/01/25/app-engine-datastore-tip-monotonically-increasing-values-are-bad/">monotonically
          increasing values are bad</link>。使用了顺序的行键会将原本没有顺序的数据变得有顺序，从而会导致负载被压在一台机器上。所以要尽量避免时间戳或者(例如 1, 2, 3)这样的行键。
 </para>
      <para>
          如果你需要导入有时间顺序的数据到HBase中，可以学习<link
              xlink:href="http://opentsdb.net/">OpenTSDB</link>的成功经验。这里有一个描述它在HBase中使用的设计模式的页面<link
              xlink:href=" http://opentsdb.net/schema.html">schema</link>。 它使用的行键的格式是[metric_type][event_timestamp]，
          乍一看，这似乎违背了不将时间戳作为行键的建议。但是它并没有将时间戳放在行键的 <emphasis>起始</emphasis> 位置，并且该设计假设会有数十或者上百（或者更多）个不同的度量(metric)类型。 因此，持续的输入数据流会包含多种度量类型，插入(Puts)操作将会针对一个表的多个不同的region来进行，压力也就分散了。
     </para>
      <para>查看 <xref
          linkend="schema.casestudies" /> 可以了解更多关于行键设计的例子。 </para>
    </section>
    <section
      xml:id="keysize">
      <title>尽量最小化行和列的大小</title>
      <subtitle>或者为何我的StoreFile文件的索引很大？</subtitle>
      <para>
            在HBase中，通过行、列名和时间戳来确定一个值，它以单元格的值为单位在系统中传递。如果你的行和列的名字很大，甚至比值的大小还要大，你可能会遇到一些有趣的情况。比如Marc Limotte在<link
              xlink:href="https://issues.apache.org/jira/browse/HBASE-3551?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&amp;focusedCommentId=13005272#comment-13005272">HBASE-3551</link>
          （推荐！）末尾提到的情况。HBase的StoreFile (<xref
              linkend="hfile" />) 文件中保存了一些方便访问的索引信息。但是如果访问一个单元格的坐标信息太大，就会占用很大的内存。Marc在上面的引用的评论中建议要么将块的大小调高以便有足够的空间存储这些索引项，要么就修改表的模式将行和列名变小。压缩也可以产生很大的索引。可以查看用户邮件列表中的讨论<link
              xlink:href="http://search-hadoop.com/m/hemBv1LiN4Q1/a+question+storefileIndexSize&amp;subj=a+question+storefileIndexSize">a
          question storefileIndexSize</link>。
     </para>
      <para>
          大部分情况下小的低效率不会影响很大。不幸的是，在这里会是个问题。无论是列族，属性和行键都会在你的数据中重复几十亿次。
          </para>
      <para>查阅<xref
              linkend="keyvalue" />可以获取更多关于HBase内部数据存储的信息，以便了解为什么这很重要。</para>
      <section
        xml:id="keysize.cf">
        <title>列族</title>
        <para>
            尽可能将列族名设置的足够小，最好只用一个字符（比如，"d"表示data/default）。
             </para>
        <para>
            查阅<xref
                linkend="keyvalue" />可以获取更多关于HBase内部数据存储的信息，以便了解为什么这很重要。</para>
      </section>
      <section
        xml:id="keysize.attributes">
        <title>属性</title>
        <para>
            尽管详细的属性名(例如，"myVeryImportantAttribute")更易阅读，但在HBase中最好选择一个较短的属性名（例如，"via"）。 </para>
        <para>查阅<xref
                linkend="keyvalue" />可以获取更多关于HBase内部数据存储的信息，以便了解为什么这很重要。</para>
      </section>
      <section
        xml:id="keysize.row">
        <title>行键长度</title>
        <para>
            将它们设置的尽可能短是合理的。这样不会影响它们在数据访问（例如，Get和Scan）中的有效性。但是一个短的行键并不意味着
            在Get/Scan时比长的行键有更好的效果。在设计行键时需要进行权衡。
             </para>
      </section>
      <section
        xml:id="keysize.patterns">
        <title>字节模式</title>
        <para>长整型用8个字节来表示。用8个字节你可以存储最大值为18,446,744,073,709,551,615的无符号数。如果你将
            该数作为一个字符串存储 -- 那每一个字符都需要一个字节 -- 你需要的字节数接近用无符号数存储的3倍。
             </para>
        <para>不信？下面是一些示例代码，你可以自己运行一下。</para>
        <programlisting language="java">
// long
//
long l = 1234567890L;
byte[] lb = Bytes.toBytes(l);
System.out.println("long bytes length: " + lb.length);   // returns 8

String s = "" + l;
byte[] sb = Bytes.toBytes(s);
System.out.println("long as string length: " + sb.length);    // returns 10

// hash
//
MessageDigest md = MessageDigest.getInstance("MD5");
byte[] digest = md.digest(Bytes.toBytes(s));
System.out.println("md5 digest bytes length: " + digest.length);    // returns 16

String sDigest = new String(digest);
byte[] sbDigest = Bytes.toBytes(sDigest);
System.out.println("md5 digest as string length: " + sbDigest.length);    // returns 26
        </programlisting>
        <para>
            不幸的是，使用二进制类型来表示数据将会让你的数据在代码以外很难阅读。例如，当你增加一个值时，这就是你会在shell看到的：
            </para>
        <programlisting>
hbase(main):001:0> incr 't', 'r', 'f:q', 1
COUNTER VALUE = 1

hbase(main):002:0> get 't', 'r'
COLUMN                                        CELL
 f:q                                          timestamp=1369163040570, value=\x00\x00\x00\x00\x00\x00\x00\x01
1 row(s) in 0.0310 seconds
        </programlisting>
        <para>
            shell在尽最大努力打印一个字符串，并且在这种情况下它只能以16进制打印。同样的情况会发生在你region名中的行键上。
            如果你知道存储的是什么，那还好些。但是如果在同一个单元格里可以存放任意形式，那它也可能是不可读的。这就需要进行权衡。
            </para>
      </section>

    </section>
    <section
      xml:id="reverse.timestamp">
      <title>倒序时间戳</title>
      <note>
        <title>逆序Scan的API</title>
        <para>
            <link
                    xlink:href="https://issues.apache.org/jira/browse/HBASE-4811">HBASE-4811</link>实现了一个API，可以对一个表或一个表中的某一范围进行逆序遍历，而你不需要为了实现正向或反向遍历而优化你的设计模式。该特征在HBase
            0.98和更高的版本中可用。查看<link
                xlink:href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html#setReversed%28boolean" /> 以获取更多信息。
           </para>
      </note>

      <para>
          在数据处理过程中快速查找一个值的最近版本是一个通用的问题。 采用倒序的时间戳作为行键的一部分对某些特定情况的该问题会有所帮助。在Tom White的书Hadoop权威指南(Hadoop:The Definitive Guide)(O'Reilly出版社出版)的关于HBase的章节中可以找到这种例子，它采用了将(<code>Long.MAX_VALUE - timestamp</code>)添加到行键的最后，例如[key][reverse_timestamp]。
     </para>
      <para>
          在执行一个[key]的Scan操作时，该[key]的在表中最近的值（即时间戳最大的值）将会被找到并且获取第一个记录。因为HBase的行键是有序的，该行键排在比它更老的行键的前面，所以它会是第一个。
           </para>
      <para>该技术将会被用来代替 <xref
          linkend="schema.versions" /> 。其目的是“永远”(或一段很长时间)保存所有版本。同时，采用同样的Scan技术，可以很快获取到其他版本的数据。</para>
    </section>
    <section
      xml:id="rowkey.scope">
      <title>行键和列族</title>
      <para>
          行键在列族范围内。所以同样的行键可以在同一个表的每个列族中存在而不会冲突。
          </para>
    </section>
    <section
      xml:id="changing.rowkeys">
      <title>行键的不变性</title>
      <para>
          行键不能被改变。唯一可以“改变”行键的方式是先删除然后再插入。这是一个常见的问题，所以要注意最开始(和/或在插入很多数据之前)就保证行键是正确的。
           </para>
    </section>
    <section
      xml:id="rowkey.regionsplits">
      <title>行键和分区(Region)切割的关系</title>
      <para>
          如果你的表执行了预切割，那理解行键是如何跨越界限进行分区对你就很 <emphasis>重要</emphasis> 。
          作为解释这为什么很重要的一个例子，考虑在开头使用16进制字符的行键（例如&quot;0000000000000000&quot;到
          &quot;ffffffffffffffff&quot;）的实例。对这些行键执行<code>Bytes.split</code> （它在使用<code>HBaseAdmin.createTable(byte[] startKey, byte[] endKey, numRegions)</code>创建分区的时候就会进行分割操作）操作，将它们分割成10个分区，如下：
          </para>
      <screen>
48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48                                // 0
54 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10                 // 6
61 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -68                 // =
68 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -126  // D
75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 72                                // K
82 18 18 18 18 18 18 18 18 18 18 18 18 18 18 14                                // R
88 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -44                 // X
95 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -102                // _
102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102                // f
      </screen>
      <para>... (
          注意：最开始的字节在右边的评论中列了出来
          ) 鉴于第一次分割是'0'，最后一次分割是'f'，一切都很正常，是吗？还没这么快。 </para>
      <para>
          问题是所有的数据都会跑到最前面的两个分区和最后的分区中，因此会产生一个热点分区问题。为了理解为什么，参考
          <link
                  xlink:href="http://www.asciitable.com">ASCII表</link>。'0'是48的字节表示，'f'是102的字节表示，但是
          这里存在一个字节值的巨大跨越（字节58到96），它们<emphasis>从来没有在行键中出现过</emphasis>，因为仅有的值是
          [0-9]和[a-f]。因此，中间的分区将从不会被使用。为了对该例子中的行键执行预分区操作，需要自己定义分割（例如，不依赖内置的分割方法）。
          </para>
      <para>经验1: 预分割表通常是最好的做法，但是你需要保证预分割之后所有的分区可以访问所有的行键。在这个使用了16进制行键的例子中，就有一些问题。相同的问题也可以发生在<emphasis>任何</emphasis>行键中。你需要了解你的数据。 </para>
      <para>经验2: 虽然通常不建议使用16进制的行键（和可显示的数据），但是它也可以对表进行预分区，只需要所有被创建的分区都能访问所有的行键即可。 </para>
      <para>作为该示例的总结，下面展示了如何对16进制的行键进行预分割分区的一个例子： </para>
      <programlisting language="java"><![CDATA[public static boolean createTable(HBaseAdmin admin, HTableDescriptor table, byte[][] splits)
throws IOException {
  try {
    admin.createTable( table, splits );
    return true;
  } catch (TableExistsException e) {
    logger.info("table " + table.getNameAsString() + " already exists");
    // the table already exists...
    return false;
  }
}

public static byte[][] getHexSplits(String startKey, String endKey, int numRegions) {
  byte[][] splits = new byte[numRegions-1][];
  BigInteger lowestKey = new BigInteger(startKey, 16);
  BigInteger highestKey = new BigInteger(endKey, 16);
  BigInteger range = highestKey.subtract(lowestKey);
  BigInteger regionIncrement = range.divide(BigInteger.valueOf(numRegions));
  lowestKey = lowestKey.add(regionIncrement);
  for(int i=0; i < numRegions-1;i++) {
    BigInteger key = lowestKey.add(regionIncrement.multiply(BigInteger.valueOf(i)));
    byte[] b = String.format("%016x", key).getBytes();
    splits[i] = b;
  }
  return splits;
}]]></programlisting>
    </section>
  </section>
  <!--  rowkey design -->
  <section
    xml:id="schema.versions">
    <title> 版本数 </title>
    <section
      xml:id="schema.versions.max">
      <title>最大版本数</title>
      <para>
          行的最大版本数目是通过<link
              xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html">HColumnDescriptor</link>对每一个列族单独配置的。默认的最大版本数是1.这个参数很重要，在<xref
              linkend="datamodel" />中进行了描述。HBase<emphasis>不会</emphasis>覆盖行值，但是每行的不同时间（和不同列）存储的值不同。多余的版本会在major压缩时会移除。最大版本数可以根据应用的需要来增加或者减少。
     </para>
      <para>
          不推荐将版本最大值设置的很大(例如, 成百或更多)，除非老数据对你很重要。因为这会大大增加StoreFile的大小。
           </para>
    </section>
    <section
      xml:id="schema.minversions">
      <title> 最小版本数 </title>
      <para>
          和行的最大版本数一样，最小版本数也是通过<link
              xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html">HColumnDescriptor</link> 对每个列族中单独配置的。默认的最小版本数是0，表示该特性被禁用。 参数最小版本数和参数存活时间一起使用，允许配置如“保存最后T分钟的有价值的数据，最多N个版本，<emphasis>但最少大约M个版本</emphasis>”(M是最小版本数，M&lt;N)。 该参数仅在存活时间对列族启用，且必须小于行版本数。
     </para>
    </section>
  </section>
  <section
    xml:id="supported.datatypes">
    <title> 支持的数据类型 </title>
    <para>
        HBase通过<link
            xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Put.html">Put</link>和 <link
            xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Result.html">Result</link>支持"bytes-in/bytes-out"接口，所以任何可被转为字节数组的数据都可以作为值被存入。输入可以是字符串，数字，复杂对象，甚至是图像，只要他们能转化为字节。
     </para>
    <para>
        值的实际长度存在限制(例如，保存10-50MB对象到HBase中对查询操作来说太长); 搜索邮件列表可以获取关于本话题的对话。 HBase的所有行都遵循<xref
            linkend="datamodel" />, 包括版本。设计时需考虑到这些，也需要考虑列族的块大小。
    </para>

    <section
      xml:id="counters">
      <title>计数器</title>
      <para>
          HBase支持的数据类型“计数器”(例如, 具有原子递增数字的能力)值得一提。 参考HTable的<link
              xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#increment%28org.apache.hadoop.hbase.client.Increment%29">Increment</link>。
           </para>
      <para>
          计数器的同步在RegionServer中完成，而不是客户端。 </para>
    </section>
  </section>
  <section
    xml:id="schema.joins">
    <title>Joins</title>
    <para>
        如果你有多个表格，在进行模式设计时，不要忘了潜在的<xref
            linkend="joins" />因素。 </para>
  </section>
  <section
    xml:id="ttl">
    <title>存活时间 (TTL)</title>
    <para>
        列族可以设置TTL秒数，HBase在超时后将自动删除数据。影响一行的 <emphasis>全部</emphasis> 版本 - 甚至当前版本。HBase里面TTL时间是UTC时间.
         </para>
    <para>查看 <link
        xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html">HColumnDescriptor</link>
      可以获得更多的信息。 </para>
  </section>
  <section
    xml:id="cf.keep.deleted">
    <title> 保留被删除的单元格 </title>
    <para>
        列族可以选择性的保留被删除的单元格。这就是说<link
            xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Get.html">Get</link>或 <link
            xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html">Scan</link>操作仍可以获取被删除的单元格。只要这些操作都指定了时间范围，且在被删除的单元格生效的时间之前。
         </para>
    <para>
        被删除的单元格依然受TTL的影响，并且永远不会超过“最大版本数”。一个新的“原始”遍历(scan)返回所有被删除的行和删除标记。
         </para>
    <para>查看 <link
        xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html">HColumnDescriptor</link>
      可以获取到更多信息。 </para>
  </section>
  <section
    xml:id="secondary.indexes">
    <title> 二级索引和备用查询路径 </title>
    <para>
        本节也可以使用标题“如果我的表的行键看起来是<emphasis>这样</emphasis>但是我却想要像<emphasis>那样</emphasis>来查询表”。
        一个常见的例子是行键的格式是"用户-时间戳"，但是也有查询一个给定的时间范围内都有哪些用户的需求。因此，通过用户进行查询很容易，因为行键的开头部分是用户，但通过时间查询就不容易了。
         </para>
    <para>在这点的处理上，没有一个最好的方式，因为它依赖于： </para>
    <itemizedlist>
      <listitem>
        <para>用户数</para>
      </listitem>
      <listitem>
        <para>数据大小和数据到达率</para>
      </listitem>
      <listitem>
        <para>需求的灵活性（例如，完成一个临时的日期查询或预先指定范围的日期查询） </para>
      </listitem>
      <listitem>
        <para>期望的查询执行速度（例如，对于一些临时性的需求，90秒可能是合理的，但对于其他一些需求就可能显得太长） </para>
      </listitem>
    </itemizedlist>
    <para>... 并且，解决方案也可能会受到集群大小和处理能力等的影响。下面的小节有一些常用的技术。这是一个综合的，但不详尽的方法列表。 </para>
    <para>
        它不应该是一个惊喜，二级索引需要额外的集群空间和额外的处理。这些事情在关系型数据库管理系统中也同样会发生。因为创建一个可替代的索引需要空间和处理周期更新。关系型数据库管理系统在这方面更先进一些，因为对可替代索引的管理工作开箱即用。然而，HBase在数据规模很大的情况下表现更好，这是一个功能上的权衡。
     </para>
    <para>当实现这些方法中的任意一个的时候，请注意 <xref
        linkend="performance" /> 。</para>
    <para>此外，可以参考David Butler在回应用户邮件列表中的 <link
        xlink:href="http://search-hadoop.com/m/nvbiBp2TDP/Stargate%252Bhbase&amp;subj=Stargate+hbase">HBase,
        mail # user - Stargate+hbase</link>的跟帖。
    </para>
    <section
      xml:id="secondary.indexes.filter">
      <title> 过滤查询 </title>
      <para>
          根据具体应用，可以适当的使用<xref  linkend="client.filter" />。在这种情况下，没有创建二级索引。然而，不要试图从一个应用（例如，单线程客户端）对一个大表进行全表遍历。
           </para>
    </section>
    <section
      xml:id="secondary.indexes.periodic">
      <title> 定期更新二级索引 </title>
      <para>
          二级索引可以在另一个表中创建，可以通过MapReduce任务进行定期更新。这个任务可以在当天执行，但是依赖于加载策略，可能会与主表不同步。
          </para>
      <para>查看 <xref
          linkend="mapreduce.example.readwrite" /> 可以了解更多相关信息。</para>
    </section>
    <section
      xml:id="secondary.indexes.dualwrite">
      <title> 同时写入二级索引 </title>
      <para>
          另外一个策略是在给集群写入数据的时候同时写入二级索引（例如，既写入数据表也写入索引表）。如果这个方法在数据表已经存在之后才采用，就需要使用MapReduce任务(参考 <xref
              linkend="secondary.indexes.periodic" />)来对已有的数据生成索引。
          </para>
    </section>
    <section
      xml:id="secondary.indexes.summary">
      <title> 汇总表 </title>
      <para>
          对时间跨度长(例如,年报)和数据量巨大，通常使用汇总表。可通过MapReduce任务将结果生成到另一个表。
          </para>
      <para>See <xref
          linkend="mapreduce.example.summary" /> for more information.</para>
    </section>
    <section
      xml:id="secondary.indexes.coproc">
      <title> 协处理器二级索引 </title>
      <para> 协处理器就像关系型数据库管理系统中的触发器。该功能在0.92版本中被加入。更多的有关信息，可以参考 <xref
          linkend="coprocessors" /> 。
      </para>
    </section>
  </section>
  <section
    xml:id="constraints">
    <title>约束</title>
    <para>
        HBase的当前支持传统的（SQL）数据库中的说法“约束”。在表中的属性执行商业规则（例如，确保值在范围1-10以内）时建议使用约束。约束也可以用于强制引用的完整性，但是强烈反对这么做。
        因为完整性检查一旦启用将极大的降低表的吞吐量。使用约束的丰富文档可以在这里找到： <link
        xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/constraint">Constraint</link>。
      约束在0.94版本中被加入。 </para>
  </section>
  <section
    xml:id="schema.casestudies">
    <title>模式设计案例研究</title>
    <para>
        下面将描述一些典型的使用了HBase的数据获取的例子，以及如何进行行键的设计和构建。注：这只是潜在方法的举例说明，并非详尽无遗的方法列表。你需要了解你的数据和你的处理需求。
     </para>
    <para>强烈推荐你在阅读这些案例研究之前，先阅读 <xref
            linkend="schema" />的其他部分。 </para>
    <para>下面是有关这些案例研究的一些描述: </para>
    <itemizedlist>
      <listitem>
        <para>日志数据 / 时间序列数据</para>
      </listitem>
      <listitem>
        <para>日志数据 / 疯狂增长的时间序列数据</para>
      </listitem>
      <listitem>
        <para>客户/订单</para>
      </listitem>
      <listitem>
        <para>高/宽/中 模式设计</para>
      </listitem>
      <listitem>
        <para>列表数据</para>
      </listitem>
    </itemizedlist>
    <section
      xml:id="schema.casestudies.log-timeseries">
      <title>案例研究 - 日志数据和时间序列数据</title>
      <para>假设收集了下面的数据元素。 </para>
      <itemizedlist>
        <listitem>
          <para>主机名（Hostname）</para>
        </listitem>
        <listitem>
          <para>时间戳（Timestamp）</para>
        </listitem>
        <listitem>
          <para>日志事件（Log event）</para>
        </listitem>
        <listitem>
          <para>值/消息（Value/message）</para>
        </listitem>
      </itemizedlist>
      <para>
          我们可以将这些数据存在HBase中一个叫做LOG_DATA的表中，但是行键应该是什么呢？从这些属性中我们知道行键将会是域名、时间戳以及日志事件的某种组合，但具体是什么呢？
           </para>
      <section
        xml:id="schema.casestudies.log-timeseries.tslead">
        <title>时间戳在行键的起始位置</title>
        <para>行键 <code>[时间戳][主机名][日志事件]</code> 将会引起在<xref
                linkend="timeseries" />中提到的单调递增的行键问题。 </para>
        <para>
            还有另外的一个设计，就是通过对时间戳进行一个取余操作，使用“成组(bucketing)”的时间戳。无否时间顺序的遍历操作很重要，这可能会是一个有用的方法。注意必须指定好组(bucket)的大小，因为在遍历返回值时，同样需要这个大小。
            </para>
        <programlisting language="java">
long bucket = timestamp % numBuckets;
        </programlisting>
        <para>… 以构建：</para>
        <programlisting>
[bucket][timestamp][hostname][log-event]
        </programlisting>
        <para>
            如上所述，查询一个特定时间范围的数据，遍历（Scan）操作需要在每一个组中进行。例如，有100个组，将会把行键分布的很广泛（而不会造成热点现象），但是对于一个单独的时间戳，它将需要执行100次遍历操作，因此需要进行权衡。 </para>
      </section>
      <!-- ts lead -->
      <section
        xml:id="schema.casestudies.log-timeseries.hostlead">
        <title>主机名放在行键的起始位置</title>
        <para>如果有很多的主机来将写和读分布到所有的行键上，那行键 <code>[主机名][日志事件][时间戳]</code> 是一个备选项。如果优先使用主机名来遍历数据，那这个方法会很有用。 </para>
      </section>
      <!--  host lead -->
      <section
        xml:id="schema.casestudies.log-timeseries.revts">
        <title>时间戳，或逆序的时间戳？</title>
        <para>
            在大多数的访问操作中，可能需要访问最近的事件，那么应该将时间戳以逆序进行存储（例如，<code>timestamp = Long.MAX_VALUE –
            timestamp</code>）。使用逆序的时间戳可以对<code>[hostname][log-event]</code> 执行遍历操作并快速获取最近捕获到的事件。
             </para>
        <para>没有方法是错误的，这仅仅依赖于什么情况下最适合。 </para>
        <note>
          <title>逆序Scan API</title>
          <para>
              <link
                      xlink:href="https://issues.apache.org/jira/browse/HBASE-4811">HBASE-4811</link>实现了一个API，可以对一个表或一个表中的某一范围进行逆序遍历，而你不需要为了实现正向或反向遍历而优化你的设计模式。该特征在HBase 0.98和更高的版本中可用。查看<link
                  xlink:href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html#setReversed%28boolean" /> 以获取更多信息。
            </para>
        </note>
      </section>
      <!--  revts -->
      <section
        xml:id="schema.casestudies.log-timeseries.varkeys">
        <title>可变长度或固定长度的行键？</title>
        <para>
            HBase的行键的每一列都很重要。如果主机名是"a"并且事件类型是"e1"，那么组成的行键将会很小。但是，如果存储主机名是"myserver1.mycompany.com"，事件类型是
            "com.package1.subpackage2.subsubpackage3.ImportantService"呢？
             </para>
        <para>
            在行键中使用某些替代值是很有必要的。有至少两种方法：哈希和数字化。在主机名在行键的起始位置的例子中，它可能看起来是这个样子：
             </para>
        <para>使用哈希来组成行键：</para>
        <itemizedlist>
          <listitem>
            <para>[主机名的MD5哈希值] = 16 bytes</para>
          </listitem>
          <listitem>
            <para>[事件类型的MD5哈希值] = 16 bytes</para>
          </listitem>
          <listitem>
            <para>[时间戳] = 8 bytes</para>
          </listitem>
        </itemizedlist>
        <para>使用替代数字来组成行键： </para>
        <para>对于这种方法，LOG_DATA需要一个额外的查询表LOG_TYPES。LOG_TYPES的行键将是：
             </para>
        <itemizedlist>
          <listitem>
            <para>[type] (例如，, 表示主机名和事件类型的字节)</para>
          </listitem>
          <listitem>
            <para>[bytes] 原始主机名或事件类型的可变长度的字节</para>
          </listitem>
        </itemizedlist>
        <para>
            这个行键的列可能是一个分配的长整型的数字，它可以通过使用<link
                xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#incrementColumnValue%28byte[],%20byte[],%20byte[],%20long%29">HBase
            计数器</link>获取。
             </para>
        <para>因此组成的行键将是： </para>
        <itemizedlist>
          <listitem>
            <para>[替代主机名的长整型] = 8 bytes</para>
          </listitem>
          <listitem>
            <para>[替代事件类型的长整型] = 8 bytes</para>
          </listitem>
          <listitem>
            <para>[时间戳] = 8 bytes</para>
          </listitem>
        </itemizedlist>
        <para>
            在任一哈希或数字替换的方法中，主机名和事件类型的原始值都可以作为一列存储。
            </para>
      </section>
      <!--  varkeys -->
    </section>
    <!--  log data and timeseries -->
    <section
      xml:id="schema.casestudies.log-steroids">
      <title>案例研究 - 日志数据和疯狂增长的时间序列数据</title>
      <para>
          这实际上是OpenTSDB采用的方法。 OpenTSDB将数据重写并且将一行数据按照给定的时间区间写入列中。 详细的解释请参考：<link
              xlink:href="http://opentsdb.net/schema.html">http://opentsdb.net/schema.html</link> 和 HBase 2012年度会议(HBaseCon2012)上的<link
              xlink:href="http://www.cloudera.com/content/cloudera/en/resources/library/hbasecon/video-hbasecon-2012-lessons-learned-from-opentsdb.html">Lessons
          Learned from OpenTSDB</link>。
           </para>
      <para>
          但是，数据获取是如何进行的，例如，以这种方式...
          </para>
      <screen>
[hostname][log-event][timestamp1]
[hostname][log-event][timestamp2]
[hostname][log-event][timestamp3]
        </screen>
      <para>
          ......为每个具体事件使用不同的行键，但是，又被重新写成这样...
           </para>
      <screen>[hostname][log-event][timerange]</screen>
      <para>
          ...并且上面的每一个事件都会转换成一个相对于起始时间范围（例如，每5分钟）的一个时间偏移量存储在列中。这显然是一个非常先进的加工技术，但HBase的使这成为可能。
           </para>
    </section>
    <!--  log data timeseries steroids -->

    <section
      xml:id="schema.casestudies.custorder">
      <title>案例研究 - 客户/订单</title>
      <para>假设HBase被用来存储客户和订单信息。这里获取了两个主要类型的记录：客户类型的记录和订单类型的记录。
           </para>
      <para>客户类型的记录将包括你一般期望的所有数据： </para>
      <itemizedlist>
        <listitem>
          <para>客户编号</para>
        </listitem>
        <listitem>
          <para>客户名</para>
        </listitem>
        <listitem>
          <para>地址 (例如，城市、省、邮编)</para>
        </listitem>
        <listitem>
          <para>手机号码等</para>
        </listitem>
      </itemizedlist>
      <para>订单类型的记录将包括下面数据： </para>
      <itemizedlist>
        <listitem>
          <para>客户编号</para>
        </listitem>
        <listitem>
          <para>订单编号</para>
        </listitem>
        <listitem>
          <para>销售状态</para>
        </listitem>
        <listitem>
          <para>
              一系列的船舶位置和航线的嵌套对象(参考 <xref
                  linkend="schema.casestudies.custorder.obj" /> 了解更多细节)。
              </para>
        </listitem>
      </itemizedlist>
      <para>
          假设客户编号和销售订单的组合唯一地标识一个订单，这两个属性会组成行键，特别是组成这样的行键：
           </para>
      <screen>[客户编号][订单编号]</screen>
      <para>... 对于订单表。但是，还需要做出一个决定： 使用<emphasis>原始值</emphasis>作为行键是最好的选择吗？ </para>
      <para>
          我们面临和日志数据使用案例中相同的问题。在行键中应该如何表示客户编号，格式是什么样子（例如，是数字？还是字母？）。因为在HBase中使用固定长度的行键有优势，
          所以我们为了将行键分散在各个region上，做除了下面的选择：</para>
        <para>将哈希值组合成行键： </para>
      <itemizedlist>
        <listitem>
          <para>[客户编号的MD5值] = 16 字节</para>
        </listitem>
        <listitem>
          <para>[订单编号的MD5值] = 16 字节</para>
        </listitem>
      </itemizedlist>
      <para>将数值/哈希值组成的行键： </para>
      <itemizedlist>
        <listitem>
          <para>[替代客户编号的长整型(long)] = 8 字节</para>
        </listitem>
        <listitem>
          <para>[订单编号的MD5值] = 16 字节</para>
        </listitem>
      </itemizedlist>
      <section
        xml:id="schema.casestudies.custorder.tables">
        <title>单表? 多表?</title>
        <para>传统的设计方法是设计两个独立的表CUSTOMER和SALES。另一种选择是将多个类型的记录放在同一个表里（例如， CUSTOMER++）。
             </para>
        <para>客户类型记录的行键： </para>
        <itemizedlist>
          <listitem>
            <para>[customer-id]</para>
          </listitem>
          <listitem>
            <para>[type] = type 如果为‘1’ 表示客户类型的记录</para>
          </listitem>
        </itemizedlist>
        <para>订单类型记录的行键： </para>
        <itemizedlist>
          <listitem>
            <para>[customer-id]</para>
          </listitem>
          <listitem>
            <para>[type] = type 如果为‘2’ 表示订单类型的记录</para>
          </listitem>
          <listitem>
            <para>[order]</para>
          </listitem>
        </itemizedlist>
        <para>
            使用后面一种方法的优点是将不同类型的记录按照客户id组织到了一起（例如，一次遍历就可以获取到客户的所有信息）。缺点不容易遍历特定类型的记录。
        </para>
      </section>
      <section
        xml:id="schema.casestudies.custorder.obj">
        <title>订单对象设计</title>
        <para>
            现在我们需要了解如何给订单对象建模。假设类结构如下：
            </para>
        <variablelist>
          <varlistentry>
            <term>订单(Order)</term>
            <listitem>
              <para>(一个订单可以有多个ShippingLocations</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>LineItem</term>
            <listitem>
              <para>(一个ShippingLocation可以有多个LineItems</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>... 存储了该数据的多个选项。 </para>
        <section
          xml:id="schema.casestudies.custorder.obj.norm">
          <title>完全规范化</title>
          <para>
              使用该方法，将会有三个独立的表：ORDER、SHIPPING_LOCATION和LINE_ITEM。
               </para>
          <para>ORDER表的行键在 <xref
              linkend="schema.casestudies.custorder" />已经描述过了。
          </para>
          <para>TSHIPPING_LOCATION表使用组合行键，看起来像这样： </para>
          <itemizedlist>
            <listitem>
              <para>[order-rowkey]</para>
            </listitem>
            <listitem>
              <para>[shipping location 编号] (例如，位置1，位置2，等等。)</para>
            </listitem>
          </itemizedlist>
          <para>LINE_ITEM表使用组合行键，看起来像这样： </para>
          <itemizedlist>
            <listitem>
              <para>[order-rowkey]</para>
            </listitem>
            <listitem>
              <para>[shipping location 编号] (例如，位置1，位置2，等等。)</para>
            </listitem>
            <listitem>
              <para>[line item 编号] (例如，第1个lineitem，第二个，等等。)</para>
            </listitem>
          </itemizedlist>
          <para>
              这种标准化模式和关系型数据库管理系统中的方法很像，但是这不是你用HBase的唯一选择。这种方法的缺点是，查询任何订单的信息，你需要：
               </para>
          <itemizedlist>
            <listitem>
              <para>从ORDER表获取订单</para>
            </listitem>
            <listitem>
              <para>遍历the SHIPPING_LOCATION表找到一个给定的ShippingLocation的实例
                </para>
            </listitem>
            <listitem>
              <para>对于每一个ShippingLocation，遍历LINE_ITEM表</para>
            </listitem>
          </itemizedlist>
          <para>
              当然，关系型数据库管理系统会在后台做这些工作，但是HBase没有join操作，所以不会自动做这些，你需要了解这个事实。 </para>
        </section>
        <section
          xml:id="schema.casestudies.custorder.obj.rectype">
          <title>对所有类型的记录使用一张表</title>
          <para>
              使用这种方法，将会在ORDER表中包含
               </para>
          <para>在 <xref
              linkend="schema.casestudies.custorder" />中提到的订单行键。</para>
          <itemizedlist>
            <listitem>
              <para>[order-rowkey]</para>
            </listitem>
            <listitem>
              <para>[ORDER record type]</para>
            </listitem>
          </itemizedlist>
          <para>ShippingLocation组合行键看起来会像这个样子： </para>
          <itemizedlist>
            <listitem>
              <para>[order-rowkey]</para>
            </listitem>
            <listitem>
              <para>[SHIPPING record type]</para>
            </listitem>
            <listitem>
              <para>[shipping location 编号] (例如，位置1，位置2，等等。)</para>
            </listitem>
          </itemizedlist>
          <para>LineItem组合行键看起来会像这个样子： </para>
          <itemizedlist>
            <listitem>
              <para>[order-rowkey]</para>
            </listitem>
            <listitem>
              <para>[LINE record type]</para>
            </listitem>
            <listitem>
              <para>[shipping location 编号] (例如，位置1，位置2，等等。)</para>
            </listitem>
            <listitem>
              <para>[line item 编号] (例如，第1个lineitem，第二个，等等。)</para>
            </listitem>
          </itemizedlist>
        </section>
        <section
          xml:id="schema.casestudies.custorder.obj.denorm">
          <title>非规范化</title>
          <para>
              用单表保存所有类型的记录的方法是非规范化的并且将一些对象层次扁平化，例如将ShippingLocation分配给了每一个LineItem实例。
               </para>
          <para>LineItem组合行键看起来会像这个样子： </para>
          <itemizedlist>
            <listitem>
              <para>[order-rowkey]</para>
            </listitem>
            <listitem>
              <para>[LINE record type]</para>
            </listitem>
            <listitem>
              <para>[line item number] (例如，位置1，位置2，等等。 - 注意必须对订单都是唯一的)</para>
            </listitem>
          </itemizedlist>
          <para>... LineItem的列看起来像这个样子： </para>
          <itemizedlist>
            <listitem>
              <para>itemNumber</para>
            </listitem>
            <listitem>
              <para>quantity</para>
            </listitem>
            <listitem>
              <para>price</para>
            </listitem>
            <listitem>
              <para>shipToLine1 (ShippingLocation的非标准化)</para>
            </listitem>
            <listitem>
              <para>shipToLine2 (ShippingLocation的非标准化)</para>
            </listitem>
            <listitem>
              <para>shipToCity (ShippingLocation的非标准化)</para>
            </listitem>
            <listitem>
              <para>shipToState (ShippingLocation的非标准化)</para>
            </listitem>
            <listitem>
              <para>shipToZip (ShippingLocation的非标准化)</para>
            </listitem>
          </itemizedlist>
          <para>
              这种方法的优点是对象层次结构不太复杂，但缺点之一是任何信息的变化，将会让更新变得更为复杂。
          </para>
        </section>
        <section
          xml:id="schema.casestudies.custorder.obj.singleobj">
          <title>BLOB对象</title>
          <para>
              使用该方法，整个订单对象图将会被当做一个BLOB对象。例如，使用<xref
                  linkend="schema.casestudies.custorder" /> 中的描述来表示ORDER表的行键，并且"order"列将包含一个Order、ShippingLocations和LineItems的容器的反序列化对象。
            </para>
          <para>
              有很多选择：JSON，XML，Java序列化，Avro，Hadoop Writables等。他们全部都是同一种方法的变体：将对象编码成一个字节数组。注意，这种方法保证了对象模型变化情况下的向后兼容性，使得较旧的持久化结构仍然可以被HBase读出。
         </para>
          <para>
              优点是能够以最小的I/O（例如，本例中对每个Order的一次单独的HBase的Get操作）管理复杂的对象图，但缺点包括序列化的向后兼容性，系列化对语言的依赖（例如，Java序列仅适用于Java客户端），你必须反序列化整个对象以获取任何一块BLOB中的信息的事实，还有在类似于Hive这样的框架下获取客户对象的难度。
          </para>
        </section>
      </section>
      <!--  cust/order order object -->
    </section>
    <!--  cust/order -->

    <section
      xml:id="schema.smackdown">
      <title>Case Study - "Tall/Wide/Middle" Schema Design Smackdown</title>
      <para>This section will describe additional schema design questions that appear on the
        dist-list, specifically about tall and wide tables. These are general guidelines and not
        laws - each application must consider its own needs. </para>
      <section
        xml:id="schema.smackdown.rowsversions">
        <title>Rows vs. Versions</title>
        <para>A common question is whether one should prefer rows or HBase's built-in-versioning.
          The context is typically where there are "a lot" of versions of a row to be retained
          (e.g., where it is significantly above the HBase default of 1 max versions). The
          rows-approach would require storing a timestamp in some portion of the rowkey so that they
          would not overwite with each successive update. </para>
        <para>Preference: Rows (generally speaking). </para>
      </section>
      <section
        xml:id="schema.smackdown.rowscols">
        <title>Rows vs. Columns</title>
        <para>Another common question is whether one should prefer rows or columns. The context is
          typically in extreme cases of wide tables, such as having 1 row with 1 million attributes,
          or 1 million rows with 1 columns apiece. </para>
        <para>Preference: Rows (generally speaking). To be clear, this guideline is in the context
          is in extremely wide cases, not in the standard use-case where one needs to store a few
          dozen or hundred columns. But there is also a middle path between these two options, and
          that is "Rows as Columns." </para>
      </section>
      <section
        xml:id="schema.smackdown.rowsascols">
        <title>Rows as Columns</title>
        <para>The middle path between Rows vs. Columns is packing data that would be a separate row
          into columns, for certain rows. OpenTSDB is the best example of this case where a single
          row represents a defined time-range, and then discrete events are treated as columns. This
          approach is often more complex, and may require the additional complexity of re-writing
          your data, but has the advantage of being I/O efficient. For an overview of this approach,
          see <xref
            linkend="schema.casestudies.log-steroids" />. </para>
      </section>
    </section>
    <!--  note:  the following id is not consistent with the others becaus it was formerly in the Case Studies chapter,
	    but I didn't want to break backward compatibility of the link.  But future entries should look like the above case-study
	    links (schema.casestudies. ...)  -->
    <section
      xml:id="casestudies.schema.listdata">
      <title>Case Study - List Data</title>
      <para>The following is an exchange from the user dist-list regarding a fairly common question:
        how to handle per-user list data in Apache HBase. </para>
      <para>*** QUESTION ***</para>
      <para> We're looking at how to store a large amount of (per-user) list data in HBase, and we
        were trying to figure out what kind of access pattern made the most sense. One option is
        store the majority of the data in a key, so we could have something like: </para>

      <programlisting><![CDATA[
<FixedWidthUserName><FixedWidthValueId1>:"" (no value)
<FixedWidthUserName><FixedWidthValueId2>:"" (no value)
<FixedWidthUserName><FixedWidthValueId3>:"" (no value)
]]></programlisting>

      <para>The other option we had was to do this entirely using:</para>
      <programlisting language="xml"><![CDATA[
<FixedWidthUserName><FixedWidthPageNum0>:<FixedWidthLength><FixedIdNextPageNum><ValueId1><ValueId2><ValueId3>...
<FixedWidthUserName><FixedWidthPageNum1>:<FixedWidthLength><FixedIdNextPageNum><ValueId1><ValueId2><ValueId3>...
    		]]></programlisting>
      <para> where each row would contain multiple values. So in one case reading the first thirty
        values would be: </para>
      <programlisting language="java"><![CDATA[
scan { STARTROW => 'FixedWidthUsername' LIMIT => 30}
    		]]></programlisting>
      <para>And in the second case it would be </para>
      <programlisting>
get 'FixedWidthUserName\x00\x00\x00\x00'
    		</programlisting>
      <para> The general usage pattern would be to read only the first 30 values of these lists,
        with infrequent access reading deeper into the lists. Some users would have &lt;= 30 total
        values in these lists, and some users would have millions (i.e. power-law distribution) </para>
      <para> The single-value format seems like it would take up more space on HBase, but would
        offer some improved retrieval / pagination flexibility. Would there be any significant
        performance advantages to be able to paginate via gets vs paginating with scans? </para>
      <para> My initial understanding was that doing a scan should be faster if our paging size is
        unknown (and caching is set appropriately), but that gets should be faster if we'll always
        need the same page size. I've ended up hearing different people tell me opposite things
        about performance. I assume the page sizes would be relatively consistent, so for most use
        cases we could guarantee that we only wanted one page of data in the fixed-page-length case.
        I would also assume that we would have infrequent updates, but may have inserts into the
        middle of these lists (meaning we'd need to update all subsequent rows). </para>
      <para> Thanks for help / suggestions / follow-up questions. </para>
      <para>*** ANSWER ***</para>
      <para> If I understand you correctly, you're ultimately trying to store triples in the form
        "user, valueid, value", right? E.g., something like: </para>
      <programlisting>
"user123, firstname, Paul",
"user234, lastname, Smith"
			</programlisting>
      <para> (But the usernames are fixed width, and the valueids are fixed width). </para>
      <para> And, your access pattern is along the lines of: "for user X, list the next 30 values,
        starting with valueid Y". Is that right? And these values should be returned sorted by
        valueid? </para>
      <para> The tl;dr version is that you should probably go with one row per user+value, and not
        build a complicated intra-row pagination scheme on your own unless you're really sure it is
        needed. </para>
      <para> Your two options mirror a common question people have when designing HBase schemas:
        should I go "tall" or "wide"? Your first schema is "tall": each row represents one value for
        one user, and so there are many rows in the table for each user; the row key is user +
        valueid, and there would be (presumably) a single column qualifier that means "the value".
        This is great if you want to scan over rows in sorted order by row key (thus my question
        above, about whether these ids are sorted correctly). You can start a scan at any
        user+valueid, read the next 30, and be done. What you're giving up is the ability to have
        transactional guarantees around all the rows for one user, but it doesn't sound like you
        need that. Doing it this way is generally recommended (see here <link
          xlink:href="http://hbase.apache.org/book.html#schema.smackdown">http://hbase.apache.org/book.html#schema.smackdown</link>). </para>
      <para> Your second option is "wide": you store a bunch of values in one row, using different
        qualifiers (where the qualifier is the valueid). The simple way to do that would be to just
        store ALL values for one user in a single row. I'm guessing you jumped to the "paginated"
        version because you're assuming that storing millions of columns in a single row would be
        bad for performance, which may or may not be true; as long as you're not trying to do too
        much in a single request, or do things like scanning over and returning all of the cells in
        the row, it shouldn't be fundamentally worse. The client has methods that allow you to get
        specific slices of columns. </para>
      <para> Note that neither case fundamentally uses more disk space than the other; you're just
        "shifting" part of the identifying information for a value either to the left (into the row
        key, in option one) or to the right (into the column qualifiers in option 2). Under the
        covers, every key/value still stores the whole row key, and column family name. (If this is
        a bit confusing, take an hour and watch Lars George's excellent video about understanding
        HBase schema design: <link
          xlink:href="http://www.youtube.com/watch?v=_HLoH_PgrLk)">http://www.youtube.com/watch?v=_HLoH_PgrLk)</link>. </para>
      <para> A manually paginated version has lots more complexities, as you note, like having to
        keep track of how many things are in each page, re-shuffling if new values are inserted,
        etc. That seems significantly more complex. It might have some slight speed advantages (or
        disadvantages!) at extremely high throughput, and the only way to really know that would be
        to try it out. If you don't have time to build it both ways and compare, my advice would be
        to start with the simplest option (one row per user+value). Start simple and iterate! :)
      </para>
    </section>
    <!--  listdata -->

  </section>
  <!--  schema design cases -->
  <section
    xml:id="schema.ops">
    <title>操作和性能配置选项</title>
    <para>请参考有关性能的章节 <xref
        linkend="perf.schema" /> ，了解更多和设计模式的操作和性能有关的选项，例如布隆过滤器、配置表的region（分区）大小，压缩以及块大小。
    </para>
  </section>

</chapter>
<!--  schema design -->
