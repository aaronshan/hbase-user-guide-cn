<?xml version="1.0" encoding="UTF-8"?>
<chapter
  version="5.0"
  xml:id="schema"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xlink="http://www.w3.org/1999/xlink"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:svg="http://www.w3.org/2000/svg"
  xmlns:m="http://www.w3.org/1998/Math/MathML"
  xmlns:html="http://www.w3.org/1999/xhtml"
  xmlns:db="http://docbook.org/ns/docbook">
  <!--
/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
-->
  <title>HBase和模式(Schema)设计</title>
  <para>
      Ian Varley的硕士论文<link
          xlink:href="http://ianvarley.com/UT/MR/Varley_MastersReport_Full_2009-08-07.pdf">No Relation:
      The Mixed Blessings of Non-Relational Databases</link> 中对各种非关系型数据库的优缺点从总体上做了很好的介绍。推荐阅读该论文。
      另外，可以阅读<xref
          linkend="keyvalue" /> 和<xref
          linkend="schema.casestudies" /> 章节以了解HBase内部如何存储数据。
       </para>
  <section
    xml:id="schema.creation">
    <title> 模式创建 </title>
    <para> 我们可以使用Java API <link
            xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html">HBaseAdmin</link> 来对HBase的模式(schema)进行创建和更新。 </para>
    <para>当对表的列族进行修改时，必须先将表禁用。例如：</para>
    <programlisting language="java">
Configuration config = HBaseConfiguration.create();
HBaseAdmin admin = new HBaseAdmin(conf);
String table = "myTable";

admin.disableTable(table);

HColumnDescriptor cf1 = ...;
admin.addColumn(table, cf1);      // adding new ColumnFamily
HColumnDescriptor cf2 = ...;
admin.modifyColumn(table, cf2);    // modifying existing ColumnFamily

admin.enableTable(table);
    </programlisting>
    <para>查阅 <xref
        linkend="client_dependencies" /> 可以获取关于客户端连接配置的更多信息。</para>
    <para>注意：在0.92.x版本的HBase才支持在线的模式变更，而0.90.x版本的HBase必须先对表进行禁用操作。 </para>
    <section
      xml:id="schema.updates">
      <title>模式更新</title>
      <para>
          当改变了表或者列族（例如，分区(region)大小，块大小等），这些改变将在下一次的major压缩与StoreFile被重写时才会起作用。
          </para>
      <para>查看 <xref
          linkend="store" /> 可以获得关于StoreFile的更多信息。 </para>
    </section>
  </section>
  <section
    xml:id="number.of.cfs">
    <title> 列族的数目 </title>
    <para>
        目前HBase在有两个或者三个以上的列族的情况下表现的不是很好，所以尽量让你的模式中的列族数少一些。目前，刷写和压缩操作是基于每一个分区(Region)进行的，所以如果一个列族正在进行数据量很大的刷写操作，那相邻的列族也将进行刷写操作，虽然它们刷写的数据量很少。当很多列族被刷写和压缩，那么将会造成很多本不需要的I/O负载
        （解决该问题需要让刷写和压缩操作基于列族进行）。更多关于压缩的细节，请参考<xref
            linkend="compaction" />。
         </para>
    <para>
        尽量在你的模式中使用一个列族。只有你的所有数据读写操作只针对一个列族的时候，才可以引入第二个和第三个列族。例如，有两个列族,但你查询的时候总是只访问其中的一个，从来不会两个一起访问。
 </para>
    <section
      xml:id="number.of.cfs.card">
      <title>列族的基数</title>
      <para>
          如果一个表有多个列族，那么注意数据的基数（例如，行数）。如果列族A有1百万行，列族B有10亿行，列族A的数据可能会被分散到集群很多很多区中（及RegionServer中）。那么对列族A的遍历操作将会很低效。
          </para>
    </section>
  </section>
  <section
    xml:id="rowkey.design">
    <title>行键(RowKey)设计</title>
    <section
      xml:id="timeseries">
      <title> 单调递增的行键/时间序列数据 </title>
      <para>
          在Tom White的 <link
              xlink:href="http://oreilly.com/catalog/9780596521981">Hadoop权威指南(Hadoop: The Definitive Guide)</link> (O'Reilly出版社出版)一书中，有一个章节描述了一个值得注意的问题：在一个集群中，一个导入数据的进程一动不动，所有的客户端都在等待该表的某一个region(也可以说在等待一个单独的节点)，过了一会后，会等待下一个region，以此类推。如果使用了单调递增或者时序的行键就会导致这样的问题。详情可以参见IKai Lan关于为何类BigTable数据库中使用单调递增的行键会存在问题的漫画<link
              xlink:href="http://ikaisays.com/2011/01/25/app-engine-datastore-tip-monotonically-increasing-values-are-bad/">monotonically
          increasing values are bad</link>。使用了顺序的行键会将原本没有顺序的数据变得有顺序，从而会导致负载被压在一台机器上。所以要尽量避免时间戳或者(例如 1, 2, 3)这样的行键。
 </para>
      <para>
          如果你需要导入有时间顺序的数据到HBase中，可以学习<link
              xlink:href="http://opentsdb.net/">OpenTSDB</link>的成功经验。这里有一个描述它在HBase中使用的设计模式的页面<link
              xlink:href=" http://opentsdb.net/schema.html">schema</link>。 它使用的行键的格式是[metric_type][event_timestamp]，
          乍一看，这似乎违背了不将时间戳作为行键的建议。但是它并没有将时间戳放在行键的 <emphasis>起始</emphasis> 位置，并且该设计假设会有数十或者上百（或者更多）个不同的度量(metric)类型。 因此，持续的输入数据流会包含多种度量类型，插入(Puts)操作将会针对一个表的多个不同的region来进行，压力也就分散了。
     </para>
      <para>查看 <xref
          linkend="schema.casestudies" /> 可以了解更多关于行键设计的例子。 </para>
    </section>
    <section
      xml:id="keysize">
      <title>尽量最小化行和列的大小</title>
      <subtitle>或者为何我的StoreFile文件的索引很大？</subtitle>
      <para>
            在HBase中，通过行、列名和时间戳来确定一个值，它以单元格的值为单位在系统中传递。如果你的行和列的名字很大，甚至比值的大小还要大，你可能会遇到一些有趣的情况。比如Marc Limotte在<link
              xlink:href="https://issues.apache.org/jira/browse/HBASE-3551?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&amp;focusedCommentId=13005272#comment-13005272">HBASE-3551</link>
          （推荐！）末尾提到的情况。HBase的StoreFile (<xref
              linkend="hfile" />) 文件中保存了一些方便访问的索引信息。但是如果访问一个单元格的坐标信息太大，就会占用很大的内存。Marc在上面的引用的评论中建议要么将块的大小调高以便有足够的空间存储这些索引项，要么就修改表的模式将行和列名变小。压缩也可以产生很大的索引。可以查看用户邮件列表中的讨论<link
              xlink:href="http://search-hadoop.com/m/hemBv1LiN4Q1/a+question+storefileIndexSize&amp;subj=a+question+storefileIndexSize">a
          question storefileIndexSize</link>。
     </para>
      <para>
          大部分情况下小的低效率不会影响很大。不幸的是，在这里会是个问题。无论是列族，属性和行键都会在你的数据中重复几十亿次。
          </para>
      <para>查阅<xref
              linkend="keyvalue" />可以获取更多关于HBase内部数据存储的信息，以便了解为什么这很重要。</para>
      <section
        xml:id="keysize.cf">
        <title>列族</title>
        <para>
            尽可能将列族名设置的足够小，最好只用一个字符（比如，"d"表示data/default）。
             </para>
        <para>
            查阅<xref
                linkend="keyvalue" />可以获取更多关于HBase内部数据存储的信息，以便了解为什么这很重要。</para>
      </section>
      <section
        xml:id="keysize.attributes">
        <title>属性</title>
        <para>
            尽管详细的属性名(例如，"myVeryImportantAttribute")更易阅读，但在HBase中最好选择一个较短的属性名（例如，"via"）。 </para>
        <para>查阅<xref
                linkend="keyvalue" />可以获取更多关于HBase内部数据存储的信息，以便了解为什么这很重要。</para>
      </section>
      <section
        xml:id="keysize.row">
        <title>行键长度</title>
        <para>
            将它们设置的尽可能短是合理的。这样不会影响它们在数据访问（例如，Get和Scan）中的有效性。但是一个短的行键并不意味着
            在Get/Scan时比长的行键有更好的效果。在设计行键时需要进行权衡。
             </para>
      </section>
      <section
        xml:id="keysize.patterns">
        <title>字节模式</title>
        <para>长整型用8个字节来表示。用8个字节你可以存储最大值为18,446,744,073,709,551,615的无符号数。如果你将
            该数作为一个字符串存储 -- 那每一个字符都需要一个字节 -- 你需要的字节数接近用无符号数存储的3倍。
             </para>
        <para>不信？下面是一些示例代码，你可以自己运行一下。</para>
        <programlisting language="java">
// long
//
long l = 1234567890L;
byte[] lb = Bytes.toBytes(l);
System.out.println("long bytes length: " + lb.length);   // returns 8

String s = "" + l;
byte[] sb = Bytes.toBytes(s);
System.out.println("long as string length: " + sb.length);    // returns 10

// hash
//
MessageDigest md = MessageDigest.getInstance("MD5");
byte[] digest = md.digest(Bytes.toBytes(s));
System.out.println("md5 digest bytes length: " + digest.length);    // returns 16

String sDigest = new String(digest);
byte[] sbDigest = Bytes.toBytes(sDigest);
System.out.println("md5 digest as string length: " + sbDigest.length);    // returns 26
        </programlisting>
        <para>
            不幸的是，使用二进制类型来表示数据将会让你的数据在代码以外很难阅读。例如，当你增加一个值时，这就是你会在shell看到的：
            </para>
        <programlisting>
hbase(main):001:0> incr 't', 'r', 'f:q', 1
COUNTER VALUE = 1

hbase(main):002:0> get 't', 'r'
COLUMN                                        CELL
 f:q                                          timestamp=1369163040570, value=\x00\x00\x00\x00\x00\x00\x00\x01
1 row(s) in 0.0310 seconds
        </programlisting>
        <para>
            shell在尽最大努力打印一个字符串，并且在这种情况下它只能以16进制打印。同样的情况会发生在你region名中的行键上。
            如果你知道存储的是什么，那还好些。但是如果在同一个单元格里可以存放任意形式，那它也可能是不可读的。这就需要进行权衡。
            </para>
      </section>

    </section>
    <section
      xml:id="reverse.timestamp">
      <title>倒序时间戳</title>
      <note>
        <title>逆序Scan的API</title>
        <para>
            <link
                    xlink:href="https://issues.apache.org/jira/browse/HBASE-4811">HBASE-4811</link>实现了一个API，可以对一个表或一个表中的某一范围进行逆序遍历，而你不需要为了实现正向或反向遍历而优化你的设计模式。该特征在HBase
            0.98和更高的版本中可用。查看<link
                xlink:href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html#setReversed%28boolean" /> 以获取更多信息。
           </para>
      </note>

      <para>
          在数据处理过程中快速查找一个值的最近版本是一个通用的问题。 采用倒序的时间戳作为行键的一部分对某些特定情况的该问题会有所帮助。在Tom White的书Hadoop权威指南(Hadoop:The Definitive Guide)(O'Reilly出版社出版)的关于HBase的章节中可以找到这种例子，它采用了将(<code>Long.MAX_VALUE - timestamp</code>)添加到行键的最后，例如[key][reverse_timestamp]。
     </para>
      <para>
          在执行一个[key]的Scan操作时，该[key]的在表中最近的值（即时间戳最大的值）将会被找到并且获取第一个记录。因为HBase的行键是有序的，该行键排在比它更老的行键的前面，所以它会是第一个。
           </para>
      <para>该技术将会被用来代替 <xref
          linkend="schema.versions" /> 。其目的是“永远”(或一段很长时间)保存所有版本。同时，采用同样的Scan技术，可以很快获取到其他版本的数据。</para>
    </section>
    <section
      xml:id="rowkey.scope">
      <title>行键和列族</title>
      <para>
          行键在列族范围内。所以同样的行键可以在同一个表的每个列族中存在而不会冲突。
          </para>
    </section>
    <section
      xml:id="changing.rowkeys">
      <title>行键的不变性</title>
      <para>
          行键不能被改变。唯一可以“改变”行键的方式是先删除然后再插入。这是一个常见的问题，所以要注意最开始(和/或在插入很多数据之前)就保证行键是正确的。
           </para>
    </section>
    <section
      xml:id="rowkey.regionsplits">
      <title>行键和分区(Region)切割的关系</title>
      <para>
          如果你的表执行了预切割，那理解行键是如何跨越界限进行分区对你就很 <emphasis>重要</emphasis> 。
          作为解释这为什么很重要的一个例子，考虑在开头使用16进制字符的行键（例如&quot;0000000000000000&quot;到
          &quot;ffffffffffffffff&quot;）的实例。对这些行键执行<code>Bytes.split</code> （它在使用<code>HBaseAdmin.createTable(byte[] startKey, byte[] endKey, numRegions)</code>创建分区的时候就会进行分割操作）操作，将它们分割成10个分区，如下：
          </para>
      <screen>
48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48                                // 0
54 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10                 // 6
61 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -68                 // =
68 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -126  // D
75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 72                                // K
82 18 18 18 18 18 18 18 18 18 18 18 18 18 18 14                                // R
88 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -44                 // X
95 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -102                // _
102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102                // f
      </screen>
      <para>... (
          注意：最开始的字节在右边的评论中列了出来
          ) 鉴于第一次分割是'0'，最后一次分割是'f'，一切都很正常，是吗？还没这么快。 </para>
      <para>
          问题是所有的数据都会跑到最前面的两个分区和最后的分区中，因此会产生一个热点分区问题。为了理解为什么，参考
          <link
                  xlink:href="http://www.asciitable.com">ASCII表</link>。'0'是48的字节表示，'f'是102的字节表示，但是
          这里存在一个字节值的巨大跨越（字节58到96），它们<emphasis>从来没有在行键中出现过</emphasis>，因为仅有的值是
          [0-9]和[a-f]。因此，中间的分区将从不会被使用。为了对该例子中的行键执行预分区操作，需要自己定义分割（例如，不依赖内置的分割方法）。
          </para>
      <para>经验1: 预分割表通常是最好的做法，但是你需要保证预分割之后所有的分区可以访问所有的行键。在这个使用了16进制行键的例子中，就有一些问题。相同的问题也可以发生在<emphasis>任何</emphasis>行键中。你需要了解你的数据。 </para>
      <para>经验2: 虽然通常不建议使用16进制的行键（和可显示的数据），但是它也可以对表进行预分区，只需要所有被创建的分区都能访问所有的行键即可。 </para>
      <para>作为该示例的总结，下面展示了如何对16进制的行键进行预分割分区的一个例子： </para>
      <programlisting language="java"><![CDATA[public static boolean createTable(HBaseAdmin admin, HTableDescriptor table, byte[][] splits)
throws IOException {
  try {
    admin.createTable( table, splits );
    return true;
  } catch (TableExistsException e) {
    logger.info("table " + table.getNameAsString() + " already exists");
    // the table already exists...
    return false;
  }
}

public static byte[][] getHexSplits(String startKey, String endKey, int numRegions) {
  byte[][] splits = new byte[numRegions-1][];
  BigInteger lowestKey = new BigInteger(startKey, 16);
  BigInteger highestKey = new BigInteger(endKey, 16);
  BigInteger range = highestKey.subtract(lowestKey);
  BigInteger regionIncrement = range.divide(BigInteger.valueOf(numRegions));
  lowestKey = lowestKey.add(regionIncrement);
  for(int i=0; i < numRegions-1;i++) {
    BigInteger key = lowestKey.add(regionIncrement.multiply(BigInteger.valueOf(i)));
    byte[] b = String.format("%016x", key).getBytes();
    splits[i] = b;
  }
  return splits;
}]]></programlisting>
    </section>
  </section>
  <!--  rowkey design -->
  <section
    xml:id="schema.versions">
    <title> 版本数 </title>
    <section
      xml:id="schema.versions.max">
      <title>最大版本数</title>
      <para>
          行的最大版本数目是通过<link
              xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html">HColumnDescriptor</link>对每一个列族单独配置的。默认的最大版本数是1.这个参数很重要，在<xref
              linkend="datamodel" />中进行了描述。HBase<emphasis>不会</emphasis>覆盖行值，但是每行的不同时间（和不同列）存储的值不同。多余的版本会在major压缩时会移除。最大版本数可以根据应用的需要来增加或者减少。
     </para>
      <para>
          不推荐将版本最大值设置的很大(例如, 成百或更多)，除非老数据对你很重要。因为这会大大增加StoreFile的大小。
           </para>
    </section>
    <section
      xml:id="schema.minversions">
      <title> 最小版本数 </title>
      <para>
          和行的最大版本数一样，最小版本数也是通过<link
              xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html">HColumnDescriptor</link> 对每个列族中单独配置的。默认的最小版本数是0，表示该特性被禁用。 参数最小版本数和参数存活时间一起使用，允许配置如“保存最后T分钟的有价值的数据，最多N个版本，<emphasis>但最少大约M个版本</emphasis>”(M是最小版本数，M&lt;N)。 该参数仅在存活时间对列族启用，且必须小于行版本数。
     </para>
    </section>
  </section>
  <section
    xml:id="supported.datatypes">
    <title> 支持的数据类型 </title>
    <para>
        HBase通过<link
            xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Put.html">Put</link>和 <link
            xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Result.html">Result</link>支持"bytes-in/bytes-out"接口，所以任何可被转为字节数组的数据都可以作为值被存入。输入可以是字符串，数字，复杂对象，甚至是图像，只要他们能转化为字节。
     </para>
    <para>
        值的实际长度存在限制(例如，保存10-50MB对象到HBase中对查询操作来说太长); 搜索邮件列表可以获取关于本话题的对话。 HBase的所有行都遵循<xref
            linkend="datamodel" />, 包括版本。设计时需考虑到这些，也需要考虑列族的块大小。
    </para>

    <section
      xml:id="counters">
      <title>计数器</title>
      <para>
          HBase支持的数据类型“计数器”(例如, 具有原子递增数字的能力)值得一提。 参考HTable的<link
              xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#increment%28org.apache.hadoop.hbase.client.Increment%29">Increment</link>。
           </para>
      <para>
          计数器的同步在RegionServer中完成，而不是客户端。 </para>
    </section>
  </section>
  <section
    xml:id="schema.joins">
    <title>Joins</title>
    <para>
        如果你有多个表格，在进行模式设计时，不要忘了潜在的<xref
            linkend="joins" />因素。 </para>
  </section>
  <section
    xml:id="ttl">
    <title>存活时间 (TTL)</title>
    <para>
        列族可以设置TTL秒数，HBase在超时后将自动删除数据。影响一行的 <emphasis>全部</emphasis> 版本 - 甚至当前版本。HBase里面TTL时间是UTC时间.
         </para>
    <para>查看 <link
        xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html">HColumnDescriptor</link>
      可以获得更多的信息。 </para>
  </section>
  <section
    xml:id="cf.keep.deleted">
    <title> 保留被删除的单元格 </title>
    <para>
        列族可以选择性的保留被删除的单元格。这就是说<link
            xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Get.html">Get</link>或 <link
            xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html">Scan</link>操作仍可以获取被删除的单元格。只要这些操作都指定了时间范围，且在被删除的单元格生效的时间之前。
         </para>
    <para>
        被删除的单元格依然受TTL的影响，并且永远不会超过“最大版本数”。一个新的“原始”遍历(scan)返回所有被删除的行和删除标记。
         </para>
    <para>查看 <link
        xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html">HColumnDescriptor</link>
      可以获取到更多信息。 </para>
  </section>
  <section
    xml:id="secondary.indexes">
    <title> 二级索引和备用查询路径 </title>
    <para>
        本节也可以使用标题“如果我的表的行键看起来是<emphasis>这样</emphasis>但是我却想要像<emphasis>那样</emphasis>来查询表”。
        一个常见的例子是行键的格式是"用户-时间戳"，但是也有查询一个给定的时间范围内都有哪些用户的需求。因此，通过用户进行查询很容易，因为行键的开头部分是用户，但通过时间查询就不容易了。
         </para>
    <para>在这点的处理上，没有一个最好的方式，因为它依赖于： </para>
    <itemizedlist>
      <listitem>
        <para>用户数</para>
      </listitem>
      <listitem>
        <para>数据大小和数据到达率</para>
      </listitem>
      <listitem>
        <para>需求的灵活性（例如，完成一个临时的日期查询或预先指定范围的日期查询） </para>
      </listitem>
      <listitem>
        <para>期望的查询执行速度（例如，对于一些临时性的需求，90秒可能是合理的，但对于其他一些需求就可能显得太长） </para>
      </listitem>
    </itemizedlist>
    <para>... 并且，解决方案也可能会受到集群大小和处理能力等的影响。下面的小节有一些常用的技术。这是一个综合的，但不详尽的方法列表。 </para>
    <para>
        它不应该是一个惊喜，二级索引需要额外的集群空间和额外的处理。这些事情在关系型数据库管理系统中也同样会发生。因为创建一个可替代的索引需要空间和处理周期更新。关系型数据库管理系统在这方面更先进一些，因为对可替代索引的管理工作开箱即用。然而，HBase在数据规模很大的情况下表现更好，这是一个功能上的权衡。
     </para>
    <para>当实现这些方法中的任意一个的时候，请注意 <xref
        linkend="performance" /> 。</para>
    <para>此外，可以参考David Butler在回应用户邮件列表中的 <link
        xlink:href="http://search-hadoop.com/m/nvbiBp2TDP/Stargate%252Bhbase&amp;subj=Stargate+hbase">HBase,
        mail # user - Stargate+hbase</link>的跟帖。
    </para>
    <section
      xml:id="secondary.indexes.filter">
      <title> 过滤查询 </title>
      <para>
          根据具体应用，可以适当的使用<xref  linkend="client.filter" />。在这种情况下，没有创建二级索引。然而，不要试图从一个应用（例如，单线程客户端）对一个大表进行全表遍历。
           </para>
    </section>
    <section
      xml:id="secondary.indexes.periodic">
      <title> 定期更新二级索引 </title>
      <para>
          二级索引可以在另一个表中创建，可以通过MapReduce任务进行定期更新。这个任务可以在当天执行，但是依赖于加载策略，可能会与主表不同步。
          </para>
      <para>查看 <xref
          linkend="mapreduce.example.readwrite" /> 可以了解更多相关信息。</para>
    </section>
    <section
      xml:id="secondary.indexes.dualwrite">
      <title> 同时写入二级索引 </title>
      <para>
          另外一个策略是在给集群写入数据的时候同时写入二级索引（例如，既写入数据表也写入索引表）。如果这个方法在数据表已经存在之后才采用，就需要使用MapReduce任务(参考 <xref
              linkend="secondary.indexes.periodic" />)来对已有的数据生成索引。
          </para>
    </section>
    <section
      xml:id="secondary.indexes.summary">
      <title> 汇总表 </title>
      <para>
          对时间跨度长(例如,年报)和数据量巨大，通常使用汇总表。可通过MapReduce任务将结果生成到另一个表。
          </para>
      <para>See <xref
          linkend="mapreduce.example.summary" /> for more information.</para>
    </section>
    <section
      xml:id="secondary.indexes.coproc">
      <title> 协处理器二级索引 </title>
      <para> 协处理器就像关系型数据库管理系统中的触发器。该功能在0.92版本中被加入。更多的有关信息，可以参考 <xref
          linkend="coprocessors" /> 。
      </para>
    </section>
  </section>
  <section
    xml:id="constraints">
    <title>约束</title>
    <para>
        HBase的当前支持传统的（SQL）数据库中的说法“约束”。在表中的属性执行商业规则（例如，确保值在范围1-10以内）时建议使用约束。约束也可以用于强制引用的完整性，但是强烈反对这么做。
        因为完整性检查一旦启用将极大的降低表的吞吐量。使用约束的丰富文档可以在这里找到： <link
        xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/constraint">Constraint</link>。
      约束在0.94版本中被加入。 </para>
  </section>
  <section
    xml:id="schema.casestudies">
    <title>模式设计案例研究</title>
    <para>
        下面将描述一些典型的使用了HBase的数据获取的例子，以及如何进行行键的设计和构建。注：这只是潜在方法的举例说明，并非详尽无遗的方法列表。你需要了解你的数据和你的处理需求。
     </para>
    <para>强烈推荐你在阅读这些案例研究之前，先阅读 <xref
            linkend="schema" />的其他部分。 </para>
    <para>下面是有关这些案例研究的一些描述: </para>
    <itemizedlist>
      <listitem>
        <para>日志数据 / 时间序列数据</para>
      </listitem>
      <listitem>
        <para>日志数据 / 疯狂增长的时间序列数据</para>
      </listitem>
      <listitem>
        <para>客户/订单</para>
      </listitem>
      <listitem>
        <para>高/宽/中 模式设计</para>
      </listitem>
      <listitem>
        <para>列表数据</para>
      </listitem>
    </itemizedlist>
    <section
      xml:id="schema.casestudies.log-timeseries">
      <title>案例研究 - 日志数据和时间序列数据</title>
      <para>假设收集了下面的数据元素。 </para>
      <itemizedlist>
        <listitem>
          <para>主机名（Hostname）</para>
        </listitem>
        <listitem>
          <para>时间戳（Timestamp）</para>
        </listitem>
        <listitem>
          <para>日志事件（Log event）</para>
        </listitem>
        <listitem>
          <para>值/消息（Value/message）</para>
        </listitem>
      </itemizedlist>
      <para>
          我们可以将这些数据存在HBase中一个叫做LOG_DATA的表中，但是行键应该是什么呢？从这些属性中我们知道行键将会是域名、时间戳以及日志事件的某种组合，但具体是什么呢？
           </para>
      <section
        xml:id="schema.casestudies.log-timeseries.tslead">
        <title>时间戳在行键的起始位置</title>
        <para>行键 <code>[时间戳][主机名][日志事件]</code> 将会引起在<xref
                linkend="timeseries" />中提到的单调递增的行键问题。 </para>
        <para>
            还有另外的一个设计，就是通过对时间戳进行一个取余操作，使用“成组(bucketing)”的时间戳。无否时间顺序的遍历操作很重要，这可能会是一个有用的方法。注意必须指定好组(bucket)的大小，因为在遍历返回值时，同样需要这个大小。
            </para>
        <programlisting language="java">
long bucket = timestamp % numBuckets;
        </programlisting>
        <para>… 以构建：</para>
        <programlisting>
[bucket][timestamp][hostname][log-event]
        </programlisting>
        <para>
            如上所述，查询一个特定时间范围的数据，遍历（Scan）操作需要在每一个组中进行。例如，有100个组，将会把行键分布的很广泛（而不会造成热点现象），但是对于一个单独的时间戳，它将需要执行100次遍历操作，因此需要进行权衡。 </para>
      </section>
      <!-- ts lead -->
      <section
        xml:id="schema.casestudies.log-timeseries.hostlead">
        <title>主机名放在行键的起始位置</title>
        <para>如果有很多的主机来将写和读分布到所有的行键上，那行键 <code>[主机名][日志事件][时间戳]</code> 是一个备选项。如果优先使用主机名来遍历数据，那这个方法会很有用。 </para>
      </section>
      <!--  host lead -->
      <section
        xml:id="schema.casestudies.log-timeseries.revts">
        <title>时间戳，或逆序的时间戳？</title>
        <para>
            在大多数的访问操作中，可能需要访问最近的事件，那么应该将时间戳以逆序进行存储（例如，<code>timestamp = Long.MAX_VALUE –
            timestamp</code>）。使用逆序的时间戳可以对<code>[hostname][log-event]</code> 执行遍历操作并快速获取最近捕获到的事件。
             </para>
        <para>没有方法是错误的，这仅仅依赖于什么情况下最适合。 </para>
        <note>
          <title>逆序Scan API</title>
          <para>
              <link
                      xlink:href="https://issues.apache.org/jira/browse/HBASE-4811">HBASE-4811</link>实现了一个API，可以对一个表或一个表中的某一范围进行逆序遍历，而你不需要为了实现正向或反向遍历而优化你的设计模式。该特征在HBase 0.98和更高的版本中可用。查看<link
                  xlink:href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html#setReversed%28boolean" /> 以获取更多信息。
            </para>
        </note>
      </section>
      <!--  revts -->
      <section
        xml:id="schema.casestudies.log-timeseries.varkeys">
        <title>可变长度或固定长度的行键？</title>
        <para>
            HBase的行键的每一列都很重要。如果主机名是"a"并且事件类型是"e1"，那么组成的行键将会很小。但是，如果存储主机名是"myserver1.mycompany.com"，事件类型是
            "com.package1.subpackage2.subsubpackage3.ImportantService"呢？
             </para>
        <para>
            在行键中使用某些替代值是很有必要的。有至少两种方法：哈希和数字化。在主机名在行键的起始位置的例子中，它可能看起来是这个样子：
             </para>
        <para>使用哈希来组成行键：</para>
        <itemizedlist>
          <listitem>
            <para>[主机名的MD5哈希值] = 16 bytes</para>
          </listitem>
          <listitem>
            <para>[事件类型的MD5哈希值] = 16 bytes</para>
          </listitem>
          <listitem>
            <para>[时间戳] = 8 bytes</para>
          </listitem>
        </itemizedlist>
        <para>使用替代数字来组成行键： </para>
        <para>对于这种方法，LOG_DATA需要一个额外的查询表LOG_TYPES。LOG_TYPES的行键将是：
             </para>
        <itemizedlist>
          <listitem>
            <para>[type] (例如，, 表示主机名和事件类型的字节)</para>
          </listitem>
          <listitem>
            <para>[bytes] 原始主机名或事件类型的可变长度的字节</para>
          </listitem>
        </itemizedlist>
        <para>
            这个行键的列可能是一个分配的长整型的数字，它可以通过使用<link
                xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#incrementColumnValue%28byte[],%20byte[],%20byte[],%20long%29">HBase
            计数器</link>获取。
             </para>
        <para>因此组成的行键将是： </para>
        <itemizedlist>
          <listitem>
            <para>[替代主机名的长整型] = 8 bytes</para>
          </listitem>
          <listitem>
            <para>[替代事件类型的长整型] = 8 bytes</para>
          </listitem>
          <listitem>
            <para>[时间戳] = 8 bytes</para>
          </listitem>
        </itemizedlist>
        <para>
            在任一哈希或数字替换的方法中，主机名和事件类型的原始值都可以作为一列存储。
            </para>
      </section>
      <!--  varkeys -->
    </section>
    <!--  log data and timeseries -->
    <section
      xml:id="schema.casestudies.log-steroids">
      <title>案例研究 - 日志数据和疯狂增长的时间序列数据</title>
      <para>
          这实际上是OpenTSDB采用的方法。 What OpenTSDB does is re-write data and pack
        rows into columns for certain time-periods. For a detailed explanation, see: <link
          xlink:href="http://opentsdb.net/schema.html">http://opentsdb.net/schema.html</link>, and <link
          xlink:href="http://www.cloudera.com/content/cloudera/en/resources/library/hbasecon/video-hbasecon-2012-lessons-learned-from-opentsdb.html">Lessons
          Learned from OpenTSDB</link> from HBaseCon2012. </para>
      <para>But this is how the general concept works: data is ingested, for example, in this
        manner…</para>
      <screen>
[hostname][log-event][timestamp1]
[hostname][log-event][timestamp2]
[hostname][log-event][timestamp3]
        </screen>
      <para>… with separate rowkeys for each detailed event, but is re-written like this… </para>
      <screen>[hostname][log-event][timerange]</screen>
      <para>… and each of the above events are converted into columns stored with a time-offset
        relative to the beginning timerange (e.g., every 5 minutes). This is obviously a very
        advanced processing technique, but HBase makes this possible. </para>
    </section>
    <!--  log data timeseries steroids -->

    <section
      xml:id="schema.casestudies.custorder">
      <title>Case Study - Customer/Order</title>
      <para>Assume that HBase is used to store customer and order information. There are two core
        record-types being ingested: a Customer record type, and Order record type. </para>
      <para>The Customer record type would include all the things that you’d typically expect: </para>
      <itemizedlist>
        <listitem>
          <para>Customer number</para>
        </listitem>
        <listitem>
          <para>Customer name</para>
        </listitem>
        <listitem>
          <para>Address (e.g., city, state, zip)</para>
        </listitem>
        <listitem>
          <para>Phone numbers, etc.</para>
        </listitem>
      </itemizedlist>
      <para>The Order record type would include things like: </para>
      <itemizedlist>
        <listitem>
          <para>Customer number</para>
        </listitem>
        <listitem>
          <para>Order number</para>
        </listitem>
        <listitem>
          <para>Sales date</para>
        </listitem>
        <listitem>
          <para>A series of nested objects for shipping locations and line-items (see <xref
              linkend="schema.casestudies.custorder.obj" /> for details)</para>
        </listitem>
      </itemizedlist>
      <para>Assuming that the combination of customer number and sales order uniquely identify an
        order, these two attributes will compose the rowkey, and specifically a composite key such
        as: </para>
      <screen>[customer number][order number]</screen>
      <para>… for a ORDER table. However, there are more design decisions to make: are the
          <emphasis>raw</emphasis> values the best choices for rowkeys? </para>
      <para>The same design questions in the Log Data use-case confront us here. What is the
        keyspace of the customer number, and what is the format (e.g., numeric? alphanumeric?) As it
        is advantageous to use fixed-length keys in HBase, as well as keys that can support a
        reasonable spread in the keyspace, similar options appear: </para>
      <para>Composite Rowkey With Hashes: </para>
      <itemizedlist>
        <listitem>
          <para>[MD5 of customer number] = 16 bytes</para>
        </listitem>
        <listitem>
          <para>[MD5 of order number] = 16 bytes</para>
        </listitem>
      </itemizedlist>
      <para>Composite Numeric/Hash Combo Rowkey: </para>
      <itemizedlist>
        <listitem>
          <para>[substituted long for customer number] = 8 bytes</para>
        </listitem>
        <listitem>
          <para>[MD5 of order number] = 16 bytes</para>
        </listitem>
      </itemizedlist>
      <section
        xml:id="schema.casestudies.custorder.tables">
        <title>Single Table? Multiple Tables?</title>
        <para>A traditional design approach would have separate tables for CUSTOMER and SALES.
          Another option is to pack multiple record types into a single table (e.g., CUSTOMER++). </para>
        <para>Customer Record Type Rowkey: </para>
        <itemizedlist>
          <listitem>
            <para>[customer-id]</para>
          </listitem>
          <listitem>
            <para>[type] = type indicating ‘1’ for customer record type</para>
          </listitem>
        </itemizedlist>
        <para>Order Record Type Rowkey: </para>
        <itemizedlist>
          <listitem>
            <para>[customer-id]</para>
          </listitem>
          <listitem>
            <para>[type] = type indicating ‘2’ for order record type</para>
          </listitem>
          <listitem>
            <para>[order]</para>
          </listitem>
        </itemizedlist>
        <para>The advantage of this particular CUSTOMER++ approach is that organizes many different
          record-types by customer-id (e.g., a single scan could get you everything about that
          customer). The disadvantage is that it’s not as easy to scan for a particular record-type.
        </para>
      </section>
      <section
        xml:id="schema.casestudies.custorder.obj">
        <title>Order Object Design</title>
        <para>Now we need to address how to model the Order object. Assume that the class structure
          is as follows:</para>
        <variablelist>
          <varlistentry>
            <term>Order</term>
            <listitem>
              <para>(an Order can have multiple ShippingLocations</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>LineItem</term>
            <listitem>
              <para>(a ShippingLocation can have multiple LineItems</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>... there are multiple options on storing this data. </para>
        <section
          xml:id="schema.casestudies.custorder.obj.norm">
          <title>Completely Normalized</title>
          <para>With this approach, there would be separate tables for ORDER, SHIPPING_LOCATION, and
            LINE_ITEM. </para>
          <para>The ORDER table's rowkey was described above: <xref
              linkend="schema.casestudies.custorder" />
          </para>
          <para>The SHIPPING_LOCATION's composite rowkey would be something like this: </para>
          <itemizedlist>
            <listitem>
              <para>[order-rowkey]</para>
            </listitem>
            <listitem>
              <para>[shipping location number] (e.g., 1st location, 2nd, etc.)</para>
            </listitem>
          </itemizedlist>
          <para>The LINE_ITEM table's composite rowkey would be something like this: </para>
          <itemizedlist>
            <listitem>
              <para>[order-rowkey]</para>
            </listitem>
            <listitem>
              <para>[shipping location number] (e.g., 1st location, 2nd, etc.)</para>
            </listitem>
            <listitem>
              <para>[line item number] (e.g., 1st lineitem, 2nd, etc.)</para>
            </listitem>
          </itemizedlist>
          <para>Such a normalized model is likely to be the approach with an RDBMS, but that's not
            your only option with HBase. The cons of such an approach is that to retrieve
            information about any Order, you will need: </para>
          <itemizedlist>
            <listitem>
              <para>Get on the ORDER table for the Order</para>
            </listitem>
            <listitem>
              <para>Scan on the SHIPPING_LOCATION table for that order to get the ShippingLocation
                instances</para>
            </listitem>
            <listitem>
              <para>Scan on the LINE_ITEM for each ShippingLocation</para>
            </listitem>
          </itemizedlist>
          <para>... granted, this is what an RDBMS would do under the covers anyway, but since there
            are no joins in HBase you're just more aware of this fact. </para>
        </section>
        <section
          xml:id="schema.casestudies.custorder.obj.rectype">
          <title>Single Table With Record Types</title>
          <para>With this approach, there would exist a single table ORDER that would contain </para>
          <para>The Order rowkey was described above: <xref
              linkend="schema.casestudies.custorder" /></para>
          <itemizedlist>
            <listitem>
              <para>[order-rowkey]</para>
            </listitem>
            <listitem>
              <para>[ORDER record type]</para>
            </listitem>
          </itemizedlist>
          <para>The ShippingLocation composite rowkey would be something like this: </para>
          <itemizedlist>
            <listitem>
              <para>[order-rowkey]</para>
            </listitem>
            <listitem>
              <para>[SHIPPING record type]</para>
            </listitem>
            <listitem>
              <para>[shipping location number] (e.g., 1st location, 2nd, etc.)</para>
            </listitem>
          </itemizedlist>
          <para>The LineItem composite rowkey would be something like this: </para>
          <itemizedlist>
            <listitem>
              <para>[order-rowkey]</para>
            </listitem>
            <listitem>
              <para>[LINE record type]</para>
            </listitem>
            <listitem>
              <para>[shipping location number] (e.g., 1st location, 2nd, etc.)</para>
            </listitem>
            <listitem>
              <para>[line item number] (e.g., 1st lineitem, 2nd, etc.)</para>
            </listitem>
          </itemizedlist>
        </section>
        <section
          xml:id="schema.casestudies.custorder.obj.denorm">
          <title>Denormalized</title>
          <para>A variant of the Single Table With Record Types approach is to denormalize and
            flatten some of the object hierarchy, such as collapsing the ShippingLocation attributes
            onto each LineItem instance. </para>
          <para>The LineItem composite rowkey would be something like this: </para>
          <itemizedlist>
            <listitem>
              <para>[order-rowkey]</para>
            </listitem>
            <listitem>
              <para>[LINE record type]</para>
            </listitem>
            <listitem>
              <para>[line item number] (e.g., 1st lineitem, 2nd, etc. - care must be taken that
                there are unique across the entire order)</para>
            </listitem>
          </itemizedlist>
          <para>... and the LineItem columns would be something like this: </para>
          <itemizedlist>
            <listitem>
              <para>itemNumber</para>
            </listitem>
            <listitem>
              <para>quantity</para>
            </listitem>
            <listitem>
              <para>price</para>
            </listitem>
            <listitem>
              <para>shipToLine1 (denormalized from ShippingLocation)</para>
            </listitem>
            <listitem>
              <para>shipToLine2 (denormalized from ShippingLocation)</para>
            </listitem>
            <listitem>
              <para>shipToCity (denormalized from ShippingLocation)</para>
            </listitem>
            <listitem>
              <para>shipToState (denormalized from ShippingLocation)</para>
            </listitem>
            <listitem>
              <para>shipToZip (denormalized from ShippingLocation)</para>
            </listitem>
          </itemizedlist>
          <para>The pros of this approach include a less complex object heirarchy, but one of the
            cons is that updating gets more complicated in case any of this information changes.
          </para>
        </section>
        <section
          xml:id="schema.casestudies.custorder.obj.singleobj">
          <title>Object BLOB</title>
          <para>With this approach, the entire Order object graph is treated, in one way or another,
            as a BLOB. For example, the ORDER table's rowkey was described above: <xref
              linkend="schema.casestudies.custorder" />, and a single column called "order" would
            contain an object that could be deserialized that contained a container Order,
            ShippingLocations, and LineItems. </para>
          <para>There are many options here: JSON, XML, Java Serialization, Avro, Hadoop Writables,
            etc. All of them are variants of the same approach: encode the object graph to a
            byte-array. Care should be taken with this approach to ensure backward compatibilty in
            case the object model changes such that older persisted structures can still be read
            back out of HBase. </para>
          <para>Pros are being able to manage complex object graphs with minimal I/O (e.g., a single
            HBase Get per Order in this example), but the cons include the aforementioned warning
            about backward compatiblity of serialization, language dependencies of serialization
            (e.g., Java Serialization only works with Java clients), the fact that you have to
            deserialize the entire object to get any piece of information inside the BLOB, and the
            difficulty in getting frameworks like Hive to work with custom objects like this.
          </para>
        </section>
      </section>
      <!--  cust/order order object -->
    </section>
    <!--  cust/order -->

    <section
      xml:id="schema.smackdown">
      <title>Case Study - "Tall/Wide/Middle" Schema Design Smackdown</title>
      <para>This section will describe additional schema design questions that appear on the
        dist-list, specifically about tall and wide tables. These are general guidelines and not
        laws - each application must consider its own needs. </para>
      <section
        xml:id="schema.smackdown.rowsversions">
        <title>Rows vs. Versions</title>
        <para>A common question is whether one should prefer rows or HBase's built-in-versioning.
          The context is typically where there are "a lot" of versions of a row to be retained
          (e.g., where it is significantly above the HBase default of 1 max versions). The
          rows-approach would require storing a timestamp in some portion of the rowkey so that they
          would not overwite with each successive update. </para>
        <para>Preference: Rows (generally speaking). </para>
      </section>
      <section
        xml:id="schema.smackdown.rowscols">
        <title>Rows vs. Columns</title>
        <para>Another common question is whether one should prefer rows or columns. The context is
          typically in extreme cases of wide tables, such as having 1 row with 1 million attributes,
          or 1 million rows with 1 columns apiece. </para>
        <para>Preference: Rows (generally speaking). To be clear, this guideline is in the context
          is in extremely wide cases, not in the standard use-case where one needs to store a few
          dozen or hundred columns. But there is also a middle path between these two options, and
          that is "Rows as Columns." </para>
      </section>
      <section
        xml:id="schema.smackdown.rowsascols">
        <title>Rows as Columns</title>
        <para>The middle path between Rows vs. Columns is packing data that would be a separate row
          into columns, for certain rows. OpenTSDB is the best example of this case where a single
          row represents a defined time-range, and then discrete events are treated as columns. This
          approach is often more complex, and may require the additional complexity of re-writing
          your data, but has the advantage of being I/O efficient. For an overview of this approach,
          see <xref
            linkend="schema.casestudies.log-steroids" />. </para>
      </section>
    </section>
    <!--  note:  the following id is not consistent with the others becaus it was formerly in the Case Studies chapter,
	    but I didn't want to break backward compatibility of the link.  But future entries should look like the above case-study
	    links (schema.casestudies. ...)  -->
    <section
      xml:id="casestudies.schema.listdata">
      <title>Case Study - List Data</title>
      <para>The following is an exchange from the user dist-list regarding a fairly common question:
        how to handle per-user list data in Apache HBase. </para>
      <para>*** QUESTION ***</para>
      <para> We're looking at how to store a large amount of (per-user) list data in HBase, and we
        were trying to figure out what kind of access pattern made the most sense. One option is
        store the majority of the data in a key, so we could have something like: </para>

      <programlisting><![CDATA[
<FixedWidthUserName><FixedWidthValueId1>:"" (no value)
<FixedWidthUserName><FixedWidthValueId2>:"" (no value)
<FixedWidthUserName><FixedWidthValueId3>:"" (no value)
]]></programlisting>

      <para>The other option we had was to do this entirely using:</para>
      <programlisting language="xml"><![CDATA[
<FixedWidthUserName><FixedWidthPageNum0>:<FixedWidthLength><FixedIdNextPageNum><ValueId1><ValueId2><ValueId3>...
<FixedWidthUserName><FixedWidthPageNum1>:<FixedWidthLength><FixedIdNextPageNum><ValueId1><ValueId2><ValueId3>...
    		]]></programlisting>
      <para> where each row would contain multiple values. So in one case reading the first thirty
        values would be: </para>
      <programlisting language="java"><![CDATA[
scan { STARTROW => 'FixedWidthUsername' LIMIT => 30}
    		]]></programlisting>
      <para>And in the second case it would be </para>
      <programlisting>
get 'FixedWidthUserName\x00\x00\x00\x00'
    		</programlisting>
      <para> The general usage pattern would be to read only the first 30 values of these lists,
        with infrequent access reading deeper into the lists. Some users would have &lt;= 30 total
        values in these lists, and some users would have millions (i.e. power-law distribution) </para>
      <para> The single-value format seems like it would take up more space on HBase, but would
        offer some improved retrieval / pagination flexibility. Would there be any significant
        performance advantages to be able to paginate via gets vs paginating with scans? </para>
      <para> My initial understanding was that doing a scan should be faster if our paging size is
        unknown (and caching is set appropriately), but that gets should be faster if we'll always
        need the same page size. I've ended up hearing different people tell me opposite things
        about performance. I assume the page sizes would be relatively consistent, so for most use
        cases we could guarantee that we only wanted one page of data in the fixed-page-length case.
        I would also assume that we would have infrequent updates, but may have inserts into the
        middle of these lists (meaning we'd need to update all subsequent rows). </para>
      <para> Thanks for help / suggestions / follow-up questions. </para>
      <para>*** ANSWER ***</para>
      <para> If I understand you correctly, you're ultimately trying to store triples in the form
        "user, valueid, value", right? E.g., something like: </para>
      <programlisting>
"user123, firstname, Paul",
"user234, lastname, Smith"
			</programlisting>
      <para> (But the usernames are fixed width, and the valueids are fixed width). </para>
      <para> And, your access pattern is along the lines of: "for user X, list the next 30 values,
        starting with valueid Y". Is that right? And these values should be returned sorted by
        valueid? </para>
      <para> The tl;dr version is that you should probably go with one row per user+value, and not
        build a complicated intra-row pagination scheme on your own unless you're really sure it is
        needed. </para>
      <para> Your two options mirror a common question people have when designing HBase schemas:
        should I go "tall" or "wide"? Your first schema is "tall": each row represents one value for
        one user, and so there are many rows in the table for each user; the row key is user +
        valueid, and there would be (presumably) a single column qualifier that means "the value".
        This is great if you want to scan over rows in sorted order by row key (thus my question
        above, about whether these ids are sorted correctly). You can start a scan at any
        user+valueid, read the next 30, and be done. What you're giving up is the ability to have
        transactional guarantees around all the rows for one user, but it doesn't sound like you
        need that. Doing it this way is generally recommended (see here <link
          xlink:href="http://hbase.apache.org/book.html#schema.smackdown">http://hbase.apache.org/book.html#schema.smackdown</link>). </para>
      <para> Your second option is "wide": you store a bunch of values in one row, using different
        qualifiers (where the qualifier is the valueid). The simple way to do that would be to just
        store ALL values for one user in a single row. I'm guessing you jumped to the "paginated"
        version because you're assuming that storing millions of columns in a single row would be
        bad for performance, which may or may not be true; as long as you're not trying to do too
        much in a single request, or do things like scanning over and returning all of the cells in
        the row, it shouldn't be fundamentally worse. The client has methods that allow you to get
        specific slices of columns. </para>
      <para> Note that neither case fundamentally uses more disk space than the other; you're just
        "shifting" part of the identifying information for a value either to the left (into the row
        key, in option one) or to the right (into the column qualifiers in option 2). Under the
        covers, every key/value still stores the whole row key, and column family name. (If this is
        a bit confusing, take an hour and watch Lars George's excellent video about understanding
        HBase schema design: <link
          xlink:href="http://www.youtube.com/watch?v=_HLoH_PgrLk)">http://www.youtube.com/watch?v=_HLoH_PgrLk)</link>. </para>
      <para> A manually paginated version has lots more complexities, as you note, like having to
        keep track of how many things are in each page, re-shuffling if new values are inserted,
        etc. That seems significantly more complex. It might have some slight speed advantages (or
        disadvantages!) at extremely high throughput, and the only way to really know that would be
        to try it out. If you don't have time to build it both ways and compare, my advice would be
        to start with the simplest option (one row per user+value). Start simple and iterate! :)
      </para>
    </section>
    <!--  listdata -->

  </section>
  <!--  schema design cases -->
  <section
    xml:id="schema.ops">
    <title>Operational and Performance Configuration Options</title>
    <para>See the Performance section <xref
        linkend="perf.schema" /> for more information operational and performance schema design
      options, such as Bloom Filters, Table-configured regionsizes, compression, and blocksizes.
    </para>
  </section>

</chapter>
<!--  schema design -->
