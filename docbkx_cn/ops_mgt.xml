<?xml version="1.0" encoding="UTF-8"?>
<chapter
        version="5.0"
        xml:id="ops_mgt"
        xmlns="http://docbook.org/ns/docbook"
        xmlns:xlink="http://www.w3.org/1999/xlink"
        xmlns:xi="http://www.w3.org/2001/XInclude"
        xmlns:svg="http://www.w3.org/2000/svg"
        xmlns:m="http://www.w3.org/1998/Math/MathML"
        xmlns:html="http://www.w3.org/1999/xhtml"
        xmlns:db="http://docbook.org/ns/docbook">
    <!--
  /**
   * Licensed to the Apache Software Foundation (ASF) under one
   * or more contributor license agreements.  See the NOTICE file
   * distributed with this work forf additional information
   * regarding copyright ownership.  The ASF licenses this file
   * to you under the Apache License, Version 2.0 (the
   * "License"); you may not use this file except in compliance
   * with the License.  You may obtain a copy of the License at
   *
   *     http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   */
  -->
    <title>Apache HBase 运维管理</title>
    <para> 这一章将介绍运行HBase集群需要的一些运维工具和实践操作。
        讲述的主题与<xref
                linkend="trouble" />, <xref
                linkend="performance" />, 和 <xref
                linkend="configuration" /> 相关，但也有其独特性。</para>

    <section
            xml:id="tools">
        <title>HBase 工具和实用程序</title>

        <para>HBase 提供了一些工具，用来管理、分析和调试集群。
            其中大部分工具的入口点是 <filename>bin/hbase</filename>命令，有一些在 <filename>dev-support/</filename>目录。
        </para>
        <para> 通过无参或 带<option>-h</option> 运行<filename>bin/hbase</filename>命令，可以查看其使用说明。
            下面这些是HBase 0.98.x相关命令的使用说明。某些命令，如<command>version</command>, <command>pe</command>,
            <command>ltt</command>, <command>clean</command> 在之前的版本中并不可用。</para>
        <screen>
            $ <userinput>bin/hbase</userinput>
            <![CDATA[Usage: hbase [<options>] <command> [<args>]]]>
            Options:
            --config DIR    Configuration direction to use. Default: ./conf
            --hosts HOSTS   Override the list in 'regionservers' file

            Commands:
            Some commands take arguments. Pass no args or -h for usage.
            shell           Run the HBase shell
            hbck            Run the hbase 'fsck' tool
            hlog            Write-ahead-log analyzer
            hfile           Store file analyzer
            zkcli           Run the ZooKeeper shell
            upgrade         Upgrade hbase
            master          Run an HBase HMaster node
            regionserver    Run an HBase HRegionServer node
            zookeeper       Run a Zookeeper server
            rest            Run an HBase REST server
            thrift          Run the HBase Thrift server
            thrift2         Run the HBase Thrift2 server
            clean           Run the HBase clean up script
            classpath       Dump hbase CLASSPATH
            mapredcp        Dump CLASSPATH entries required by mapreduce
            pe              Run PerformanceEvaluation
            ltt             Run LoadTestTool
            version         Print the version
            CLASSNAME       Run the class named CLASSNAME
        </screen>
        <para>下面的一些工具和实用程序，是直接传递给<filename>bin/hbase</filename>命令的Java类，如使用说明最后一行所示。
            另外，  <command>hbase shell</command> (<xref linkend="shell"/>),
            <command>hbase upgrade</command> (<xref linkend="upgrading"/>), 和 <command>hbase
                thrift</command> (<xref linkend="thrift"/>) 记录在本指南的其它地方。</para>
        <section
                xml:id="canary">
            <title>Canary</title>
            <para> 这个Canary类能帮组用户canary-test  HBase集群状态，对每个region或regionserver的每个列镞。
                使用<literal>--help</literal>参数，可以查看具体用法。 </para>
            <screen language="bourne">$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.tool.Canary -help

                Usage: bin/hbase org.apache.hadoop.hbase.tool.Canary [opts] [table1 [table2]...] | [regionserver1 [regionserver2]..]
                where [opts] are:
                -help          Show this help and exit.
                -regionserver  replace the table argument to regionserver,
                which means to enable regionserver mode
                -daemon        Continuous check at defined intervals.
                -interval &lt;N>  Interval between checks (sec)
                -e             Use region/regionserver as regular expression
                which means the region/regionserver is regular expression pattern
                -f &lt;B>         stop whole program if first error occurs, default is true
                -t &lt;N>         timeout for a check, default is 600000 (milliseconds)</screen>
            <para>这个工具能返回非零错误代码，可用于与其他管理工具协作，比如Nagios。 错误代码定义如下： </para>
            <programlisting language="java">private static final int USAGE_EXIT_CODE = 1;
                private static final int INIT_ERROR_EXIT_CODE = 2;
                private static final int TIMEOUT_ERROR_EXIT_CODE = 3;
                private static final int ERROR_EXIT_CODE = 4;</programlisting>
            <para> 这里有些例子（基于以下给定的情况）， 两个名为test-01和test-02的HTable, 它们有两个列镞分别是cf1 和 cf2,
                部署在3个regionserver上。见下表： </para>

            <informaltable>
                <tgroup
                        cols="3"
                        align="center"
                        colsep="1"
                        rowsep="1">
                    <colspec
                            colname="regionserver"
                            align="center" />
                    <colspec
                            colname="test-01"
                            align="center" />
                    <colspec
                            colname="test-02"
                            align="center" />
                    <thead>
                        <row>
                            <entry>RegionServer</entry>
                            <entry>test-01</entry>
                            <entry>test-02</entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry>rs1</entry>
                            <entry>r1</entry>
                            <entry>r2</entry>
                        </row>
                        <row>
                            <entry>rs2</entry>
                            <entry>r2</entry>
                            <entry />
                        </row>
                        <row>
                            <entry>rs3</entry>
                            <entry>r2</entry>
                            <entry>r1</entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
            <para>基于以上情况，下面是一些例子。 </para>
            <section>
                <title>为每个表的每个region的每个列镞做Canary test</title>
                <screen language="bourne">$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.tool.Canary

                    3/12/09 03:26:32 INFO tool.Canary: read from region test-01,,1386230156732.0e3c7d77ffb6361ea1b996ac1042ca9a. column family cf1 in 2ms
                    13/12/09 03:26:32 INFO tool.Canary: read from region test-01,,1386230156732.0e3c7d77ffb6361ea1b996ac1042ca9a. column family cf2 in 2ms
                    13/12/09 03:26:32 INFO tool.Canary: read from region test-01,0004883,1386230156732.87b55e03dfeade00f441125159f8ca87. column family cf1 in 4ms
                    13/12/09 03:26:32 INFO tool.Canary: read from region test-01,0004883,1386230156732.87b55e03dfeade00f441125159f8ca87. column family cf2 in 1ms
                    ...
                    13/12/09 03:26:32 INFO tool.Canary: read from region test-02,,1386559511167.aa2951a86289281beee480f107bb36ee. column family cf1 in 5ms
                    13/12/09 03:26:32 INFO tool.Canary: read from region test-02,,1386559511167.aa2951a86289281beee480f107bb36ee. column family cf2 in 3ms
                    13/12/09 03:26:32 INFO tool.Canary: read from region test-02,0004883,1386559511167.cbda32d5e2e276520712d84eaaa29d84. column family cf1 in 31ms
                    13/12/09 03:26:32 INFO tool.Canary: read from region test-02,0004883,1386559511167.cbda32d5e2e276520712d84eaaa29d84. column family cf2 in 8ms
                </screen>
                <para> 可见，表test-01有两个region和两个列镞, 因此 Canary 工具从4个（2 region*2 store）不同的store中去了4小块数据，
                    这只是该工具的一种缺省行为。
                </para>
            </section>

            <section>
                <title>为指定表的每个region的每个列镞（store）做 Canary test</title>
                <para> 可以对一个或多个表检测：</para>
                <screen language="bourne">$ ${HBASE_HOME}/bin/hbase orghapache.hadoop.hbase.tool.Canary test-01 test-02</screen>
            </section>

            <section>
                <title> regionserver 粒度的Canary test </title>
                <para> 这种检测将从每个regionserver中取一小块数据，当然你也可以指定regionserver的名字作为参数进行canary-test。</para>
                <screen language="bourne">$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.tool.Canary -regionserver

                    13/12/09 06:05:17 INFO tool.Canary: Read from table:test-01 on region server:rs2 in 72ms
                    13/12/09 06:05:17 INFO tool.Canary: Read from table:test-02 on region server:rs3 in 34ms
                    13/12/09 06:05:17 INFO tool.Canary: Read from table:test-01 on region server:rs1 in 56ms</screen>
            </section>
            <section>
                <title> 正则表达式模式的Canary test </title>
                <para> 下面会同时检测表test-01和test-02.</para>
                <screen language="bourne">$ ${HBASE_HOME}/bin/hbase orghapache.hadoop.hbase.tool.Canary -e test-0[1-2]</screen>
            </section>

            <section>
                <title>以守护进程模式运行 canary test</title>
                <para> 按间隔时间重复运行的选项是-interval，默认时间是6秒。选项-f默认值为true,当出现任何错误时，守护进程将终止并返回非零错误代码。
                </para>
                <screen language="bourne">$ ${HBASE_HOME}/bin/hbase orghapache.hadoop.hbase.tool.Canary -daemon</screen>
                <para> 下面例子，定义重复运行的间隔时间为5秒，并且就算出错也不会停止运行。
                    the test.</para>
                <screen language="bourne">$ ${HBASE_HOME}/bin/hbase orghapache.hadoop.hbase.tool.Canary -daemon -interval 50000 -f false</screen>
            </section>

            <section>
                <title>如果canary test卡住将强制超时</title>
                <para>在某些情况下，我们会遇到请求阻塞在regionserver，无法给客户端返回响应的问题。
                    regionserver出现问题，通过Master也不能表明其已死（would also not indicated to be dead by
                    Master）,这将会导致客户端挂起。所以我们提供超时选项来强制终止canary test并返回非零错误代码。下面例子将超时时间设置为60秒，默认是600秒。
                </para>
                <screen language="bourne">$ ${HBASE_HOME}/bin/hbase orghapache.hadoop.hbase.tool.Canary -t 600000</screen>
            </section>

        </section>

        <section
                xml:id="health.check">
            <title>Health Checker</title>
            <para>你可以配置HBase在一段时间内运行一个脚本，如果失败了N次（可配置），则退出服务。配置及细节见 <link
                    xlink:href="">HBASE-7351 Periodic health check script</link>  </para>
        </section>

        <section
                xml:id="driver">
            <title>驱动</title>
            <para>一些频繁使用的工具以<code>驱动</code>类的方式提供，通过<filename>bin/hbase</filename>命令执行。
                这些工具是一些运行在你的集群上的MapReduce任务，它们按下面的方式运行，
                用你想运行的工具替换<replaceable>UtilityName</replaceable>。这个命令需要你将环境变量<literal>HBASE_HOME</literal>设置为HBase的安装目录。
              </para>
            <screen language="bourne">
                ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.mapreduce.<replaceable>UtilityName</replaceable>
            </screen>
            <para>有下列工具可以使用:</para>
            <variablelist>
                <varlistentry>
                    <term><command>LoadIncrementalHFiles</command></term>
                    <listitem><para>完成批量数据加载</para></listitem>
                </varlistentry>
                <varlistentry>
                    <term><command>CopyTable</command></term>
                    <listitem><para>从本地集群导出一个表到另一集群</para></listitem>
                </varlistentry>
                <varlistentry>
                    <term><command>Export</command></term>
                    <listitem><para>将表数据写入HDFS</para></listitem>
                </varlistentry>
                <varlistentry>
                    <term><command>Import</command></term>
                    <listitem><para>导入之前 <command>Export</command>操作所写的数据</para></listitem>
                </varlistentry>
                <varlistentry>
                    <term><command>ImportTsv</command></term>
                    <listitem><para>以TSV格式导入数据</para></listitem>
                </varlistentry>
                <varlistentry>
                    <term><command>RowCounter</command></term>
                    <listitem><para>计算HBase表的行数</para></listitem>
                </varlistentry>
                <varlistentry>
                    <term><command>replication.VerifyReplication</command></term>
                    <listitem><para>比较两个不同集群的表的数据。警告：
                        不适合incrementColumnValues单元格，因为时间戳已经改变。注意，这个命令在一个不同包里。</para></listitem>
                </varlistentry>
            </variablelist>
            <para>除<command>RowCounter</command>之外的命令，都可以通过<literal>--help</literal>参数查看使用说明。
                </para>
        </section>
        <section
                xml:id="hbck">
            <title>HBase <application>hbck</application></title>
            <subtitle>HBase安装中的 <command>fsck</command> </subtitle>
            <para>对你的HBase集群执行<command>$
                ./bin/hbase hbck</command>命令，最后会输出<literal>OK</literal>或 <literal>INCONSISTENCY</literal>。
                如果你的集群出现inconsistencies, 通过 <command>-details</command>能查看更多细节。
                如果inconsistencies, 再运行<command>hbck</command>几次，因为inconsistency可能是暂时性的（如集群启动或region分裂）。
                通过 <command>-fix</command> 也许能修复 inconsistency (这个是一个实验性功能)。 </para>
            <para>更多信息，参见<xref
                    linkend="hbck.in.depth" />. </para>
        </section>
        <section
                xml:id="hfile_tool2">
            <title>HFile 工具</title>
            <para>见 <xref
                    linkend="hfile_tool" />.</para>
        </section>
        <section
                xml:id="wal_tools">
            <title>WAL 工具</title>

            <section
                    xml:id="hlog_tool">
                <title><classname>FSHLog</classname> 工具</title>

                <para><classname>FSHLog</classname>的main方法提供手动切分和转储工具。
                    传给它 WALs 或需要切分的日志， <filename>recovered.edits</filename>. 目录中内容。</para>

                <para>如下所做，可以得到一个WAL文件的文本转储：</para>
                <screen language="bourne"> $ ./bin/hbase org.apache.hadoop.hbase.regionserver.wal.FSHLog --dump hdfs://example.org:8020/hbase/.logs/example.org,60020,1283516293161/10.10.21.10%3A60020.1283973724012 </screen>
                <para>如果文件有问题则返回非零，因此你可以将 <varname>STDOUT</varname> 重定向<code>/dev/null</code>，并测试文件是否健全</para>

                <para>与此类似，你也可以强制切分一个日志文件目录：</para>
                <screen language="bourne"> $ ./bin/hbase org.apache.hadoop.hbase.regionserver.wal.FSHLog --split hdfs://example.org:8020/hbase/.logs/example.org,60020,1283516293161/</screen>

                <section
                        xml:id="hlog_tool.prettyprint">
                    <title><classname>HLogPrettyPrinter</classname></title>
                    <para><classname>HLogPrettyPrinter</classname> 是一个打印HLog内容的工具。 </para>
                </section>

            </section>
        </section>
        <section
                xml:id="compression.tool">
            <title>压缩工具</title>
            <para>见 <xref
                    linkend="compression.test" />.</para>
        </section>
        <section
                xml:id="copytable">
            <title>CopyTable</title>
            <para> CopyTable用来拷贝一个表的部分或全部内容, 可以拷贝到同一集群或其它集群。目标表必须存在。用法如下：</para>

            <screen language="bourne">
                $ <userinput>./bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable --help </userinput>
                /bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable --help
                Usage: CopyTable [general options] [--starttime=X] [--endtime=Y] [--new.name=NEW] [--peer.adr=ADR] &lt;tablename&gt;

                Options:
                rs.class     hbase.regionserver.class of the peer cluster,
                specify if different from current cluster
                rs.impl      hbase.regionserver.impl of the peer cluster,
                startrow     the start row
                stoprow      the stop row
                starttime    beginning of the time range (unixtime in millis)
                without endtime means from starttime to forever
                endtime      end of the time range.  Ignored if no starttime specified.
                versions     number of cell versions to copy
                new.name     new table's name
                peer.adr     Address of the peer cluster given in the format
                hbase.zookeeer.quorum:hbase.zookeeper.client.port:zookeeper.znode.parent
                families     comma-separated list of families to copy
                To copy from cf1 to cf2, give sourceCfName:destCfName.
                To keep the same name, just give "cfName"
                all.cells    also copy delete markers and deleted cells

                Args:
                tablename    Name of the table to copy

                Examples:
                To copy 'TestTable' to a cluster that uses replication for a 1 hour window:
                $ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable --starttime=1265875194289 --endtime=1265878794289 --peer.adr=server1,server2,server3:2181:/hbase --families=myOldCf:myNewCf,cf2,cf3 TestTable

                For performance consider the following general options:
                It is recommended that you set the following to >=100. A higher value uses more memory but
                decreases the round trip time to the server and may increase performance.
                -Dhbase.client.scanner.caching=100
                The following should always be set to false, to prevent writing data twice, which may produce
                inaccurate results.
                -Dmapred.map.tasks.speculative.execution=false
            </screen>
            <note>
                <title>Scanner 缓存</title>
                <para>Scan缓存可通过job configuration中的<code>hbase.client.scanner.caching</code>配置。</para>
            </note>
            <note>
                <title>Versions</title>
                <para>默认情况下，CopyTable工具只拷贝行单元格的最新版本，除非在命令中指定<code>--versions=n</code>。 </para>
            </note>
            <para> Jonathan Hsieh's <link
                    xlink:href="http://www.cloudera.com/blog/2012/06/online-hbase-backups-with-copytable-2/">Online
                HBase Backups with CopyTable</link> 博客上有关于 <command>CopyTable</command>的更多内容。
            </para>
        </section>
        <section
                xml:id="export">
            <title>Export</title>
            <para>Export工具用来将表数据转储成HDFS中的序列化文件。
                调用如下:</para>
            <screen language="bourne">$ bin/hbase org.apache.hadoop.hbase.mapreduce.Export &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]]]
            </screen>
        </section>
        <section
                xml:id="import">
            <title>Import</title>
            <para>Import工具可以将导出的数据重新导入回HBase。调用如下:</para>
            <screen language="bourne">$ bin/hbase org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt;
            </screen>
            <para>如果要导入0.94版的导出文件到0.96版（或更高）的集群，你还需要设置系统属性"hbase.import.version"，命令如下：
                </para>
            <screen language="bourne">$ bin/hbase -Dhbase.import.version=0.94 org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt;
            </screen>
        </section>
        <section
                xml:id="importtsv">
            <title>ImportTsv</title>
            <para>ImportTsv工具将以TSV格式导入数据到HBase。 它有两个不同用法：
                1、通过Puts导入HDFS中TSV格式数据到HBase；2、通过<code>completebulkload</code>准备要导入的StoreFiles。
                </para>
            <para>通过Puts导入数据 (非大批量导入):</para>
            <screen language="bourne">$ bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=a,b,c &lt;tablename&gt; &lt;hdfs-inputdir&gt;
            </screen>

            <para>生成供大批量导入的StoreFiles </para>
            <programlisting language="bourne">$ bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=a,b,c -Dimporttsv.bulk.output=hdfs://storefile-outputdir &lt;tablename&gt; &lt;hdfs-data-inputdir&gt;
            </programlisting>
            <para>这些生成的StoreFiles能通过<xref
                    linkend="completebulkload" />导入HBase。 </para>
            <section
                    xml:id="importtsv.options">
                <title>ImportTsv 选项</title>
                <para>无参数执行 <command>ImportTsv</command> 命令会打印简单使用信息：</para>
                <screen>
                    Usage: importtsv -Dimporttsv.columns=a,b,c &lt;tablename&gt; &lt;inputdir&gt;

                    Imports the given input directory of TSV data into the specified table.

                    The column names of the TSV data must be specified using the -Dimporttsv.columns
                    option. This option takes the form of comma-separated column names, where each
                    column name is either a simple column family, or a columnfamily:qualifier. The special
                    column name HBASE_ROW_KEY is used to designate that this column should be used
                    as the row key for each imported record. You must specify exactly one column
                    to be the row key, and you must specify a column name for every column that exists in the
                    input data.

                    By default importtsv will load data directly into HBase. To instead generate
                    HFiles of data to prepare for a bulk data load, pass the option:
                    -Dimporttsv.bulk.output=/path/for/output
                    Note: the target table will be created with default column family descriptors if it does not already exist.

                    Other options that may be specified with -D include:
                    -Dimporttsv.skip.bad.lines=false - fail if encountering an invalid line
                    '-Dimporttsv.separator=|' - eg separate on pipes instead of tabs
                    -Dimporttsv.timestamp=currentTimeAsLong - use the specified timestamp for the import
                    -Dimporttsv.mapper.class=my.Mapper - A user-defined Mapper to use instead of org.apache.hadoop.hbase.mapreduce.TsvImporterMapper
                </screen>
            </section>
            <section
                    xml:id="importtsv.example">
                <title>ImportTsv 实例</title>
                <para>举个例子，假设我们正往表'datatsv'导入数据， 列族为'd'，两列为"c1"和"c2"。 </para>
                <para>输入文件假定如下：:
                    <screen>
                        row1	c1	c2
                        row2	c1	c2
                        row3	c1	c2
                        row4	c1	c2
                        row5	c1	c2
                        row6	c1	c2
                        row7	c1	c2
                        row8	c1	c2
                        row9	c1	c2
                        row10	c1	c2
                    </screen>
                </para>
                <para>通过如下命令，ImportTsv就可使用该输入文件：</para>
                <screen language="bourne">
                    HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-VERSION.jar importtsv -Dimporttsv.columns=HBASE_ROW_KEY,d:c1,d:c2 -Dimporttsv.bulk.output=hdfs://storefileoutput datatsv hdfs://inputfile
                </screen>
                <para> ... 在这个例子中，第一列是rowkey，所以才使用HBASE_ROW_KEY。
                    文件中的第二、三列将分别作为 "d:c1"和"d:c2"导入。 </para>
            </section>
            <section
                    xml:id="importtsv.warning">
                <title>ImportTsv 警告</title>
                <para>如果你准备bulk导入大量数据，一定要确保目标表已适当切分。 </para>
            </section>
            <section
                    xml:id="importtsv.also">
                <title>参考</title>
                <para>关于大批量导入HFiles到HBase的更多信息，请参考<xref
                        linkend="arch.bulk.load" /></para>
            </section>
        </section>

        <section
                xml:id="completebulkload">
            <title>CompleteBulkLoad</title>
            <para> <code>completebulkload</code>工具用来移动StoreFiles到HBase表。
                这个工具经常与<xref
                        linkend="importtsv" />结合使用。 </para>
            <para>有两种方式调用此工具，通过指定classname或driver：</para>
            <screen language="bourne">$ bin/hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles &lt;hdfs://storefileoutput&gt; &lt;tablename&gt;
            </screen>
            <para> .. 通过 Driver..</para>
            <screen language="bourne">HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-VERSION.jar completebulkload &lt;hdfs://storefileoutput&gt; &lt;tablename&gt;
            </screen>
            <section
                    xml:id="completebulkload.warning">
                <title>CompleteBulkLoad 警告</title>
                <para>通过MapReduce产生的数据，其文件权限经常与运行的HBase进程不兼容。
                    如果你正带着权限运行HDFS， 在执行CompleteBulkLoad前你需要更新这些权限。
                    </para>
                <para>关于大批量导入HFiles到HBase的更多信息，参考 <xref
                        linkend="arch.bulk.load" />. </para>
            </section>

        </section>
        <section
                xml:id="walplayer">
            <title>WALPlayer</title>
            <para>WALPlayer工具可以重放WAL文件到HBase. </para>
            <para>可以为一组表或所有表重放WAL, 并选择时间范围(毫秒级)，过滤这组表的WAL。输出还可以映射到另外一组表。</para>
            <para>WALPlayer 也可以为接下来的大批量导入生成 HFiles，在这种情况下，只能指定单个表且无法映射。
                 </para>
            <para>调用如下：</para>
            <screen language="bourne">$ bin/hbase org.apache.hadoop.hbase.mapreduce.WALPlayer [options] &lt;wal inputdir&gt; &lt;tables&gt; [&lt;tableMappings>]&gt;
            </screen>
            <para>例如:</para>
            <screen language="bourne">$ bin/hbase org.apache.hadoop.hbase.mapreduce.WALPlayer /backuplogdir oldTable1,oldTable2 newTable1,newTable2
            </screen>
            <para> WALPlayer, 默认情况下，以mapreduce任务执行。在命令行上添加<code>-Dmapreduce.jobtracker.address=local</code>标志
                能强制让它以本地进程的形式运行。 </para>
        </section>
        <section
                xml:id="rowcounter">
            <title>RowCounter 和 CellCounter</title>
            <para><link
                    xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/RowCounter.html">RowCounter</link>
                是一个mapreduce任务，用来计算一个表的所有行。这是一个好工具，可以用来检测HBase是否可以读取一个表的所有块，
                借此发现元数据是否出现不一致。这个工具将在一个单独的进程内运行mapreduce，如果你有一个MapReduce集群供它使用的话，它会运行的更快。
                </para>
            <screen language="bourne">$ bin/hbase org.apache.hadoop.hbase.mapreduce.RowCounter &lt;tablename&gt; [&lt;column1&gt; &lt;column2&gt;...]
            </screen>
            <para>注意:Scan缓存可通过job configuration中的<code>hbase.client.scanner.caching</code>配置。
                 </para>
            <para>HBase 还推出了另外一个叫<link
                    xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/CellCounter.html">CellCounter</link>的诊断性mapreduce任务。
                与RowCounter类似, 但它会更细粒度地收集表的统计数据，包括： </para>
            <itemizedlist>
                <listitem>
                    <para>Total number of rows in the table.</para>
                </listitem>
                <listitem>
                    <para>Total number of CFs across all rows.</para>
                </listitem>
                <listitem>
                    <para>Total qualifiers across all rows.</para>
                </listitem>
                <listitem>
                    <para>Total occurrence of each CF.</para>
                </listitem>
                <listitem>
                    <para>Total occurrence of each qualifier.</para>
                </listitem>
                <listitem>
                    <para>Total number of versions of each qualifier.</para>
                </listitem>
            </itemizedlist>
            <para>这个程序可以限定运行范围。它提供了行正则表达式/前缀参数来限定要分析的行。
                使用 <code>hbase.mapreduce.scan.column.family</code> 可指定要扫描的单个列族。</para>
            <screen language="bourne">$ bin/hbase org.apache.hadoop.hbase.mapreduce.CellCounter &lt;tablename&gt; &lt;outputDir&gt; [regex or prefix]</screen>
            <para>注意:和RowCounter类似, Scan缓存可通过job configuration中的<code>hbase.client.scanner.caching</code>配置。
             </para>
        </section>
        <section
                xml:id="mlockall">
            <title>mlockall</title>
            <para>在启动时让服务器调用<link xlink:href="http://linux.die.net/man/2/mlockall">mlockall</link>，
                可以将其锁定在物理内存中，使其在超配额环境中不太可能发生内存交换。
                参考 <link
                        xlink:href="https://issues.apache.org/jira/browse/HBASE-4391">HBASE-4391 Add ability to
                    start RS as root and call mlockall</link>，如何构建可选库并使其在启动时运行。 </para>
        </section>
        <section
                xml:id="compaction.tool">
            <title>离线压缩工具</title>
            <para>参考<link
                    xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/CompactionTool.html">压缩工具
                </link>使用方法。 调用如下： <command>./bin/hbase
                org.apache.hadoop.hbase.regionserver.CompactionTool</command>
            </para>
        </section>

        <section>
            <title><command>hbase 清理</command></title>
            <para><command>hbase clean</command>命令用来清除ZooKeeper/HDFS中的HBase数据。
               该命令比较适合测试，无参数执行它可以查看使用说明，HBase 0.98版本引入了该命令。
               </para>
            <screen>
                $ <userinput>bin/hbase clean</userinput>
                Usage: hbase clean (--cleanZk|--cleanHdfs|--cleanAll)
                Options:
                --cleanZk   cleans hbase related data from zookeeper.
                --cleanHdfs cleans hbase related data from hdfs.
                --cleanAll  cleans hbase related data from both zookeeper and hdfs.
            </screen>
        </section>
        <section>
            <title><command>hbase pe</command></title>
            <para> <command>hbase pe</command>命令是运行<code>org.apache.hadoop.hbase.PerformanceEvaluation</code> 工具的快捷方式,
                该工具用来测试。HBase 0.98版本中引入了<command>hbase pe</command>命令。</para>
        </section>
        <section>
            <title><command>hbase ltt</command></title>
            <para> <command>hbase ltt</command>命令是运行 <code>rg.apache.hadoop.hbase.util.LoadTestTool</code>工具的快捷方式，
                该工具用来测试。HBase 0.98版本中引入了<command>hbase ltt</command>命令。
                </para>
        </section>
    </section>
    <!--  tools -->

    <section
            xml:id="ops.regionmgt">
        <title>Region 管理</title>
        <section
                xml:id="ops.regionmgt.majorcompact">
            <title>Major 压缩</title>
            <para>Major 压缩可以通过HBase shell或 <link
                    xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html#majorCompact%28java.lang.String%29">HBaseAdmin.majorCompact</link>执行。 </para>
            <para>注意： major压缩不会合并region。更多信息，请参考<xref
                    linkend="compaction" /> </para>
        </section>
        <section
                xml:id="ops.regionmgt.merge">
            <title>Merge</title>
            <para>Merge工具用来合并同一表的相邻region。(参考
                org.apache.hadoop.hbase.util.Merge).</para>
            <programlisting language="bourne">$ bin/hbase org.apache.hadoop.hbase.util.Merge &lt;tablename&gt; &lt;region1&gt; &lt;region2&gt;
            </programlisting>
            <para>如果你觉得region太多，想合并它们，Merge工具就是你的不二选择。
                Merge操作必须再集群停止前完成。参考 <link
                        xlink:href="http://ofps.oreilly.com/titles/9781449396107/performance.html">O'Reilly HBase
                    Book</link> ，里面有使用示例。 </para>
            <para>这个程序需要三个参数，第一个是表名，第二个是需要合并的第一个region的全名，如"table_name,\x0A,1342956111995.7cef47f192318ba7ccc75b1bbf27a82b."，
                第三个是需要合并的第二个region的全名。 </para>
            <para>另外，这里有 <link
                    xlink:href="https://issues.apache.org/jira/browse/HBASE-1621">HBASE-1621</link>附带的用来region合并的Ruby脚本。
                 </para>
        </section>
    </section>

    <section
            xml:id="node.management">
        <title>节点管理</title>
        <section
                xml:id="decommission">
            <title>节点下线</title>
            <para>你可以在HBase的特定节点上运行下面的脚本来停止RegionServer：</para>
            <screen language="bourne">$ ./bin/hbase-daemon.sh stop regionserver</screen>
            <para> RegionServer首先会关闭所有的region，然后关闭自己。在停止过程中，ZooKeeper中的RegionServer的临时节点会过期。
                master 会发现 RegionServer 已经死了，然后将它当做崩溃的server来处理，并重新重新分配该RegionServer上的region。
               </para>
            <note>
                <title>下线节点前要停止 Load Balancer</title>
                <para>If the load balancer runs while a node is shutting down, then there could be
                    contention between the Load Balancer and the Master's recovery of the just decommissioned
                    RegionServer. Avoid any problems by disabling the balancer first. See <xref
                            linkend="lb" /> below. </para>
            </note>
            <para> A downside to the above stop of a RegionServer is that regions could be offline for a
                good period of time. Regions are closed in order. If many regions on the server, the first
                region to close may not be back online until all regions close and after the master notices
                the RegionServer's znode gone. In Apache HBase 0.90.2, we added facility for having a node
                gradually shed its load and then shutdown itself down. Apache HBase 0.90.2 added the
                <filename>graceful_stop.sh</filename> script. Here is its usage:</para>
            <screen language="bourne">$ ./bin/graceful_stop.sh
                Usage: graceful_stop.sh [--config &amp;conf-dir>] [--restart] [--reload] [--thrift] [--rest] &amp;hostname>
                thrift      If we should stop/start thrift before/after the hbase stop/start
                rest        If we should stop/start rest before/after the hbase stop/start
                restart     If we should restart after graceful stop
                reload      Move offloaded regions back on to the stopped server
                debug       Move offloaded regions back on to the stopped server
                hostname    Hostname of server we are to stop</screen>
            <para> To decommission a loaded RegionServer, run the following: <command>$
                ./bin/graceful_stop.sh HOSTNAME</command> where <varname>HOSTNAME</varname> is the host
                carrying the RegionServer you would decommission. </para>
            <note>
                <title>On <varname>HOSTNAME</varname></title>
                <para>The <varname>HOSTNAME</varname> passed to <filename>graceful_stop.sh</filename> must
                    match the hostname that hbase is using to identify RegionServers. Check the list of
                    RegionServers in the master UI for how HBase is referring to servers. Its usually hostname
                    but can also be FQDN. Whatever HBase is using, this is what you should pass the
                    <filename>graceful_stop.sh</filename> decommission script. If you pass IPs, the script
                    is not yet smart enough to make a hostname (or FQDN) of it and so it will fail when it
                    checks if server is currently running; the graceful unloading of regions will not run.
                </para>
            </note>
            <para> The <filename>graceful_stop.sh</filename> script will move the regions off the
                decommissioned RegionServer one at a time to minimize region churn. It will verify the
                region deployed in the new location before it will moves the next region and so on until the
                decommissioned server is carrying zero regions. At this point, the
                <filename>graceful_stop.sh</filename> tells the RegionServer <command>stop</command>. The
                master will at this point notice the RegionServer gone but all regions will have already
                been redeployed and because the RegionServer went down cleanly, there will be no WAL logs to
                split. </para>
            <note
                    xml:id="lb">
                <title>Load Balancer</title>
                <para> It is assumed that the Region Load Balancer is disabled while the
                    <command>graceful_stop</command> script runs (otherwise the balancer and the
                    decommission script will end up fighting over region deployments). Use the shell to
                    disable the balancer:</para>
                <programlisting>hbase(main):001:0> balance_switch false
                    true
                    0 row(s) in 0.3590 seconds</programlisting>
                <para> This turns the balancer OFF. To reenable, do:</para>
                <programlisting>hbase(main):001:0> balance_switch true
                    false
                    0 row(s) in 0.3590 seconds</programlisting>
                <para>The <command>graceful_stop</command> will check the balancer and if enabled, will turn
                    it off before it goes to work. If it exits prematurely because of error, it will not have
                    reset the balancer. Hence, it is better to manage the balancer apart from
                    <command>graceful_stop</command> reenabling it after you are done w/ graceful_stop.
                </para>
            </note>
            <section
                    xml:id="draining.servers">
                <title>Decommissioning several Regions Servers concurrently</title>
                <para>If you have a large cluster, you may want to decommission more than one machine at a
                    time by gracefully stopping mutiple RegionServers concurrently. To gracefully drain
                    multiple regionservers at the same time, RegionServers can be put into a "draining" state.
                    This is done by marking a RegionServer as a draining node by creating an entry in
                    ZooKeeper under the <filename>hbase_root/draining</filename> znode. This znode has format
                    <code>name,port,startcode</code> just like the regionserver entries under
                    <filename>hbase_root/rs</filename> znode. </para>
                <para>Without this facility, decommissioning mulitple nodes may be non-optimal because
                    regions that are being drained from one region server may be moved to other regionservers
                    that are also draining. Marking RegionServers to be in the draining state prevents this
                    from happening<footnote>
                        <para>See this <link
                                xlink:href="http://inchoate-clatter.blogspot.com/2012/03/hbase-ops-automation.html">blog
                            post</link> for more details.</para>
                    </footnote>. </para>
            </section>

            <section
                    xml:id="bad.disk">
                <title>Bad or Failing Disk</title>
                <para>It is good having <xref
                        linkend="dfs.datanode.failed.volumes.tolerated" /> set if you have a decent number of
                    disks per machine for the case where a disk plain dies. But usually disks do the "John
                    Wayne" -- i.e. take a while to go down spewing errors in <filename>dmesg</filename> -- or
                    for some reason, run much slower than their companions. In this case you want to
                    decommission the disk. You have two options. You can <link
                            xlink:href="http://wiki.apache.org/hadoop/FAQ#I_want_to_make_a_large_cluster_smaller_by_taking_out_a_bunch_of_nodes_simultaneously._How_can_this_be_done.3F">decommission
                        the datanode</link> or, less disruptive in that only the bad disks data will be
                    rereplicated, can stop the datanode, unmount the bad volume (You can't umount a volume
                    while the datanode is using it), and then restart the datanode (presuming you have set
                    dfs.datanode.failed.volumes.tolerated > 0). The regionserver will throw some errors in its
                    logs as it recalibrates where to get its data from -- it will likely roll its WAL log too
                    -- but in general but for some latency spikes, it should keep on chugging. </para>
                <note>
                    <title>Short Circuit Reads</title>
                    <para>If you are doing short-circuit reads, you will have to move the regions off the
                        regionserver before you stop the datanode; when short-circuiting reading, though chmod'd
                        so regionserver cannot have access, because it already has the files open, it will be
                        able to keep reading the file blocks from the bad disk even though the datanode is down.
                        Move the regions back after you restart the datanode.</para>
                </note>
            </section>
        </section>
        <section
                xml:id="rolling">
            <title>Rolling Restart</title>

            <para>Some cluster configuration changes require either the entire cluster, or the
                RegionServers, to be restarted in order to pick up the changes. In addition, rolling
                restarts are supported for upgrading to a minor or maintenance release, and to a major
                release if at all possible. See the release notes for release you want to upgrade to, to
                find out about limitations to the ability to perform a rolling upgrade.</para>
            <para>There are multiple ways to restart your cluster nodes, depending on your situation.
                These methods are detailed below.</para>
            <section>
                <title>Using the <command>rolling-restart.sh</command> Script</title>

                <para>HBase ships with a script, <filename>bin/rolling-restart.sh</filename>, that allows
                    you to perform rolling restarts on the entire cluster, the master only, or the
                    RegionServers only. The script is provided as a template for your own script, and is not
                    explicitly tested. It requires password-less SSH login to be configured and assumes that
                    you have deployed using a tarball. The script requires you to set some environment
                    variables before running it. Examine the script and modify it to suit your needs.</para>
                <example>
                    <title><filename>rolling-restart.sh</filename> General Usage</title>
                    <screen language="bourne">
                        $ <userinput>./bin/rolling-restart.sh --help</userinput><![CDATA[
Usage: rolling-restart.sh [--config <hbase-confdir>] [--rs-only] [--master-only] [--graceful] [--maxthreads xx]          
        ]]></screen>
                </example>
                <variablelist>
                    <varlistentry>
                        <term>Rolling Restart on RegionServers Only</term>
                        <listitem>
                            <para>To perform a rolling restart on the RegionServers only, use the
                                <code>--rs-only</code> option. This might be necessary if you need to reboot the
                                individual RegionServer or if you make a configuration change that only affects
                                RegionServers and not the other HBase processes.</para>
                            <para>If you need to restart only a single RegionServer, or if you need to do extra
                                actions during the restart, use the <filename>bin/graceful_stop.sh</filename>
                                command instead. See <xref linkend="rolling.restart.manual" />.</para>
                        </listitem>
                    </varlistentry>
                    <varlistentry>
                        <term>Rolling Restart on Masters Only</term>
                        <listitem>
                            <para>To perform a rolling restart on the active and backup Masters, use the
                                <code>--master-only</code> option. You might use this if you know that your
                                configuration change only affects the Master and not the RegionServers, or if you
                                need to restart the server where the active Master is running.</para>
                            <para>If you are not running backup Masters, the Master is simply restarted. If you
                                are running backup Masters, they are all stopped before any are restarted, to avoid
                                a race condition in ZooKeeper to determine which is the new Master. First the main
                                Master is restarted, then the backup Masters are restarted. Directly after restart,
                                it checks for and cleans out any regions in transition before taking on its normal
                                workload.</para>
                        </listitem>
                    </varlistentry>
                    <varlistentry>
                        <term>Graceful Restart</term>
                        <listitem>
                            <para>If you specify the <code>--graceful</code> option, RegionServers are restarted
                                using the <filename>bin/graceful_stop.sh</filename> script, which moves regions off
                                a RegionServer before restarting it. This is safer, but can delay the
                                restart.</para>
                        </listitem>
                    </varlistentry>
                    <varlistentry>
                        <term>Limiting the Number of Threads</term>
                        <listitem>
                            <para>To limit the rolling restart to using only a specific number of threads, use the
                                <code>--maxthreads</code> option.</para>
                        </listitem>
                    </varlistentry>
                </variablelist>
            </section>
            <section xml:id="rolling.restart.manual">
                <title>Manual Rolling Restart</title>
                <para>To retain more control over the process, you may wish to manually do a rolling restart
                    across your cluster. This uses the <command>graceful-stop.sh</command> command <xref
                            linkend="decommission" />. In this method, you can restart each RegionServer
                    individually and then move its old regions back into place, retaining locality. If you
                    also need to restart the Master, you need to do it separately, and restart the Master
                    before restarting the RegionServers using this method. The following is an example of such
                    a command. You may need to tailor it to your environment. This script does a rolling
                    restart of RegionServers only. It disables the load balancer before moving the
                    regions.</para>
                <screen><![CDATA[
$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i; done &> /tmp/log.txt &;     
        ]]></screen>
                <para>Monitor the output of the <filename>/tmp/log.txt</filename> file to follow the
                    progress of the script. </para>
            </section>

            <section>
                <title>Logic for Crafting Your Own Rolling Restart Script</title>
                <para>Use the following guidelines if you want to create your own rolling restart script.</para>
                <orderedlist>
                    <listitem>
                        <para>Extract the new release, verify its configuration, and synchronize it to all nodes
                            of your cluster using <command>rsync</command>, <command>scp</command>, or another
                            secure synchronization mechanism.</para></listitem>
                    <listitem><para>Use the hbck utility to ensure that the cluster is consistent.</para>
                        <screen>
                            $ ./bin/hbck
                        </screen>
                        <para>Perform repairs if required. See <xref linkend="hbck" /> for details.</para>
                    </listitem>
                    <listitem><para>Restart the master first. You may need to modify these commands if your
                        new HBase directory is different from the old one, such as for an upgrade.</para>
                        <screen>
                            $ ./bin/hbase-daemon.sh stop master; ./bin/hbase-daemon.sh start master
                        </screen>
                    </listitem>
                    <listitem><para>Gracefully restart each RegionServer, using a script such as the
                        following, from the Master.</para>
                        <screen><![CDATA[
$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i; done &> /tmp/log.txt &            
          ]]></screen>
                        <para>If you are running Thrift or REST servers, pass the --thrift or --rest options.
                            For other available options, run the <command>bin/graceful-stop.sh --help</command>
                            command.</para>
                        <para>It is important to drain HBase regions slowly when restarting multiple
                            RegionServers. Otherwise, multiple regions go offline simultaneously and must be
                            reassigned to other nodes, which may also go offline soon. This can negatively affect
                            performance. You can inject delays into the script above, for instance, by adding a
                            Shell command such as <command>sleep</command>. To wait for 5 minutes between each
                            RegionServer restart, modify the above script to the following:</para>
                        <screen><![CDATA[
$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i & sleep 5m; done &> /tmp/log.txt &            
          ]]></screen>
                    </listitem>
                    <listitem><para>Restart the Master again, to clear out the dead servers list and re-enable
                        the load balancer.</para></listitem>
                    <listitem><para>Run the <command>hbck</command> utility again, to be sure the cluster is
                        consistent.</para></listitem>
                </orderedlist>
            </section>
        </section>
        <section
                xml:id="adding.new.node">
            <title>Adding a New Node</title>
            <para>Adding a new regionserver in HBase is essentially free, you simply start it like this:
                <command>$ ./bin/hbase-daemon.sh start regionserver</command> and it will register itself
                with the master. Ideally you also started a DataNode on the same machine so that the RS can
                eventually start to have local files. If you rely on ssh to start your daemons, don't forget
                to add the new hostname in <filename>conf/regionservers</filename> on the master. </para>
            <para>At this point the region server isn't serving data because no regions have moved to it
                yet. If the balancer is enabled, it will start moving regions to the new RS. On a
                small/medium cluster this can have a very adverse effect on latency as a lot of regions will
                be offline at the same time. It is thus recommended to disable the balancer the same way
                it's done when decommissioning a node and move the regions manually (or even better, using a
                script that moves them one by one). </para>
            <para>The moved regions will all have 0% locality and won't have any blocks in cache so the
                region server will have to use the network to serve requests. Apart from resulting in higher
                latency, it may also be able to use all of your network card's capacity. For practical
                purposes, consider that a standard 1GigE NIC won't be able to read much more than
                <emphasis>100MB/s</emphasis>. In this case, or if you are in a OLAP environment and
                require having locality, then it is recommended to major compact the moved regions. </para>

        </section>
    </section>
    <!--  node mgt -->

    <section
            xml:id="hbase_metrics">
        <title>HBase Metrics</title>
        <section
                xml:id="metric_setup">
            <title>Metric Setup</title>
            <para>See <link
                    xlink:href="http://hbase.apache.org/metrics.html">Metrics</link> for an introduction and
                how to enable Metrics emission. Still valid for HBase 0.94.x. </para>
            <para>For HBase 0.95.x and up, see <link
                    xlink:href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/metrics2/package-summary.html" />
            </para>
        </section>
        <section
                xml:id="rs_metrics_ganglia">
            <title>Warning To Ganglia Users</title>
            <para>Warning to Ganglia Users: by default, HBase will emit a LOT of metrics per RegionServer
                which may swamp your installation. Options include either increasing Ganglia server
                capacity, or configuring HBase to emit fewer metrics. </para>
        </section>
        <section
                xml:id="rs_metrics">
            <title>Most Important RegionServer Metrics</title>
            <section
                    xml:id="hbase.regionserver.blockCacheHitCachingRatio">
                <title><varname>blockCacheExpressCachingRatio (formerly
                    blockCacheHitCachingRatio)</varname></title>
                <para>Block cache hit caching ratio (0 to 100). The cache-hit ratio for reads configured to
                    look in the cache (i.e., cacheBlocks=true). </para>
            </section>
            <section
                    xml:id="hbase.regionserver.callQueueLength">
                <title><varname>callQueueLength</varname></title>
                <para>Point in time length of the RegionServer call queue. If requests arrive faster than
                    the RegionServer handlers can process them they will back up in the callQueue.</para>
            </section>
            <section
                    xml:id="hbase.regionserver.compactionQueueSize">
                <title><varname>compactionQueueLength (formerly compactionQueueSize)</varname></title>
                <para>Point in time length of the compaction queue. This is the number of Stores in the
                    RegionServer that have been targeted for compaction.</para>
            </section>
            <section
                    xml:id="hbase.regionserver.flushQueueSize">
                <title><varname>flushQueueSize</varname></title>
                <para>Point in time number of enqueued regions in the MemStore awaiting flush.</para>
            </section>
            <section
                    xml:id="hbase.regionserver.hdfsBlocksLocalityIndex">
                <title><varname>hdfsBlocksLocalityIndex</varname></title>
                <para>Point in time percentage of HDFS blocks that are local to this RegionServer. The
                    higher the better. </para>
            </section>
            <section
                    xml:id="hbase.regionserver.memstoreSizeMB">
                <title><varname>memstoreSizeMB</varname></title>
                <para>Point in time sum of all the memstore sizes in this RegionServer (MB). Watch for this
                    nearing or exceeding the configured high-watermark for MemStore memory in the
                    RegionServer. </para>
            </section>
            <section
                    xml:id="hbase.regionserver.regions">
                <title><varname>numberOfOnlineRegions</varname></title>
                <para>Point in time number of regions served by the RegionServer. This is an important
                    metric to track for RegionServer-Region density. </para>
            </section>
            <section
                    xml:id="hbase.regionserver.readRequestsCount">
                <title><varname>readRequestsCount</varname></title>
                <para>Number of read requests for this RegionServer since startup. Note: this is a 32-bit
                    integer and can roll. </para>
            </section>
            <section
                    xml:id="hbase.regionserver.slowHLogAppendCount">
                <title><varname>slowHLogAppendCount</varname></title>
                <para>Number of slow HLog append writes for this RegionServer since startup, where "slow" is
                    > 1 second. This is a good "canary" metric for HDFS. </para>
            </section>
            <section
                    xml:id="hbase.regionserver.usedHeapMB">
                <title><varname>usedHeapMB</varname></title>
                <para>Point in time amount of memory used by the RegionServer (MB).</para>
            </section>
            <section
                    xml:id="hbase.regionserver.writeRequestsCount">
                <title><varname>writeRequestsCount</varname></title>
                <para>Number of write requests for this RegionServer since startup. Note: this is a 32-bit
                    integer and can roll. </para>
            </section>

        </section>
        <section
                xml:id="rs_metrics_other">
            <title>Other RegionServer Metrics</title>
            <section
                    xml:id="hbase.regionserver.blockCacheCount">
                <title><varname>blockCacheCount</varname></title>
                <para>Point in time block cache item count in memory. This is the number of blocks of
                    StoreFiles (HFiles) in the cache.</para>
            </section>
            <section
                    xml:id="hbase.regionserver.blockCacheEvictedCount">
                <title><varname>blockCacheEvictedCount</varname></title>
                <para>Number of blocks that had to be evicted from the block cache due to heap size
                    constraints by RegionServer since startup.</para>
            </section>
            <section
                    xml:id="hbase.regionserver.blockCacheFree">
                <title><varname>blockCacheFreeMB</varname></title>
                <para>Point in time block cache memory available (MB).</para>
            </section>
            <section
                    xml:id="hbase.regionserver.blockCacheHitCount">
                <title><varname>blockCacheHitCount</varname></title>
                <para>Number of blocks of StoreFiles (HFiles) read from the cache by RegionServer since
                    startup.</para>
            </section>
            <section
                    xml:id="hbase.regionserver.blockCacheHitRatio">
                <title><varname>blockCacheHitRatio</varname></title>
                <para>Block cache hit ratio (0 to 100) from RegionServer startup. Includes all read
                    requests, although those with cacheBlocks=false will always read from disk and be counted
                    as a "cache miss", which means that full-scan MapReduce jobs can affect this metric
                    significantly.</para>
            </section>
            <section
                    xml:id="hbase.regionserver.blockCacheMissCount">
                <title><varname>blockCacheMissCount</varname></title>
                <para>Number of blocks of StoreFiles (HFiles) requested but not read from the cache from
                    RegionServer startup.</para>
            </section>
            <section
                    xml:id="hbase.regionserver.blockCacheSize">
                <title><varname>blockCacheSizeMB</varname></title>
                <para>Point in time block cache size in memory (MB). i.e., memory in use by the
                    BlockCache</para>
            </section>
            <section
                    xml:id="hbase.regionserver.fsPreadLatency">
                <title><varname>fsPreadLatency*</varname></title>
                <para>There are several filesystem positional read latency (ms) metrics, all measured from
                    RegionServer startup.</para>
            </section>
            <section
                    xml:id="hbase.regionserver.fsReadLatency">
                <title><varname>fsReadLatency*</varname></title>
                <para>There are several filesystem read latency (ms) metrics, all measured from RegionServer
                    startup. The issue with interpretation is that ALL reads go into this metric (e.g.,
                    single-record Gets, full table Scans), including reads required for compactions. This
                    metric is only interesting "over time" when comparing major releases of HBase or your own
                    code.</para>
            </section>
            <section
                    xml:id="hbase.regionserver.fsWriteLatency">
                <title><varname>fsWriteLatency*</varname></title>
                <para>There are several filesystem write latency (ms) metrics, all measured from
                    RegionServer startup. The issue with interpretation is that ALL writes go into this metric
                    (e.g., single-record Puts, full table re-writes due to compaction). This metric is only
                    interesting "over time" when comparing major releases of HBase or your own code.</para>
            </section>
            <section
                    xml:id="hbase.regionserver.stores">
                <title><varname>NumberOfStores</varname></title>
                <para>Point in time number of Stores open on the RegionServer. A Store corresponds to a
                    ColumnFamily. For example, if a table (which contains the column family) has 3 regions on
                    a RegionServer, there will be 3 stores open for that column family. </para>
            </section>
            <section
                    xml:id="hbase.regionserver.storeFiles">
                <title><varname>NumberOfStorefiles</varname></title>
                <para>Point in time number of StoreFiles open on the RegionServer. A store may have more
                    than one StoreFile (HFile).</para>
            </section>
            <section
                    xml:id="hbase.regionserver.requests">
                <title><varname>requestsPerSecond</varname></title>
                <para>Point in time number of read and write requests. Requests correspond to RegionServer
                    RPC calls, thus a single Get will result in 1 request, but a Scan with caching set to 1000
                    will result in 1 request for each 'next' call (i.e., not each row). A bulk-load request
                    will constitute 1 request per HFile. This metric is less interesting than
                    readRequestsCount and writeRequestsCount in terms of measuring activity due to this metric
                    being periodic. </para>
            </section>
            <section
                    xml:id="hbase.regionserver.storeFileIndexSizeMB">
                <title><varname>storeFileIndexSizeMB</varname></title>
                <para>Point in time sum of all the StoreFile index sizes in this RegionServer (MB)</para>
            </section>
        </section>
    </section>

    <section
            xml:id="ops.monitoring">
        <title>HBase Monitoring</title>
        <section
                xml:id="ops.monitoring.overview">
            <title>Overview</title>
            <para>The following metrics are arguably the most important to monitor for each RegionServer
                for "macro monitoring", preferably with a system like <link
                        xlink:href="http://opentsdb.net/">OpenTSDB</link>. If your cluster is having performance
                issues it's likely that you'll see something unusual with this group. </para>
            <itemizedlist>
                <title>HBase:</title>
                <listitem>
                    <para>See <xref
                            linkend="rs_metrics" /></para>
                </listitem>
            </itemizedlist>

            <itemizedlist>
                <title>OS:</title>
                <listitem>
                    <para>IO Wait</para>
                </listitem>
                <listitem>
                    <para>User CPU</para>
                </listitem>
            </itemizedlist>
            <itemizedlist>
                <title>Java:</title>
                <listitem>
                    <para>GC</para>
                </listitem>
            </itemizedlist>
            <para> For more information on HBase metrics, see <xref
                    linkend="hbase_metrics" />. </para>
        </section>

        <section
                xml:id="ops.slow.query">
            <title>Slow Query Log</title>
            <para>The HBase slow query log consists of parseable JSON structures describing the properties
                of those client operations (Gets, Puts, Deletes, etc.) that either took too long to run, or
                produced too much output. The thresholds for "too long to run" and "too much output" are
                configurable, as described below. The output is produced inline in the main region server
                logs so that it is easy to discover further details from context with other logged events.
                It is also prepended with identifying tags <constant>(responseTooSlow)</constant>,
                <constant>(responseTooLarge)</constant>, <constant>(operationTooSlow)</constant>, and
                <constant>(operationTooLarge)</constant> in order to enable easy filtering with grep, in
                case the user desires to see only slow queries. </para>

            <section>
                <title>Configuration</title>
                <para>There are two configuration knobs that can be used to adjust the thresholds for when
                    queries are logged. </para>

                <itemizedlist>
                    <listitem>
                        <para><varname>hbase.ipc.warn.response.time</varname> Maximum number of milliseconds
                            that a query can be run without being logged. Defaults to 10000, or 10 seconds. Can be
                            set to -1 to disable logging by time. </para>
                    </listitem>
                    <listitem>
                        <para><varname>hbase.ipc.warn.response.size</varname> Maximum byte size of response that
                            a query can return without being logged. Defaults to 100 megabytes. Can be set to -1
                            to disable logging by size. </para>
                    </listitem>
                </itemizedlist>
            </section>

            <section>
                <title>Metrics</title>
                <para>The slow query log exposes to metrics to JMX.</para>
                <itemizedlist>
                    <listitem>
                        <para><varname>hadoop.regionserver_rpc_slowResponse</varname> a global metric reflecting
                            the durations of all responses that triggered logging.</para>
                    </listitem>
                    <listitem>
                        <para><varname>hadoop.regionserver_rpc_methodName.aboveOneSec</varname> A metric
                            reflecting the durations of all responses that lasted for more than one second.</para>
                    </listitem>
                </itemizedlist>

            </section>

            <section>
                <title>Output</title>
                <para>The output is tagged with operation e.g. <constant>(operationTooSlow)</constant> if
                    the call was a client operation, such as a Put, Get, or Delete, which we expose detailed
                    fingerprint information for. If not, it is tagged <constant>(responseTooSlow)</constant>
                    and still produces parseable JSON output, but with less verbose information solely
                    regarding its duration and size in the RPC itself. <constant>TooLarge</constant> is
                    substituted for <constant>TooSlow</constant> if the response size triggered the logging,
                    with <constant>TooLarge</constant> appearing even in the case that both size and duration
                    triggered logging. </para>
            </section>
            <section>
                <title>Example</title>
                <para>
                    <programlisting>2011-09-08 10:01:25,824 WARN org.apache.hadoop.ipc.HBaseServer: (operationTooSlow): {"tables":{"riley2":{"puts":[{"totalColumns":11,"families":{"actions":[{"timestamp":1315501284459,"qualifier":"0","vlen":9667580},{"timestamp":1315501284459,"qualifier":"1","vlen":10122412},{"timestamp":1315501284459,"qualifier":"2","vlen":11104617},{"timestamp":1315501284459,"qualifier":"3","vlen":13430635}]},"row":"cfcd208495d565ef66e7dff9f98764da:0"}],"families":["actions"]}},"processingtimems":956,"client":"10.47.34.63:33623","starttimems":1315501284456,"queuetimems":0,"totalPuts":1,"class":"HRegionServer","responsesize":0,"method":"multiPut"}</programlisting>
                </para>

                <para>Note that everything inside the "tables" structure is output produced by MultiPut's
                    fingerprint, while the rest of the information is RPC-specific, such as processing time
                    and client IP/port. Other client operations follow the same pattern and the same general
                    structure, with necessary differences due to the nature of the individual operations. In
                    the case that the call is not a client operation, that detailed fingerprint information
                    will be completely absent. </para>

                <para>This particular example, for example, would indicate that the likely cause of slowness
                    is simply a very large (on the order of 100MB) multiput, as we can tell by the "vlen," or
                    value length, fields of each put in the multiPut. </para>
            </section>
        </section>
        <section>
            <title>Block Cache Monitoring</title>
            <para>Starting with HBase 0.98, the HBase Web UI includes the ability to monitor and report on
                the performance of the block cache. To view the block cache reports, click <menuchoice>
                    <guimenu>Tasks</guimenu>
                    <guisubmenu>Show Non-RPC Tasks</guisubmenu>
                    <guimenuitem>Block Cache</guimenuitem>
                </menuchoice>. Following are a few examples of the reporting capabilities.</para>
            <figure>
                <title>Basic Info</title>
                <mediaobject>
                    <imageobject>
                        <imagedata fileref="bc_basic.png" width="100%"/>
                    </imageobject>
                    <textobject>
                        <para>Shows the cache implementation</para>
                    </textobject>
                </mediaobject>
            </figure>
            <figure>
                <title>Config</title>
                <mediaobject>
                    <imageobject>
                        <imagedata fileref="bc_config.png" width="100%"/>
                    </imageobject>
                    <textobject>
                        <para>Shows all cache configuration options.</para>
                    </textobject>
                </mediaobject>
            </figure>
            <figure>
                <title>Stats</title>
                <mediaobject>
                    <imageobject>
                        <imagedata fileref="bc_stats.png" width="100%"/>
                    </imageobject>
                    <textobject>
                        <para>Shows statistics about the performance of the cache.</para>
                    </textobject>
                </mediaobject>
            </figure>
            <figure>
                <title>L1 and L2</title>
                <mediaobject>
                    <imageobject>
                        <imagedata fileref="bc_l1.png" width="100%"/>
                    </imageobject>
                    <imageobject>
                        <imagedata fileref="bc_l2_buckets.png" width="100%"/>
                    </imageobject>
                    <textobject>
                        <para>Shows information about the L1 and L2 caches.</para>
                    </textobject>
                </mediaobject>
            </figure>
            <para>This is not an exhaustive list of all the screens and reports available. Have a look in
                the Web UI.</para>
        </section>



    </section>

    <section
            xml:id="cluster_replication">
        <title>Cluster Replication</title>
        <note>
            <para>This information was previously available at <link
                    xlink:href="http://hbase.apache.org/replication.html">Cluster Replication</link>. </para>
        </note>
        <para>HBase provides a replication mechanism to copy data between HBase
            clusters. Replication can be used as a disaster recovery solution and as a mechanism for high
            availability. You can also use replication to separate web-facing operations from back-end
            jobs such as MapReduce.</para>

        <para>In terms of architecture, HBase replication is master-push. This takes advantage of the
            fact that each region server has its own write-ahead log (WAL). One master cluster can
            replicate to any number of slave clusters, and each region server replicates its own stream of
            edits. For more information on the different properties of master/slave replication and other
            types of replication, see the article <link
                    xlink:href="http://highscalability.com/blog/2009/8/24/how-google-serves-data-from-multiple-datacenters.html">How
                Google Serves Data From Multiple Datacenters</link>.</para>

        <para>Replication is asynchronous, allowing clusters to be geographically distant or to have
            some gaps in availability. This also means that data between master and slave clusters will
            not be instantly consistent. Rows inserted on the master are not immediately available or
            consistent with rows on the slave clusters. rows inserted on the master cluster won’t be
            available at the same time on the slave clusters. The goal is eventual consistency. </para>

        <para>The replication format used in this design is conceptually the same as the <firstterm><link
                xlink:href="http://dev.mysql.com/doc/refman/5.1/en/replication-formats.html">statement-based
            replication</link></firstterm> design used by MySQL. Instead of SQL statements, entire
            WALEdits (consisting of multiple cell inserts coming from Put and Delete operations on the
            clients) are replicated in order to maintain atomicity. </para>

        <para>The WALs for each region server must be kept in HDFS as long as they are needed to
            replicate data to any slave cluster. Each region server reads from the oldest log it needs to
            replicate and keeps track of the current position inside ZooKeeper to simplify failure
            recovery. That position, as well as the queue of WALs to process, may be different for every
            slave cluster.</para>

        <para>The clusters participating in replication can be of different sizes. The master
            cluster relies on randomization to attempt to balance the stream of replication on the slave clusters</para>

        <para>HBase supports master/master and cyclic replication as well as replication to multiple
            slaves.</para>

        <figure>
            <title>Replication Architecture Overview</title>
            <mediaobject>
                <imageobject>
                    <imagedata fileref="replication_overview.png" />
                </imageobject>
                <textobject>
                    <para>Illustration of the replication architecture in HBase, as described in the prior
                        text.</para>
                </textobject>
            </mediaobject>
        </figure>

        <formalpara>
            <title>Enabling and Configuring Replication</title>
            <para>See the <link
                    xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/replication/package-summary.html#requirements">
                API documentation for replication</link> for information on enabling and configuring
                replication.</para>
        </formalpara>

        <section>
            <title>Life of a WAL Edit</title>
            <para>A single WAL edit goes through several steps in order to be replicated to a slave
                cluster.</para>

            <orderedlist>
                <title>When the slave responds correctly:</title>
                <listitem>
                    <para>A HBase client uses a Put or Delete operation to manipulate data in HBase.</para>
                </listitem>
                <listitem>
                    <para>The region server writes the request to the WAL in a way that would allow it to be
                        replayed if it were not written successfully.</para>
                </listitem>
                <listitem>
                    <para>If the changed cell corresponds to a column family that is scoped for replication,
                        the edit is added to the queue for replication.</para>
                </listitem>
                <listitem>
                    <para>In a separate thread, the edit is read from the log, as part of a batch process.
                        Only the KeyValues that are eligible for replication are kept. Replicable KeyValues are
                        part of a column family whose schema is scoped GLOBAL, are not part of a catalog such as
                        <code>hbase:meta</code>, and did not originate from the target slave cluster, in the
                        case of cyclic replication.</para>
                </listitem>
                <listitem>
                    <para>The edit is tagged with the master's UUID and added to a buffer. When the buffer is
                        filled, or the reader reaches the end of the file, the buffer is sent to a random region
                        server on the slave cluster.</para>
                </listitem>
                <listitem>
                    <para>The region server reads the edits sequentially and separates them into buffers, one
                        buffer per table. After all edits are read, each buffer is flushed using <link
                                xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html"
                                >HTable</link>, HBase's normal client. The master's UUID is preserved in the edits
                        they are applied, in order to allow for cyclic replication.</para>
                </listitem>
                <listitem>
                    <para>In the master, the offset for the WAL that is currently being replicated is
                        registered in ZooKeeper.</para>
                </listitem>
            </orderedlist>
            <orderedlist>
                <title>When the slave does not respond:</title>
                <listitem>
                    <para>The first three steps, where the edit is inserted, are identical.</para>
                </listitem>
                <listitem>
                    <para>Again in a separate thread, the region server reads, filters, and edits the log
                        edits in the same way as above. The slave region server does not answer the RPC
                        call.</para>
                </listitem>
                <listitem>
                    <para>The master sleeps and tries again a configurable number of times.</para>
                </listitem>
                <listitem>
                    <para>If the slave region server is still not available, the master selects a new subset
                        of region server to replicate to, and tries again to send the buffer of edits.</para>
                </listitem>
                <listitem>
                    <para>Meanwhile, the WALs are rolled and stored in a queue in ZooKeeper. Logs that are
                        <firstterm>archived</firstterm> by their region server, by moving them from the region
                        server's log directory to a central log directory, will update their paths in the
                        in-memory queue of the replicating thread.</para>
                </listitem>
                <listitem>
                    <para>When the slave cluster is finally available, the buffer is applied in the same way
                        as during normal processing. The master region server will then replicate the backlog of
                        logs that accumulated during the outage.</para>
                </listitem>
            </orderedlist>


            <note xml:id="cluster.replication.preserving.tags">
                <title>Preserving Tags During Replication</title>
                <para>By default, the codec used for replication between clusters strips tags, such as
                    cell-level ACLs, from cells. To prevent the tags from being stripped, you can use a
                    different codec which does not strip them. Configure
                    <code>hbase.replication.rpc.codec</code> to use
                    <literal>org.apache.hadoop.hbase.codec.KeyValueCodecWithTags</literal>, on both the
                    source and sink RegionServers involved in the replication. This option was introduced in
                    <link xlink:href="https://issues.apache.org/jira/browse/HBASE-10322"
                            >HBASE-10322</link>.</para>
            </note>
        </section>

        <section>
            <title>Replication Internals</title>
            <variablelist>
                <varlistentry>
                    <term>Replication State in ZooKeeper</term>
                    <listitem>
                        <para>HBase replication maintains its state in ZooKeeper. By default, the state is
                            contained in the base node <filename>/hbase/replication</filename>. This node contains
                            two child nodes, the <code>Peers</code> znode and the <code>RS</code> znode.</para>
                        <warning>
                            <para>Replication may be disrupted and data loss may occur if you delete the
                                replication tree (<filename>/hbase/replication/</filename>) from ZooKeeper. This is
                                despite the information about invariants at <xref
                                        linkend="design.invariants.zk.data"/>. Follow progress on this issue at <link
                                        xlink:href="https://issues.apache.org/jira/browse/HBASE-10295"
                                        >HBASE-10295</link>.</para>
                        </warning>
                    </listitem>
                </varlistentry>
                <varlistentry>
                    <term>The <code>Peers</code> Znode</term>
                    <listitem>
                        <para>The <code>peers</code> znode is stored in
                            <filename>/hbase/replication/peers</filename> by default. It consists of a list of
                            all peer replication clusters, along with the status of each of them. The value of
                            each peer is its cluster key, which is provided in the HBase Shell. The cluster key
                            contains a list of ZooKeeper nodes in the cluster's quorum, the client port for the
                            ZooKeeper quorum, and the base znode for HBase in HDFS on that cluster.</para>
                        <screen>
                            /hbase/replication/peers
                            /1 [Value: zk1.host.com,zk2.host.com,zk3.host.com:2181:/hbase]
                            /2 [Value: zk5.host.com,zk6.host.com,zk7.host.com:2181:/hbase]
                        </screen>
                        <para>Each peer has a child znode which indicates whether or not replication is enabled
                            on that cluster. These peer-state znodes do not contain any child znodes, but only
                            contain a Boolean value. This value is read and maintained by the
                            R<code>eplicationPeer.PeerStateTracker</code> class.</para>
                        <screen>
                            /hbase/replication/peers
                            /1/peer-state [Value: ENABLED]
                            /2/peer-state [Value: DISABLED]
                        </screen>
                    </listitem>
                </varlistentry>
                <varlistentry>
                    <term>The <code>RS</code> Znode</term>
                    <listitem>
                        <para>The <code>rs</code> znode contains a list of WAL logs which need to be replicated.
                            This list is divided into a set of queues organized by region server and the peer
                            cluster the region server is shipping the logs to. The rs znode has one child znode
                            for each region server in the cluster. The child znode name is the region server's
                            hostname, client port, and start code. This list includes both live and dead region
                            servers.</para>
                        <screen>
                            /hbase/replication/rs
                            /hostname.example.org,6020,1234
                            /hostname2.example.org,6020,2856
                        </screen>
                        <para>Each <code>rs</code> znode contains a list of WAL replication queues, one queue
                            for each peer cluster it replicates to. These queues are represented by child znodes
                            named by the cluster ID of the peer cluster they represent.</para>
                        <screen>
                            /hbase/replication/rs
                            /hostname.example.org,6020,1234
                            /1
                            /2
                        </screen>
                        <para>Each queue has one child znode for each WAL log that still needs to be replicated.
                            the value of these child znodes is the last position that was replicated. This
                            position is updated each time a WAL log is replicated.</para>
                        <screen>
                            /hbase/replication/rs
                            /hostname.example.org,6020,1234
                            /1
                            23522342.23422 [VALUE: 254]
                            12340993.22342 [VALUE: 0]
                        </screen>
                    </listitem>
                </varlistentry>
            </variablelist>
        </section>
        <section>
            <title>Replication Configuration Options</title>
            <informaltable>
                <tgroup cols="3">
                    <thead>
                        <row>
                            <entry>Option</entry>
                            <entry>Description</entry>
                            <entry>Default</entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry><para><code>zookeeper.znode.parent</code></para></entry>
                            <entry><para>The name of the base ZooKeeper znode used for HBase</para></entry>
                            <entry><para><literal>/hbase</literal></para></entry>
                        </row>
                        <row>
                            <entry><para><code>zookeeper.znode.replication</code></para></entry>
                            <entry><para>The name of the base znode used for replication</para></entry>
                            <entry><para><literal>replication</literal></para></entry>
                        </row>
                        <row>
                            <entry><para><code>zookeeper.znode.replication.peers</code></para></entry>
                            <entry><para>The name of the <code>peer</code> znode</para></entry>
                            <entry><para><literal>peers</literal></para></entry>
                        </row>
                        <row>
                            <entry><para><code>zookeeper.znode.replication.peers.state</code></para></entry>
                            <entry><para>The name of <code>peer-state</code> znode</para></entry>
                            <entry><para><literal>peer-state</literal></para></entry>
                        </row>
                        <row>
                            <entry><para><code>zookeeper.znode.replication.rs</code></para></entry>
                            <entry><para>The name of the <code>rs</code> znode</para></entry>
                            <entry><para><literal>rs</literal></para></entry>
                        </row>
                        <row>
                            <entry><para><code>hbase.replication</code></para></entry>
                            <entry><para>Whether replication is enabled or disabled on a given cluster</para></entry>
                            <entry><para><literal>false</literal></para></entry>
                        </row>
                        <row>
                            <entry><para><code>eplication.sleep.before.failover</code></para></entry>
                            <entry><para>How many milliseconds a worker should sleep before attempting to replicate
                                a dead region server's WAL queues.</para></entry>
                            <entry><para><literal></literal></para></entry>
                        </row>
                        <row>
                            <entry><para><code>replication.executor.workers</code></para></entry>
                            <entry><para>The number of region servers a given region server should attempt to
                                failover simultaneously.</para></entry>
                            <entry><para><literal>1</literal></para></entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
        </section>

        <section>
            <title>Replication Implementation Details</title>
            <formalpara>
                <title>Choosing Region Servers to Replicate To</title>
                <para>When a master cluster region server initiates a replication source to a slave cluster,
                    it first connects to the slave's ZooKeeper ensemble using the provided cluster key . It
                    then scans the <filename>rs/</filename> directory to discover all the available sinks
                    (region servers that are accepting incoming streams of edits to replicate) and randomly
                    chooses a subset of them using a configured ratio which has a default value of 10%. For
                    example, if a slave cluster has 150 machines, 15 will be chosen as potential recipient for
                    edits that this master cluster region server sends. Because this selection is performed by
                    each master region server, the probability that all slave region servers are used is very
                    high, and this method works for clusters of any size. For example, a master cluster of 10
                    machines replicating to a slave cluster of 5 machines with a ratio of 10% causes the
                    master cluster region servers to choose one machine each at random.</para>
            </formalpara>
            <para>A ZooKeeper watcher is placed on the
                <filename>${<replaceable>zookeeper.znode.parent</replaceable>}/rs</filename> node of the
                slave cluster by each of the master cluster's region servers. This watch is used to monitor
                changes in the composition of the slave cluster. When nodes are removed from the slave
                cluster, or if nodes go down or come back up, the master cluster's region servers will
                respond by selecting a new pool of slave region servers to replicate to.</para>

            <formalpara>
                <title>Keeping Track of Logs</title>

                <para>Each master cluster region server has its own znode in the replication znodes
                    hierarchy. It contains one znode per peer cluster (if 5 slave clusters, 5 znodes are
                    created), and each of these contain a queue of WALs to process. Each of these queues will
                    track the WALs created by that region server, but they can differ in size. For example, if
                    one slave cluster becomes unavailable for some time, the WALs should not be deleted, so
                    they need to stay in the queue while the others are processed. See <xref
                            linkend="rs.failover.details"/> for an example.</para>
            </formalpara>
            <para>When a source is instantiated, it contains the current WAL that the region server is
                writing to. During log rolling, the new file is added to the queue of each slave cluster's
                znode just before it is made available. This ensures that all the sources are aware that a
                new log exists before the region server is able to append edits into it, but this operations
                is now more expensive. The queue items are discarded when the replication thread cannot read
                more entries from a file (because it reached the end of the last block) and there are other
                files in the queue. This means that if a source is up to date and replicates from the log
                that the region server writes to, reading up to the "end" of the current file will not
                delete the item in the queue.</para>
            <para>A log can be archived if it is no longer used or if the number of logs exceeds
                <code>hbase.regionserver.maxlogs</code> because the insertion rate is faster than regions
                are flushed. When a log is archived, the source threads are notified that the path for that
                log changed. If a particular source has already finished with an archived log, it will just
                ignore the message. If the log is in the queue, the path will be updated in memory. If the
                log is currently being replicated, the change will be done atomically so that the reader
                doesn't attempt to open the file when has already been moved. Because moving a file is a
                NameNode operation , if the reader is currently reading the log, it won't generate any
                exception.</para>
            <formalpara>
                <title>Reading, Filtering and Sending Edits</title>
                <para>By default, a source attempts to read from a WAL and ship log entries to a sink as
                    quickly as possible. Speed is limited by the filtering of log entries Only KeyValues that
                    are scoped GLOBAL and that do not belong to catalog tables will be retained. Speed is also
                    limited by total size of the list of edits to replicate per slave, which is limited to 64
                    MB by default. With this configuration, a master cluster region server with three slaves
                    would use at most 192 MB to store data to replicate. This does not account for the data
                    which was filtered but not garbage collected.</para>
            </formalpara>
            <para>Once the maximum size of edits has been buffered or the reader reaces the end of the
                WAL, the source thread stops reading and chooses at random a sink to replicate to (from the
                list that was generated by keeping only a subset of slave region servers). It directly
                issues a RPC to the chosen region server and waits for the method to return. If the RPC was
                successful, the source determines whether the current file has been emptied or it contains
                more data which needs to be read. If the file has been emptied, the source deletes the znode
                in the queue. Otherwise, it registers the new offset in the log's znode. If the RPC threw an
                exception, the source will retry 10 times before trying to find a different sink.</para>
            <formalpara>
                <title>Cleaning Logs</title>
                <para>If replication is not enabled, the master's log-cleaning thread deletes old logs using
                    a configured TTL. This TTL-based method does not work well with replication, because
                    archived logs which have exceeded their TTL may still be in a queue. The default behavior
                    is augmented so that if a log is past its TTL, the cleaning thread looks up every queue
                    until it finds the log, while caching queues it has found. If the log is not found in any
                    queues, the log will be deleted. The next time the cleaning process needs to look for a
                    log, it starts by using its cached list.</para>
            </formalpara>
            <formalpara xml:id="rs.failover.details">
                <title>Region Server Failover</title>
                <para>When no region servers are failing, keeping track of the logs in ZooKeeper adds no
                    value. Unfortunately, region servers do fail, and since ZooKeeper is highly available, it
                    is useful for managing the transfer of the queues in the event of a failure.</para>
            </formalpara>
            <para>Each of the master cluster region servers keeps a watcher on every other region server,
                in order to be notified when one dies (just as the master does). When a failure happens,
                they all race to create a znode called <literal>lock</literal> inside the dead region
                server's znode that contains its queues. The region server that creates it successfully then
                transfers all the queues to its own znode, one at a time since ZooKeeper does not support
                renaming queues. After queues are all transferred, they are deleted from the old location.
                The znodes that were recovered are renamed with the ID of the slave cluster appended with
                the name of the dead server.</para>
            <para>Next, the master cluster region server creates one new source thread per copied queue,
                and each of the source threads follows the read/filter/ship pattern. The main difference is
                that those queues will never receive new data, since they do not belong to their new region
                server. When the reader hits the end of the last log, the queue's znode is deleted and the
                master cluster region server closes that replication source.</para>
            <para>Given a master cluster with 3 region servers replicating to a single slave with id
                <literal>2</literal>, the following hierarchy represents what the znodes layout could be
                at some point in time. The region servers' znodes all contain a <literal>peers</literal>
                znode which contains a single queue. The znode names in the queues represent the actual file
                names on HDFS in the form
                <literal><replaceable>address</replaceable>,<replaceable>port</replaceable>.<replaceable>timestamp</replaceable></literal>.</para>
            <screen>
                /hbase/replication/rs/
                1.1.1.1,60020,123456780/
                2/
                1.1.1.1,60020.1234  (Contains a position)
                1.1.1.1,60020.1265
                1.1.1.2,60020,123456790/
                2/
                1.1.1.2,60020.1214  (Contains a position)
                1.1.1.2,60020.1248
                1.1.1.2,60020.1312
                1.1.1.3,60020,    123456630/
                2/
                1.1.1.3,60020.1280  (Contains a position)
            </screen>
            <para>Assume that 1.1.1.2 loses its ZooKeeper session. The survivors will race to create a
                lock, and, arbitrarily, 1.1.1.3 wins. It will then start transferring all the queues to its
                local peers znode by appending the name of the dead server. Right before 1.1.1.3 is able to
                clean up the old znodes, the layout will look like the following:</para>
            <screen>
                /hbase/replication/rs/
                1.1.1.1,60020,123456780/
                2/
                1.1.1.1,60020.1234  (Contains a position)
                1.1.1.1,60020.1265
                1.1.1.2,60020,123456790/
                lock
                2/
                1.1.1.2,60020.1214  (Contains a position)
                1.1.1.2,60020.1248
                1.1.1.2,60020.1312
                1.1.1.3,60020,123456630/
                2/
                1.1.1.3,60020.1280  (Contains a position)

                2-1.1.1.2,60020,123456790/
                1.1.1.2,60020.1214  (Contains a position)
                1.1.1.2,60020.1248
                1.1.1.2,60020.1312
            </screen>
            <para>Some time later, but before 1.1.1.3 is able to finish replicating the last WAL from
                1.1.1.2, it dies too. Some new logs were also created in the normal queues. The last region
                server will then try to lock 1.1.1.3's znode and will begin transferring all the queues. The
                new layout will be:</para>
            <screen>
                /hbase/replication/rs/
                1.1.1.1,60020,123456780/
                2/
                1.1.1.1,60020.1378  (Contains a position)

                2-1.1.1.3,60020,123456630/
                1.1.1.3,60020.1325  (Contains a position)
                1.1.1.3,60020.1401

                2-1.1.1.2,60020,123456790-1.1.1.3,60020,123456630/
                1.1.1.2,60020.1312  (Contains a position)
                1.1.1.3,60020,123456630/
                lock
                2/
                1.1.1.3,60020.1325  (Contains a position)
                1.1.1.3,60020.1401

                2-1.1.1.2,60020,123456790/
                1.1.1.2,60020.1312  (Contains a position)
            </screen>
            <formalpara>
                <title>Replication Metrics</title>
                <para>The following metrics are exposed at the global region server level and (since HBase
                    0.95) at the peer level:</para>
            </formalpara>
            <variablelist>
                <varlistentry>
                    <term><code>source.sizeOfLogQueue</code></term>
                    <listitem>
                        <para>number of WALs to process (excludes the one which is being processed) at the
                            Replication source</para>
                    </listitem>
                </varlistentry>
                <varlistentry>
                    <term><code>source.shippedOps</code></term>
                    <listitem>
                        <para>number of mutations shipped</para>
                    </listitem>
                </varlistentry>
                <varlistentry>
                    <term><code>source.logEditsRead</code></term>
                    <listitem>
                        <para>number of mutations read from HLogs at the replication source</para>
                    </listitem>
                </varlistentry>
                <varlistentry>
                    <term><code>source.ageOfLastShippedOp</code></term>
                    <listitem>
                        <para>age of last batch that was shipped by the replication source</para>
                    </listitem>
                </varlistentry>
            </variablelist>

        </section>
    </section>
    <section
            xml:id="ops.backup">
        <title>HBase Backup</title>
        <para>There are two broad strategies for performing HBase backups: backing up with a full
            cluster shutdown, and backing up on a live cluster. Each approach has pros and cons. </para>
        <para>For additional information, see <link
                xlink:href="http://blog.sematext.com/2011/03/11/hbase-backup-options/">HBase Backup
            Options</link> over on the Sematext Blog. </para>
        <section
                xml:id="ops.backup.fullshutdown">
            <title>Full Shutdown Backup</title>
            <para>Some environments can tolerate a periodic full shutdown of their HBase cluster, for
                example if it is being used a back-end analytic capacity and not serving front-end
                web-pages. The benefits are that the NameNode/Master are RegionServers are down, so there is
                no chance of missing any in-flight changes to either StoreFiles or metadata. The obvious con
                is that the cluster is down. The steps include: </para>
            <section
                    xml:id="ops.backup.fullshutdown.stop">
                <title>Stop HBase</title>
                <para> </para>
            </section>
            <section
                    xml:id="ops.backup.fullshutdown.distcp">
                <title>Distcp</title>
                <para>Distcp could be used to either copy the contents of the HBase directory in HDFS to
                    either the same cluster in another directory, or to a different cluster. </para>
                <para>Note: Distcp works in this situation because the cluster is down and there are no
                    in-flight edits to files. Distcp-ing of files in the HBase directory is not generally
                    recommended on a live cluster. </para>
            </section>
            <section
                    xml:id="ops.backup.fullshutdown.restore">
                <title>Restore (if needed)</title>
                <para>The backup of the hbase directory from HDFS is copied onto the 'real' hbase directory
                    via distcp. The act of copying these files creates new HDFS metadata, which is why a
                    restore of the NameNode edits from the time of the HBase backup isn't required for this
                    kind of restore, because it's a restore (via distcp) of a specific HDFS directory (i.e.,
                    the HBase part) not the entire HDFS file-system. </para>
            </section>
        </section>
        <section
                xml:id="ops.backup.live.replication">
            <title>Live Cluster Backup - Replication</title>
            <para>This approach assumes that there is a second cluster. See the HBase page on <link
                    xlink:href="http://hbase.apache.org/replication.html">replication</link> for more
                information. </para>
        </section>
        <section
                xml:id="ops.backup.live.copytable">
            <title>Live Cluster Backup - CopyTable</title>
            <para>The <xref
                    linkend="copytable" /> utility could either be used to copy data from one table to another
                on the same cluster, or to copy data to another table on another cluster. </para>
            <para>Since the cluster is up, there is a risk that edits could be missed in the copy process.
            </para>
        </section>
        <section
                xml:id="ops.backup.live.export">
            <title>Live Cluster Backup - Export</title>
            <para>The <xref
                    linkend="export" /> approach dumps the content of a table to HDFS on the same cluster. To
                restore the data, the <xref
                        linkend="import" /> utility would be used. </para>
            <para>Since the cluster is up, there is a risk that edits could be missed in the export
                process. </para>
        </section>
    </section>
    <!--  backup -->

    <section
            xml:id="ops.snapshots">
        <title>HBase Snapshots</title>
        <para>HBase Snapshots allow you to take a snapshot of a table without too much impact on Region
            Servers. Snapshot, Clone and restore operations don't involve data copying. Also, Exporting
            the snapshot to another cluster doesn't have impact on the Region Servers. </para>
        <para>Prior to version 0.94.6, the only way to backup or to clone a table is to use
            CopyTable/ExportTable, or to copy all the hfiles in HDFS after disabling the table. The
            disadvantages of these methods are that you can degrade region server performance (Copy/Export
            Table) or you need to disable the table, that means no reads or writes; and this is usually
            unacceptable. </para>
        <section
                xml:id="ops.snapshots.configuration">
            <title>Configuration</title>
            <para>To turn on the snapshot support just set the <varname>hbase.snapshot.enabled</varname>
                property to true. (Snapshots are enabled by default in 0.95+ and off by default in
                0.94.6+)</para>
            <programlisting language="java">
                &lt;property>
                &lt;name>hbase.snapshot.enabled&lt;/name>
                &lt;value>true&lt;/value>
                &lt;/property>
            </programlisting>
        </section>
        <section
                xml:id="ops.snapshots.takeasnapshot">
            <title>Take a Snapshot</title>
            <para>You can take a snapshot of a table regardless of whether it is enabled or disabled. The
                snapshot operation doesn't involve any data copying.</para>
            <screen language="bourne">
                $ ./bin/hbase shell
                hbase> snapshot 'myTable', 'myTableSnapshot-122112'
            </screen>
        </section>
        <section
                xml:id="ops.snapshots.list">
            <title>Listing Snapshots</title>
            <para>List all snapshots taken (by printing the names and relative information).</para>
            <screen language="bourne">
                $ ./bin/hbase shell
                hbase> list_snapshots
            </screen>
        </section>
        <section
                xml:id="ops.snapshots.delete">
            <title>Deleting Snapshots</title>
            <para>You can remove a snapshot, and the files retained for that snapshot will be removed if
                no longer needed.</para>
            <screen language="bourne">
                $ ./bin/hbase shell
                hbase> delete_snapshot 'myTableSnapshot-122112'
            </screen>
        </section>
        <section
                xml:id="ops.snapshots.clone">
            <title>Clone a table from snapshot</title>
            <para>From a snapshot you can create a new table (clone operation) with the same data that you
                had when the snapshot was taken. The clone operation, doesn't involve data copies, and a
                change to the cloned table doesn't impact the snapshot or the original table.</para>
            <screen language="bourne">
                $ ./bin/hbase shell
                hbase> clone_snapshot 'myTableSnapshot-122112', 'myNewTestTable'
            </screen>
        </section>
        <section
                xml:id="ops.snapshots.restore">
            <title>Restore a snapshot</title>
            <para>The restore operation requires the table to be disabled, and the table will be restored
                to the state at the time when the snapshot was taken, changing both data and schema if
                required.</para>
            <screen language="bourne">
                $ ./bin/hbase shell
                hbase> disable 'myTable'
                hbase> restore_snapshot 'myTableSnapshot-122112'
            </screen>
            <note>
                <para>Since Replication works at log level and snapshots at file-system level, after a
                    restore, the replicas will be in a different state from the master. If you want to use
                    restore, you need to stop replication and redo the bootstrap. </para>
            </note>
            <para>In case of partial data-loss due to misbehaving client, instead of a full restore that
                requires the table to be disabled, you can clone the table from the snapshot and use a
                Map-Reduce job to copy the data that you need, from the clone to the main one. </para>
        </section>
        <section
                xml:id="ops.snapshots.acls">
            <title>Snapshots operations and ACLs</title>
            <para>If you are using security with the AccessController Coprocessor (See <xref
                    linkend="hbase.accesscontrol.configuration" />), only a global administrator can take,
                clone, or restore a snapshot, and these actions do not capture the ACL rights. This means
                that restoring a table preserves the ACL rights of the existing table, while cloning a table
                creates a new table that has no ACL rights until the administrator adds them.</para>
        </section>
        <section
                xml:id="ops.snapshots.export">
            <title>Export to another cluster</title>
            <para>The ExportSnapshot tool copies all the data related to a snapshot (hfiles, logs,
                snapshot metadata) to another cluster. The tool executes a Map-Reduce job, similar to
                distcp, to copy files between the two clusters, and since it works at file-system level the
                hbase cluster does not have to be online.</para>
            <para>To copy a snapshot called MySnapshot to an HBase cluster srv2 (hdfs:///srv2:8082/hbase)
                using 16 mappers:</para>
            <programlisting language="bourne">$ bin/hbase class org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot MySnapshot -copy-to hdfs://srv2:8082/hbase -mappers 16</programlisting>
            <formalpara>
                <title>Limiting Bandwidth Consumption</title>
                <para>You can limit the bandwidth consumption when exporting a snapshot, by specifying the
                    <code>-bandwidth</code> parameter, which expects an integer representing megabytes per
                    second. The following example limits the above example to 200 MB/sec.</para>
            </formalpara>
            <programlisting language="bourne">$ bin/hbase class org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot MySnapshot -copy-to hdfs://srv2:8082/hbase -mappers 16 -bandwidth 200</programlisting>
        </section>
    </section>
    <!--  snapshots -->

    <section
            xml:id="ops.capacity">
        <title>Capacity Planning and Region Sizing</title>
        <para>There are several considerations when planning the capacity for an HBase cluster and
            performing the initial configuration. Start with a solid understanding of how HBase handles
            data internally.</para>
        <section
                xml:id="ops.capacity.nodes">
            <title>Node count and hardware/VM configuration</title>
            <section
                    xml:id="ops.capacity.nodes.datasize">
                <title>Physical data size</title>
                <para>Physical data size on disk is distinct from logical size of your data and is affected
                    by the following: </para>
                <itemizedlist>
                    <listitem>
                        <para>Increased by HBase overhead</para>
                        <itemizedlist>
                            <listitem>
                                <para>See <xref
                                        linkend="keyvalue" /> and <xref
                                        linkend="keysize" />. At least 24 bytes per key-value (cell), can be more. Small
                                    keys/values means more relative overhead.</para>
                            </listitem>
                            <listitem>
                                <para>KeyValue instances are aggregated into blocks, which are indexed. Indexes also
                                    have to be stored. Blocksize is configurable on a per-ColumnFamily basis. See <xref
                                            linkend="regions.arch" />.</para>
                            </listitem>
                        </itemizedlist>
                    </listitem>
                    <listitem>
                        <para>Decreased by <xref
                                linkend="compression"
                                xrefstyle="template:compression" /> and data block encoding, depending on data. See
                            also <link
                                    xlink:href="http://search-hadoop.com/m/lL12B1PFVhp1">this thread</link>. You might
                            want to test what compression and encoding (if any) make sense for your data.</para>
                    </listitem>
                    <listitem>
                        <para>Increased by size of region server <xref
                                linkend="wal"
                                xrefstyle="template:WAL" /> (usually fixed and negligible - less than half of RS
                            memory size, per RS).</para>
                    </listitem>
                    <listitem>
                        <para>Increased by HDFS replication - usually x3.</para>
                    </listitem>
                </itemizedlist>
                <para>Aside from the disk space necessary to store the data, one RS may not be able to serve
                    arbitrarily large amounts of data due to some practical limits on region count and size
                    (see <xref
                            linkend="ops.capacity.regions"
                            xrefstyle="template:below" />).</para>
            </section>
            <!-- ops.capacity.nodes.datasize -->
            <section
                    xml:id="ops.capacity.nodes.throughput">
                <title>Read/Write throughput</title>
                <para>Number of nodes can also be driven by required thoughput for reads and/or writes. The
                    throughput one can get per node depends a lot on data (esp. key/value sizes) and request
                    patterns, as well as node and system configuration. Planning should be done for peak load
                    if it is likely that the load would be the main driver of the increase of the node count.
                    PerformanceEvaluation and <xref
                            linkend="ycsb"
                            xrefstyle="template:YCSB" /> tools can be used to test single node or a test
                    cluster.</para>
                <para>For write, usually 5-15Mb/s per RS can be expected, since every region server has only
                    one active WAL. There's no good estimate for reads, as it depends vastly on data,
                    requests, and cache hit rate. <xref
                            linkend="perf.casestudy" /> might be helpful.</para>
            </section>
            <!-- ops.capacity.nodes.throughput -->
            <section
                    xml:id="ops.capacity.nodes.gc">
                <title>JVM GC limitations</title>
                <para>RS cannot currently utilize very large heap due to cost of GC. There's also no good
                    way of running multiple RS-es per server (other than running several VMs per machine).
                    Thus, ~20-24Gb or less memory dedicated to one RS is recommended. GC tuning is required
                    for large heap sizes. See <xref
                            linkend="gcpause" />, <xref
                            linkend="trouble.log.gc" /> and elsewhere (TODO: where?)</para>
            </section>
            <!-- ops.capacity.nodes.gc -->
        </section>
        <!-- ops.capacity.nodes -->
        <section
                xml:id="ops.capacity.regions">
            <title>Determining region count and size</title>
            <para>Generally less regions makes for a smoother running cluster (you can always manually
                split the big regions later (if necessary) to spread the data, or request load, over the
                cluster); 20-200 regions per RS is a reasonable range. The number of regions cannot be
                configured directly (unless you go for fully <xref
                        linkend="disable.splitting"
                        xrefstyle="template:manual splitting" />); adjust the region size to achieve the target
                region size given table size.</para>
            <para>When configuring regions for multiple tables, note that most region settings can be set
                on a per-table basis via <link
                        xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HTableDescriptor.html">HTableDescriptor</link>,
                as well as shell commands. These settings will override the ones in
                <varname>hbase-site.xml</varname>. That is useful if your tables have different
                workloads/use cases.</para>
            <para>Also note that in the discussion of region sizes here, <emphasis
                    role="bold">HDFS replication factor is not (and should not be) taken into account, whereas
                other factors <xref
                        linkend="ops.capacity.nodes.datasize"
                        xrefstyle="template:above" /> should be.</emphasis> So, if your data is compressed and
                replicated 3 ways by HDFS, "9 Gb region" means 9 Gb of compressed data. HDFS replication
                factor only affects your disk usage and is invisible to most HBase code.</para>
            <section
                    xml:id="ops.capacity.regions.count">
                <title>Number of regions per RS - upper bound</title>
                <para>In production scenarios, where you have a lot of data, you are normally concerned with
                    the maximum number of regions you can have per server. <xref
                            linkend="too_many_regions" /> has technical discussion on the subject; in short, maximum
                    number of regions is mostly determined by memstore memory usage. Each region has its own
                    memstores; these grow up to a configurable size; usually in 128-256Mb range, see <xref
                            linkend="hbase.hregion.memstore.flush.size" />. There's one memstore per column family
                    (so there's only one per region if there's one CF in the table). RS dedicates some
                    fraction of total memory (see <xref
                            linkend="hbase.regionserver.global.memstore.size" />) to region memstores. If this
                    memory is exceeded (too much memstore usage), undesirable consequences such as
                    unresponsive server, or later compaction storms, can result. Thus, a good starting point
                    for the number of regions per RS (assuming one table) is:</para>

                <programlisting>(RS memory)*(total memstore fraction)/((memstore size)*(# column families))</programlisting>
                <para> E.g. if RS has 16Gb RAM, with default settings, it is 16384*0.4/128 ~ 51 regions per
                    RS is a starting point. The formula can be extended to multiple tables; if they all have
                    the same configuration, just use total number of families.</para>
                <para>This number can be adjusted; the formula above assumes all your regions are filled at
                    approximately the same rate. If only a fraction of your regions are going to be actively
                    written to, you can divide the result by that fraction to get a larger region count. Then,
                    even if all regions are written to, all region memstores are not filled evenly, and
                    eventually jitter appears even if they are (due to limited number of concurrent flushes).
                    Thus, one can have as many as 2-3 times more regions than the starting point; however,
                    increased numbers carry increased risk.</para>
                <para>For write-heavy workload, memstore fraction can be increased in configuration at the
                    expense of block cache; this will also allow one to have more regions.</para>
            </section>
            <!-- ops.capacity.regions.count -->
            <section
                    xml:id="ops.capacity.regions.mincount">
                <title>Number of regions per RS - lower bound</title>
                <para>HBase scales by having regions across many servers. Thus if you have 2 regions for
                    16GB data, on a 20 node machine your data will be concentrated on just a few machines -
                    nearly the entire cluster will be idle. This really can't be stressed enough, since a
                    common problem is loading 200MB data into HBase and then wondering why your awesome 10
                    node cluster isn't doing anything.</para>
                <para>On the other hand, if you have a very large amount of data, you may also want to go
                    for a larger number of regions to avoid having regions that are too large.</para>
            </section>
            <!-- ops.capacity.regions.mincount -->
            <section
                    xml:id="ops.capacity.regions.size">
                <title>Maximum region size</title>
                <para>For large tables in production scenarios, maximum region size is mostly limited by
                    compactions - very large compactions, esp. major, can degrade cluster performance.
                    Currently, the recommended maximum region size is 10-20Gb, and 5-10Gb is optimal. For
                    older 0.90.x codebase, the upper-bound of regionsize is about 4Gb, with a default of
                    256Mb.</para>
                <para>The size at which the region is split into two is generally configured via <xref
                        linkend="hbase.hregion.max.filesize" />; for details, see <xref
                        linkend="arch.region.splits" />.</para>
                <para>If you cannot estimate the size of your tables well, when starting off, it's probably
                    best to stick to the default region size, perhaps going smaller for hot tables (or
                    manually split hot regions to spread the load over the cluster), or go with larger region
                    sizes if your cell sizes tend to be largish (100k and up).</para>
                <para>In HBase 0.98, experimental stripe compactions feature was added that would allow for
                    larger regions, especially for log data. See <xref
                            linkend="ops.stripe" />.</para>
            </section>
            <!-- ops.capacity.regions.size -->
            <section
                    xml:id="ops.capacity.regions.total">
                <title>Total data size per region server</title>
                <para>According to above numbers for region size and number of regions per region server, in
                    an optimistic estimate 10 GB x 100 regions per RS will give up to 1TB served per region
                    server, which is in line with some of the reported multi-PB use cases. However, it is
                    important to think about the data vs cache size ratio at the RS level. With 1TB of data
                    per server and 10 GB block cache, only 1% of the data will be cached, which may barely
                    cover all block indices.</para>
            </section>
            <!-- ops.capacity.regions.total -->
        </section>
        <!-- ops.capacity.regions -->
        <section
                xml:id="ops.capacity.config">
            <title>Initial configuration and tuning</title>
            <para>First, see <xref
                    linkend="important_configurations" />. Note that some configurations, more than others,
                depend on specific scenarios. Pay special attention to:</para>
            <itemizedlist>
                <listitem>
                    <para><xref
                            linkend="hbase.regionserver.handler.count" /> - request handler thread count, vital
                        for high-throughput workloads.</para>
                </listitem>
                <listitem>
                    <para><xref
                            linkend="config.wals" /> - the blocking number of WAL files depends on your memstore
                        configuration and should be set accordingly to prevent potential blocking when doing
                        high volume of writes.</para>
                </listitem>
            </itemizedlist>
            <para>Then, there are some considerations when setting up your cluster and tables.</para>
            <section
                    xml:id="ops.capacity.config.compactions">
                <title>Compactions</title>
                <para>Depending on read/write volume and latency requirements, optimal compaction settings
                    may be different. See <xref
                            linkend="compaction" /> for some details.</para>
                <para>When provisioning for large data sizes, however, it's good to keep in mind that
                    compactions can affect write throughput. Thus, for write-intensive workloads, you may opt
                    for less frequent compactions and more store files per regions. Minimum number of files
                    for compactions (<varname>hbase.hstore.compaction.min</varname>) can be set to higher
                    value; <xref
                            linkend="hbase.hstore.blockingStoreFiles" /> should also be increased, as more files
                    might accumulate in such case. You may also consider manually managing compactions: <xref
                            linkend="managed.compactions" /></para>
            </section>
            <!-- ops.capacity.config.compactions -->
            <section
                    xml:id="ops.capacity.config.presplit">
                <title>Pre-splitting the table</title>
                <para>Based on the target number of the regions per RS (see <xref
                        linkend="ops.capacity.regions.count"
                        xrefstyle="template:above" />) and number of RSes, one can pre-split the table at
                    creation time. This would both avoid some costly splitting as the table starts to fill up,
                    and ensure that the table starts out already distributed across many servers.</para>
                <para>If the table is expected to grow large enough to justify that, at least one region per
                    RS should be created. It is not recommended to split immediately into the full target
                    number of regions (e.g. 50 * number of RSes), but a low intermediate value can be chosen.
                    For multiple tables, it is recommended to be conservative with presplitting (e.g.
                    pre-split 1 region per RS at most), especially if you don't know how much each table will
                    grow. If you split too much, you may end up with too many regions, with some tables having
                    too many small regions.</para>
                <para>For pre-splitting howto, see <xref
                        linkend="precreate.regions" />.</para>
            </section>
            <!-- ops.capacity.config.presplit -->
        </section>
        <!-- ops.capacity.config -->
    </section>
    <!-- ops.capacity -->
    <section
            xml:id="table.rename">
        <title>Table Rename</title>
        <para>In versions 0.90.x of hbase and earlier, we had a simple script that would rename the hdfs
            table directory and then do an edit of the hbase:meta table replacing all mentions of the old
            table name with the new. The script was called <command>./bin/rename_table.rb</command>. The
            script was deprecated and removed mostly because it was unmaintained and the operation
            performed by the script was brutal. </para>
        <para> As of hbase 0.94.x, you can use the snapshot facility renaming a table. Here is how you
            would do it using the hbase shell:</para>
        <screen><![CDATA[hbase shell> disable 'tableName'
hbase shell> snapshot 'tableName', 'tableSnapshot'
hbase shell> clone_snapshot 'tableSnapshot', 'newTableName'
hbase shell> delete_snapshot 'tableSnapshot'
hbase shell> drop 'tableName']]></screen>
        <para>or in code it would be as follows:</para>
        <programlisting language="Java">void rename(HBaseAdmin admin, String oldTableName, String newTableName) {
            String snapshotName = randomName();
            admin.disableTable(oldTableName);
            admin.snapshot(snapshotName, oldTableName);
            admin.cloneSnapshot(snapshotName, newTableName);
            admin.deleteSnapshot(snapshotName);
            admin.deleteTable(oldTableName);
            }</programlisting>

    </section>

</chapter>
