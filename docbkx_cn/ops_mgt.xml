<?xml version="1.0" encoding="UTF-8"?>
<chapter
        version="5.0"
        xml:id="ops_mgt"
        xmlns="http://docbook.org/ns/docbook"
        xmlns:xlink="http://www.w3.org/1999/xlink"
        xmlns:xi="http://www.w3.org/2001/XInclude"
        xmlns:svg="http://www.w3.org/2000/svg"
        xmlns:m="http://www.w3.org/1998/Math/MathML"
        xmlns:html="http://www.w3.org/1999/xhtml"
        xmlns:db="http://docbook.org/ns/docbook">
    <!--
  /**
   * Licensed to the Apache Software Foundation (ASF) under one
   * or more contributor license agreements.  See the NOTICE file
   * distributed with this work forf additional information
   * regarding copyright ownership.  The ASF licenses this file
   * to you under the Apache License, Version 2.0 (the
   * "License"); you may not use this file except in compliance
   * with the License.  You may obtain a copy of the License at
   *
   *     http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   */
  -->
    <title>Apache HBase 运维管理</title>
    <para> 这一章将介绍运行HBase集群需要的一些运维工具和实践操作。
        讲述的主题与<xref
                linkend="trouble" />, <xref
                linkend="performance" />, 和 <xref
                linkend="configuration" /> 相关，但也有其独特性。</para>

    <section
            xml:id="tools">
        <title>HBase 工具和实用程序</title>

        <para>HBase 提供了一些工具，用来管理、分析和调试集群。
            其中大部分工具的入口点是 <filename>bin/hbase</filename>命令，有一些在 <filename>dev-support/</filename>目录。
        </para>
        <para> 通过无参或 带<option>-h</option> 运行<filename>bin/hbase</filename>命令，可以查看其使用说明。
            下面这些是HBase 0.98.x相关命令的使用说明。某些命令，如<command>version</command>, <command>pe</command>,
            <command>ltt</command>, <command>clean</command> 在之前的版本中并不可用。</para>
        <screen>
            $ <userinput>bin/hbase</userinput>
            <![CDATA[Usage: hbase [<options>] <command> [<args>]]]>
            Options:
            --config DIR    Configuration direction to use. Default: ./conf
            --hosts HOSTS   Override the list in 'regionservers' file

            Commands:
            Some commands take arguments. Pass no args or -h for usage.
            shell           Run the HBase shell
            hbck            Run the hbase 'fsck' tool
            hlog            Write-ahead-log analyzer
            hfile           Store file analyzer
            zkcli           Run the ZooKeeper shell
            upgrade         Upgrade hbase
            master          Run an HBase HMaster node
            regionserver    Run an HBase HRegionServer node
            zookeeper       Run a Zookeeper server
            rest            Run an HBase REST server
            thrift          Run the HBase Thrift server
            thrift2         Run the HBase Thrift2 server
            clean           Run the HBase clean up script
            classpath       Dump hbase CLASSPATH
            mapredcp        Dump CLASSPATH entries required by mapreduce
            pe              Run PerformanceEvaluation
            ltt             Run LoadTestTool
            version         Print the version
            CLASSNAME       Run the class named CLASSNAME
        </screen>
        <para>下面的一些工具和实用程序，是直接传递给<filename>bin/hbase</filename>命令的Java类，如使用说明最后一行所示。
            另外，  <command>hbase shell</command> (<xref linkend="shell"/>),
            <command>hbase upgrade</command> (<xref linkend="upgrading"/>), 和 <command>hbase
                thrift</command> (<xref linkend="thrift"/>) 记录在本指南的其它地方。</para>
        <section
                xml:id="canary">
            <title>Canary</title>
            <para> 这个Canary类能帮组用户canary-test  HBase集群状态，对每个region或regionserver的每个列镞。
                使用<literal>--help</literal>参数，可以查看具体用法。 </para>
            <screen language="bourne">$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.tool.Canary -help

                Usage: bin/hbase org.apache.hadoop.hbase.tool.Canary [opts] [table1 [table2]...] | [regionserver1 [regionserver2]..]
                where [opts] are:
                -help          Show this help and exit.
                -regionserver  replace the table argument to regionserver,
                which means to enable regionserver mode
                -daemon        Continuous check at defined intervals.
                -interval &lt;N>  Interval between checks (sec)
                -e             Use region/regionserver as regular expression
                which means the region/regionserver is regular expression pattern
                -f &lt;B>         stop whole program if first error occurs, default is true
                -t &lt;N>         timeout for a check, default is 600000 (milliseconds)</screen>
            <para>这个工具能返回非零错误代码，可用于与其他管理工具协作，比如Nagios。 错误代码定义如下： </para>
            <programlisting language="java">private static final int USAGE_EXIT_CODE = 1;
                private static final int INIT_ERROR_EXIT_CODE = 2;
                private static final int TIMEOUT_ERROR_EXIT_CODE = 3;
                private static final int ERROR_EXIT_CODE = 4;</programlisting>
            <para> 这里有些例子（基于以下给定的情况）， 两个名为test-01和test-02的HTable, 它们有两个列镞分别是cf1 和 cf2,
                部署在3个regionserver上。见下表： </para>

            <informaltable>
                <tgroup
                        cols="3"
                        align="center"
                        colsep="1"
                        rowsep="1">
                    <colspec
                            colname="regionserver"
                            align="center" />
                    <colspec
                            colname="test-01"
                            align="center" />
                    <colspec
                            colname="test-02"
                            align="center" />
                    <thead>
                        <row>
                            <entry>RegionServer</entry>
                            <entry>test-01</entry>
                            <entry>test-02</entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry>rs1</entry>
                            <entry>r1</entry>
                            <entry>r2</entry>
                        </row>
                        <row>
                            <entry>rs2</entry>
                            <entry>r2</entry>
                            <entry />
                        </row>
                        <row>
                            <entry>rs3</entry>
                            <entry>r2</entry>
                            <entry>r1</entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
            <para>基于以上情况，下面是一些例子。 </para>
            <section>
                <title>为每个表的每个region的每个列镞做Canary test</title>
                <screen language="bourne">$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.tool.Canary

                    3/12/09 03:26:32 INFO tool.Canary: read from region test-01,,1386230156732.0e3c7d77ffb6361ea1b996ac1042ca9a. column family cf1 in 2ms
                    13/12/09 03:26:32 INFO tool.Canary: read from region test-01,,1386230156732.0e3c7d77ffb6361ea1b996ac1042ca9a. column family cf2 in 2ms
                    13/12/09 03:26:32 INFO tool.Canary: read from region test-01,0004883,1386230156732.87b55e03dfeade00f441125159f8ca87. column family cf1 in 4ms
                    13/12/09 03:26:32 INFO tool.Canary: read from region test-01,0004883,1386230156732.87b55e03dfeade00f441125159f8ca87. column family cf2 in 1ms
                    ...
                    13/12/09 03:26:32 INFO tool.Canary: read from region test-02,,1386559511167.aa2951a86289281beee480f107bb36ee. column family cf1 in 5ms
                    13/12/09 03:26:32 INFO tool.Canary: read from region test-02,,1386559511167.aa2951a86289281beee480f107bb36ee. column family cf2 in 3ms
                    13/12/09 03:26:32 INFO tool.Canary: read from region test-02,0004883,1386559511167.cbda32d5e2e276520712d84eaaa29d84. column family cf1 in 31ms
                    13/12/09 03:26:32 INFO tool.Canary: read from region test-02,0004883,1386559511167.cbda32d5e2e276520712d84eaaa29d84. column family cf2 in 8ms
                </screen>
                <para> 可见，表test-01有两个region和两个列镞, 因此 Canary 工具从4个（2 region*2 store）不同的store中去了4小块数据，
                    这只是该工具的一种缺省行为。
                </para>
            </section>

            <section>
                <title>为指定表的每个region的每个列镞（store）做 Canary test</title>
                <para> 可以对一个或多个表检测：</para>
                <screen language="bourne">$ ${HBASE_HOME}/bin/hbase orghapache.hadoop.hbase.tool.Canary test-01 test-02</screen>
            </section>

            <section>
                <title> regionserver 粒度的Canary test </title>
                <para> 这种检测将从每个regionserver中取一小块数据，当然你也可以指定regionserver的名字作为参数进行canary-test。</para>
                <screen language="bourne">$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.tool.Canary -regionserver

                    13/12/09 06:05:17 INFO tool.Canary: Read from table:test-01 on region server:rs2 in 72ms
                    13/12/09 06:05:17 INFO tool.Canary: Read from table:test-02 on region server:rs3 in 34ms
                    13/12/09 06:05:17 INFO tool.Canary: Read from table:test-01 on region server:rs1 in 56ms</screen>
            </section>
            <section>
                <title> 正则表达式模式的Canary test </title>
                <para> 下面会同时检测表test-01和test-02.</para>
                <screen language="bourne">$ ${HBASE_HOME}/bin/hbase orghapache.hadoop.hbase.tool.Canary -e test-0[1-2]</screen>
            </section>

            <section>
                <title>以守护进程模式运行 canary test</title>
                <para> 按间隔时间重复运行的选项是-interval，默认时间是6秒。选项-f默认值为true,当出现任何错误时，守护进程将终止并返回非零错误代码。
                </para>
                <screen language="bourne">$ ${HBASE_HOME}/bin/hbase orghapache.hadoop.hbase.tool.Canary -daemon</screen>
                <para> 下面例子，定义重复运行的间隔时间为5秒，并且就算出错也不会停止运行。
                    the test.</para>
                <screen language="bourne">$ ${HBASE_HOME}/bin/hbase orghapache.hadoop.hbase.tool.Canary -daemon -interval 50000 -f false</screen>
            </section>

            <section>
                <title>如果canary test卡住将强制超时</title>
                <para>在某些情况下，我们会遇到请求阻塞在regionserver，无法给客户端返回响应的问题。
                    regionserver出现问题，通过Master也不能表明其已死（would also not indicated to be dead by
                    Master）,这将会导致客户端挂起。所以我们提供超时选项来强制终止canary test并返回非零错误代码。下面例子将超时时间设置为60秒，默认是600秒。
                </para>
                <screen language="bourne">$ ${HBASE_HOME}/bin/hbase orghapache.hadoop.hbase.tool.Canary -t 600000</screen>
            </section>

        </section>

        <section
                xml:id="health.check">
            <title>Health Checker</title>
            <para>你可以配置HBase在一段时间内运行一个脚本，如果失败了N次（可配置），则退出服务。配置及细节见 <link
                    xlink:href="">HBASE-7351 Periodic health check script</link>  </para>
        </section>

        <section
                xml:id="driver">
            <title>驱动</title>
            <para>一些频繁使用的工具以<code>驱动</code>类的方式提供，通过<filename>bin/hbase</filename>命令执行。
                这些工具是一些运行在你的集群上的MapReduce任务，它们按下面的方式运行，
                用你想运行的工具替换<replaceable>UtilityName</replaceable>。这个命令需要你将环境变量<literal>HBASE_HOME</literal>设置为HBase的安装目录。
              </para>
            <screen language="bourne">
                ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.mapreduce.<replaceable>UtilityName</replaceable>
            </screen>
            <para>有下列工具可以使用:</para>
            <variablelist>
                <varlistentry>
                    <term><command>LoadIncrementalHFiles</command></term>
                    <listitem><para>完成批量数据加载</para></listitem>
                </varlistentry>
                <varlistentry>
                    <term><command>CopyTable</command></term>
                    <listitem><para>从本地集群导出一个表到另一集群</para></listitem>
                </varlistentry>
                <varlistentry>
                    <term><command>Export</command></term>
                    <listitem><para>将表数据写入HDFS</para></listitem>
                </varlistentry>
                <varlistentry>
                    <term><command>Import</command></term>
                    <listitem><para>导入之前 <command>Export</command>操作所写的数据</para></listitem>
                </varlistentry>
                <varlistentry>
                    <term><command>ImportTsv</command></term>
                    <listitem><para>以TSV格式导入数据</para></listitem>
                </varlistentry>
                <varlistentry>
                    <term><command>RowCounter</command></term>
                    <listitem><para>计算HBase表的行数</para></listitem>
                </varlistentry>
                <varlistentry>
                    <term><command>replication.VerifyReplication</command></term>
                    <listitem><para>比较两个不同集群的表的数据。警告：
                        不适合incrementColumnValues单元格，因为时间戳已经改变。注意，这个命令在一个不同包里。</para></listitem>
                </varlistentry>
            </variablelist>
            <para>除<command>RowCounter</command>之外的命令，都可以通过<literal>--help</literal>参数查看使用说明。
                </para>
        </section>
        <section
                xml:id="hbck">
            <title>HBase <application>hbck</application></title>
            <subtitle>HBase安装中的 <command>fsck</command> </subtitle>
            <para>对你的HBase集群执行<command>$
                ./bin/hbase hbck</command>命令，最后会输出<literal>OK</literal>或 <literal>INCONSISTENCY</literal>。
                如果你的集群出现inconsistencies, 通过 <command>-details</command>能查看更多细节。
                如果inconsistencies, 再运行<command>hbck</command>几次，因为inconsistency可能是暂时性的（如集群启动或region分裂）。
                通过 <command>-fix</command> 也许能修复 inconsistency (这个是一个实验性功能)。 </para>
            <para>更多信息，参见<xref
                    linkend="hbck.in.depth" />. </para>
        </section>
        <section
                xml:id="hfile_tool2">
            <title>HFile 工具</title>
            <para>见 <xref
                    linkend="hfile_tool" />.</para>
        </section>
        <section
                xml:id="wal_tools">
            <title>WAL 工具</title>

            <section
                    xml:id="hlog_tool">
                <title><classname>FSHLog</classname> 工具</title>

                <para><classname>FSHLog</classname>的main方法提供手动切分和转储工具。
                    传给它 WALs 或需要切分的日志， <filename>recovered.edits</filename>. 目录中内容。</para>

                <para>如下所做，可以得到一个WAL文件的文本转储：</para>
                <screen language="bourne"> $ ./bin/hbase org.apache.hadoop.hbase.regionserver.wal.FSHLog --dump hdfs://example.org:8020/hbase/.logs/example.org,60020,1283516293161/10.10.21.10%3A60020.1283973724012 </screen>
                <para>如果文件有问题则返回非零，因此你可以将 <varname>STDOUT</varname> 重定向<code>/dev/null</code>，并测试文件是否健全</para>

                <para>与此类似，你也可以强制切分一个日志文件目录：</para>
                <screen language="bourne"> $ ./bin/hbase org.apache.hadoop.hbase.regionserver.wal.FSHLog --split hdfs://example.org:8020/hbase/.logs/example.org,60020,1283516293161/</screen>

                <section
                        xml:id="hlog_tool.prettyprint">
                    <title><classname>HLogPrettyPrinter</classname></title>
                    <para><classname>HLogPrettyPrinter</classname> 是一个打印HLog内容的工具。 </para>
                </section>

            </section>
        </section>
        <section
                xml:id="compression.tool">
            <title>压缩工具</title>
            <para>见 <xref
                    linkend="compression.test" />.</para>
        </section>
        <section
                xml:id="copytable">
            <title>CopyTable</title>
            <para> CopyTable用来拷贝一个表的部分或全部内容, 可以拷贝到同一集群或其它集群。目标表必须存在。用法如下：</para>

            <screen language="bourne">
                $ <userinput>./bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable --help </userinput>
                /bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable --help
                Usage: CopyTable [general options] [--starttime=X] [--endtime=Y] [--new.name=NEW] [--peer.adr=ADR] &lt;tablename&gt;

                Options:
                rs.class     hbase.regionserver.class of the peer cluster,
                specify if different from current cluster
                rs.impl      hbase.regionserver.impl of the peer cluster,
                startrow     the start row
                stoprow      the stop row
                starttime    beginning of the time range (unixtime in millis)
                without endtime means from starttime to forever
                endtime      end of the time range.  Ignored if no starttime specified.
                versions     number of cell versions to copy
                new.name     new table's name
                peer.adr     Address of the peer cluster given in the format
                hbase.zookeeer.quorum:hbase.zookeeper.client.port:zookeeper.znode.parent
                families     comma-separated list of families to copy
                To copy from cf1 to cf2, give sourceCfName:destCfName.
                To keep the same name, just give "cfName"
                all.cells    also copy delete markers and deleted cells

                Args:
                tablename    Name of the table to copy

                Examples:
                To copy 'TestTable' to a cluster that uses replication for a 1 hour window:
                $ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable --starttime=1265875194289 --endtime=1265878794289 --peer.adr=server1,server2,server3:2181:/hbase --families=myOldCf:myNewCf,cf2,cf3 TestTable

                For performance consider the following general options:
                It is recommended that you set the following to >=100. A higher value uses more memory but
                decreases the round trip time to the server and may increase performance.
                -Dhbase.client.scanner.caching=100
                The following should always be set to false, to prevent writing data twice, which may produce
                inaccurate results.
                -Dmapred.map.tasks.speculative.execution=false
            </screen>
            <note>
                <title>Scanner 缓存</title>
                <para>Scan缓存可通过job configuration中的<code>hbase.client.scanner.caching</code>配置。</para>
            </note>
            <note>
                <title>Versions</title>
                <para>默认情况下，CopyTable工具只拷贝行单元格的最新版本，除非在命令中指定<code>--versions=n</code>。 </para>
            </note>
            <para> Jonathan Hsieh's <link
                    xlink:href="http://www.cloudera.com/blog/2012/06/online-hbase-backups-with-copytable-2/">Online
                HBase Backups with CopyTable</link> 博客上有关于 <command>CopyTable</command>的更多内容。
            </para>
        </section>
        <section
                xml:id="export">
            <title>Export</title>
            <para>Export工具用来将表数据转储成HDFS中的序列化文件。
                调用如下:</para>
            <screen language="bourne">$ bin/hbase org.apache.hadoop.hbase.mapreduce.Export &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]]]
            </screen>
        </section>
        <section
                xml:id="import">
            <title>Import</title>
            <para>Import工具可以将导出的数据重新导入回HBase。调用如下:</para>
            <screen language="bourne">$ bin/hbase org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt;
            </screen>
            <para>如果要导入0.94版的导出文件到0.96版（或更高）的集群，你还需要设置系统属性"hbase.import.version"，命令如下：
                </para>
            <screen language="bourne">$ bin/hbase -Dhbase.import.version=0.94 org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt;
            </screen>
        </section>
        <section
                xml:id="importtsv">
            <title>ImportTsv</title>
            <para>ImportTsv工具将以TSV格式导入数据到HBase。 它有两个不同用法：
                1、通过Puts导入HDFS中TSV格式数据到HBase；2、通过<code>completebulkload</code>准备要导入的StoreFiles。
                </para>
            <para>通过Puts导入数据 (非大批量导入):</para>
            <screen language="bourne">$ bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=a,b,c &lt;tablename&gt; &lt;hdfs-inputdir&gt;
            </screen>

            <para>生成供大批量导入的StoreFiles </para>
            <programlisting language="bourne">$ bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=a,b,c -Dimporttsv.bulk.output=hdfs://storefile-outputdir &lt;tablename&gt; &lt;hdfs-data-inputdir&gt;
            </programlisting>
            <para>这些生成的StoreFiles能通过<xref
                    linkend="completebulkload" />导入HBase。 </para>
            <section
                    xml:id="importtsv.options">
                <title>ImportTsv 选项</title>
                <para>无参数执行 <command>ImportTsv</command> 命令会打印简单使用信息：</para>
                <screen>
                    Usage: importtsv -Dimporttsv.columns=a,b,c &lt;tablename&gt; &lt;inputdir&gt;

                    Imports the given input directory of TSV data into the specified table.

                    The column names of the TSV data must be specified using the -Dimporttsv.columns
                    option. This option takes the form of comma-separated column names, where each
                    column name is either a simple column family, or a columnfamily:qualifier. The special
                    column name HBASE_ROW_KEY is used to designate that this column should be used
                    as the row key for each imported record. You must specify exactly one column
                    to be the row key, and you must specify a column name for every column that exists in the
                    input data.

                    By default importtsv will load data directly into HBase. To instead generate
                    HFiles of data to prepare for a bulk data load, pass the option:
                    -Dimporttsv.bulk.output=/path/for/output
                    Note: the target table will be created with default column family descriptors if it does not already exist.

                    Other options that may be specified with -D include:
                    -Dimporttsv.skip.bad.lines=false - fail if encountering an invalid line
                    '-Dimporttsv.separator=|' - eg separate on pipes instead of tabs
                    -Dimporttsv.timestamp=currentTimeAsLong - use the specified timestamp for the import
                    -Dimporttsv.mapper.class=my.Mapper - A user-defined Mapper to use instead of org.apache.hadoop.hbase.mapreduce.TsvImporterMapper
                </screen>
            </section>
            <section
                    xml:id="importtsv.example">
                <title>ImportTsv 实例</title>
                <para>举个例子，假设我们正往表'datatsv'导入数据， 列族为'd'，两列为"c1"和"c2"。 </para>
                <para>输入文件假定如下：:
                    <screen>
                        row1	c1	c2
                        row2	c1	c2
                        row3	c1	c2
                        row4	c1	c2
                        row5	c1	c2
                        row6	c1	c2
                        row7	c1	c2
                        row8	c1	c2
                        row9	c1	c2
                        row10	c1	c2
                    </screen>
                </para>
                <para>通过如下命令，ImportTsv就可使用该输入文件：</para>
                <screen language="bourne">
                    HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-VERSION.jar importtsv -Dimporttsv.columns=HBASE_ROW_KEY,d:c1,d:c2 -Dimporttsv.bulk.output=hdfs://storefileoutput datatsv hdfs://inputfile
                </screen>
                <para> ... 在这个例子中，第一列是rowkey，所以才使用HBASE_ROW_KEY。
                    文件中的第二、三列将分别作为 "d:c1"和"d:c2"导入。 </para>
            </section>
            <section
                    xml:id="importtsv.warning">
                <title>ImportTsv 警告</title>
                <para>如果你准备bulk导入大量数据，一定要确保目标表已适当切分。 </para>
            </section>
            <section
                    xml:id="importtsv.also">
                <title>参考</title>
                <para>关于大批量导入HFiles到HBase的更多信息，请参考<xref
                        linkend="arch.bulk.load" /></para>
            </section>
        </section>

        <section
                xml:id="completebulkload">
            <title>CompleteBulkLoad</title>
            <para> <code>completebulkload</code>工具用来移动StoreFiles到HBase表。
                这个工具经常与<xref
                        linkend="importtsv" />结合使用。 </para>
            <para>有两种方式调用此工具，通过指定classname或driver：</para>
            <screen language="bourne">$ bin/hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles &lt;hdfs://storefileoutput&gt; &lt;tablename&gt;
            </screen>
            <para> .. 通过 Driver..</para>
            <screen language="bourne">HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-VERSION.jar completebulkload &lt;hdfs://storefileoutput&gt; &lt;tablename&gt;
            </screen>
            <section
                    xml:id="completebulkload.warning">
                <title>CompleteBulkLoad 警告</title>
                <para>通过MapReduce产生的数据，其文件权限经常与运行的HBase进程不兼容。
                    如果你正带着权限运行HDFS， 在执行CompleteBulkLoad前你需要更新这些权限。
                    </para>
                <para>关于大批量导入HFiles到HBase的更多信息，参考 <xref
                        linkend="arch.bulk.load" />. </para>
            </section>

        </section>
        <section
                xml:id="walplayer">
            <title>WALPlayer</title>
            <para>WALPlayer工具可以重放WAL文件到HBase. </para>
            <para>可以为一组表或所有表重放WAL, 并选择时间范围(毫秒级)，过滤这组表的WAL。输出还可以映射到另外一组表。</para>
            <para>WALPlayer 也可以为接下来的大批量导入生成 HFiles，在这种情况下，只能指定单个表且无法映射。
                 </para>
            <para>调用如下：</para>
            <screen language="bourne">$ bin/hbase org.apache.hadoop.hbase.mapreduce.WALPlayer [options] &lt;wal inputdir&gt; &lt;tables&gt; [&lt;tableMappings>]&gt;
            </screen>
            <para>例如:</para>
            <screen language="bourne">$ bin/hbase org.apache.hadoop.hbase.mapreduce.WALPlayer /backuplogdir oldTable1,oldTable2 newTable1,newTable2
            </screen>
            <para> WALPlayer, 默认情况下，以mapreduce任务执行。在命令行上添加<code>-Dmapreduce.jobtracker.address=local</code>标志
                能强制让它以本地进程的形式运行。 </para>
        </section>
        <section
                xml:id="rowcounter">
            <title>RowCounter 和 CellCounter</title>
            <para><link
                    xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/RowCounter.html">RowCounter</link>
                是一个mapreduce任务，用来计算一个表的所有行。这是一个好工具，可以用来检测HBase是否可以读取一个表的所有块，
                借此发现元数据是否出现不一致。这个工具将在一个单独的进程内运行mapreduce，如果你有一个MapReduce集群供它使用的话，它会运行的更快。
                </para>
            <screen language="bourne">$ bin/hbase org.apache.hadoop.hbase.mapreduce.RowCounter &lt;tablename&gt; [&lt;column1&gt; &lt;column2&gt;...]
            </screen>
            <para>注意:Scan缓存可通过job configuration中的<code>hbase.client.scanner.caching</code>配置。
                 </para>
            <para>HBase 还推出了另外一个叫<link
                    xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/CellCounter.html">CellCounter</link>的诊断性mapreduce任务。
                与RowCounter类似, 但它会更细粒度地收集表的统计数据，包括： </para>
            <itemizedlist>
                <listitem>
                    <para>Total number of rows in the table.</para>
                </listitem>
                <listitem>
                    <para>Total number of CFs across all rows.</para>
                </listitem>
                <listitem>
                    <para>Total qualifiers across all rows.</para>
                </listitem>
                <listitem>
                    <para>Total occurrence of each CF.</para>
                </listitem>
                <listitem>
                    <para>Total occurrence of each qualifier.</para>
                </listitem>
                <listitem>
                    <para>Total number of versions of each qualifier.</para>
                </listitem>
            </itemizedlist>
            <para>这个程序可以限定运行范围。它提供了行正则表达式/前缀参数来限定要分析的行。
                使用 <code>hbase.mapreduce.scan.column.family</code> 可指定要扫描的单个列族。</para>
            <screen language="bourne">$ bin/hbase org.apache.hadoop.hbase.mapreduce.CellCounter &lt;tablename&gt; &lt;outputDir&gt; [regex or prefix]</screen>
            <para>注意:和RowCounter类似, Scan缓存可通过job configuration中的<code>hbase.client.scanner.caching</code>配置。
             </para>
        </section>
        <section
                xml:id="mlockall">
            <title>mlockall</title>
            <para>在启动时让服务器调用<link xlink:href="http://linux.die.net/man/2/mlockall">mlockall</link>，
                可以将其锁定在物理内存中，使其在超配额环境中不太可能发生内存交换。
                参考 <link
                        xlink:href="https://issues.apache.org/jira/browse/HBASE-4391">HBASE-4391 Add ability to
                    start RS as root and call mlockall</link>，如何构建可选库并使其在启动时运行。 </para>
        </section>
        <section
                xml:id="compaction.tool">
            <title>离线压缩工具</title>
            <para>参考<link
                    xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/CompactionTool.html">压缩工具
                </link>使用方法。 调用如下： <command>./bin/hbase
                org.apache.hadoop.hbase.regionserver.CompactionTool</command>
            </para>
        </section>

        <section>
            <title><command>hbase 清理</command></title>
            <para><command>hbase clean</command>命令用来清除ZooKeeper/HDFS中的HBase数据。
               该命令比较适合测试，无参数执行它可以查看使用说明，HBase 0.98版本引入了该命令。
               </para>
            <screen>
                $ <userinput>bin/hbase clean</userinput>
                Usage: hbase clean (--cleanZk|--cleanHdfs|--cleanAll)
                Options:
                --cleanZk   cleans hbase related data from zookeeper.
                --cleanHdfs cleans hbase related data from hdfs.
                --cleanAll  cleans hbase related data from both zookeeper and hdfs.
            </screen>
        </section>
        <section>
            <title><command>hbase pe</command></title>
            <para> <command>hbase pe</command>命令是运行<code>org.apache.hadoop.hbase.PerformanceEvaluation</code> 工具的快捷方式,
                该工具用来测试。HBase 0.98版本中引入了<command>hbase pe</command>命令。</para>
        </section>
        <section>
            <title><command>hbase ltt</command></title>
            <para> <command>hbase ltt</command>命令是运行 <code>rg.apache.hadoop.hbase.util.LoadTestTool</code>工具的快捷方式，
                该工具用来测试。HBase 0.98版本中引入了<command>hbase ltt</command>命令。
                </para>
        </section>
    </section>
    <!--  tools -->

    <section
            xml:id="ops.regionmgt">
        <title>Region 管理</title>
        <section
                xml:id="ops.regionmgt.majorcompact">
            <title>Major 压缩</title>
            <para>Major 压缩可以通过HBase shell或 <link
                    xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html#majorCompact%28java.lang.String%29">HBaseAdmin.majorCompact</link>执行。 </para>
            <para>注意： major压缩不会合并region。更多信息，请参考<xref
                    linkend="compaction" /> </para>
        </section>
        <section
                xml:id="ops.regionmgt.merge">
            <title>Merge</title>
            <para>Merge工具用来合并同一表的相邻region。(参考
                org.apache.hadoop.hbase.util.Merge).</para>
            <programlisting language="bourne">$ bin/hbase org.apache.hadoop.hbase.util.Merge &lt;tablename&gt; &lt;region1&gt; &lt;region2&gt;
            </programlisting>
            <para>如果你觉得region太多，想合并它们，Merge工具就是你的不二选择。
                Merge操作必须再集群停止前完成。参考 <link
                        xlink:href="http://ofps.oreilly.com/titles/9781449396107/performance.html">O'Reilly HBase
                    Book</link> ，里面有使用示例。 </para>
            <para>这个程序需要三个参数，第一个是表名，第二个是需要合并的第一个region的全名，如"table_name,\x0A,1342956111995.7cef47f192318ba7ccc75b1bbf27a82b."，
                第三个是需要合并的第二个region的全名。 </para>
            <para>另外，这里有 <link
                    xlink:href="https://issues.apache.org/jira/browse/HBASE-1621">HBASE-1621</link>附带的用来region合并的Ruby脚本。
                 </para>
        </section>
    </section>

    <section
            xml:id="node.management">
        <title>节点管理</title>
        <section
                xml:id="decommission">
            <title>节点下线</title>
            <para>你可以在HBase的特定节点上运行下面的脚本来停止RegionServer：</para>
            <screen language="bourne">$ ./bin/hbase-daemon.sh stop regionserver</screen>
            <para> RegionServer首先会关闭所有的region，然后关闭自己。在停止过程中，ZooKeeper中的RegionServer的临时节点会过期。
                master 会发现 RegionServer 已经死了，然后将它当做崩溃的server来处理，并重新重新分配该RegionServer上的region。
               </para>
            <note>
                <title>下线节点前要停止 Load Balancer</title>
                <para>如果load balancer运行的时候，刚好有节点要关闭，
                    则Load Balancer和Master的recovery可能会竞争这个下线的RegionServer。应先关闭balancer以避免这个问题出现，
                     参考下面的<xref
                            linkend="lb" />  </para>
            </note>
            <para>
               上文说的下线RegionServer有一个缺点就是其中的region会离线好一段时间。Regions是按顺序关闭的。如果服务器上有很多region，
                第一个region关闭后，直到最后一个region被关闭，并且Master认为该RegionServer的znode都死了，该region才可以上线。
                在HBase 0.90.2中，我们加入了一个功能，可以让节点逐步摆脱其负载后，再关闭。 HBase 0.90.2中增加了 <filename>graceful_stop.sh</filename>脚本，
                使用方法如下：
                </para>
            <screen language="bourne">$ ./bin/graceful_stop.sh
                Usage: graceful_stop.sh [--config &amp;conf-dir>] [--restart] [--reload] [--thrift] [--rest] &amp;hostname>
                thrift      If we should stop/start thrift before/after the hbase stop/start
                rest        If we should stop/start rest before/after the hbase stop/start
                restart     If we should restart after graceful stop
                reload      Move offloaded regions back on to the stopped server
                debug       Move offloaded regions back on to the stopped server
                hostname    Hostname of server we are to stop</screen>
            <para> 要下线一台 RegionServer, 可以这样做：<command>$
                ./bin/graceful_stop.sh HOSTNAME</command>  <varname>HOSTNAME</varname>是你要下线的RegionServer的主机名。 </para>
            <note>
                <title> <varname>HOSTNAME</varname></title>
                传递到graceful_stop.sh的HOSTNAME必须和hbase使用的hostname一致，
                hbase用它来区分RegionServers。可以用master的UI来检查RegionServers的id。
                通常是hostname,也可能是FQDN。不管HBase使用的哪一个，你可以将它传到 graceful_stop.sh脚本中去，
                目前他还不支持使用IP地址来推断hostname。
                所以使用IP就会发现server不在运行，也没有办法下线了。
                <para>传给<filename>graceful_stop.sh</filename>的 <varname>HOSTNAME</varname>，
                    必须与hbase用来区分RegionServers的hostname一致。检查下 master UI中的RegionServers列表，
                    你会看到HBase是怎么提及这些servers的。
                    通常是hostname，也可能是FQDN。不管HBase使用的哪个，你应当将它传到 <filename>graceful_stop.sh</filename>脚本中去。
                    目前该脚本还没聪明到通过IP地址来推断hostname (or FQDN)，因此传递IP地址时会检查失败，也就无法下线节点。
                    </para>
            </note>
            <para> <filename>graceful_stop.sh</filename>脚本会一个个将region从下线的RegionServer上移除。
                它会对部署到新位置的region先进行验证，再移除下一个，直到全部移除。最后，<filename>graceful_stop.sh</filename> 脚本会让
                RegionServer <command>stop</command>。这时候Master会注意到 RegionServer已经下线，而所有的regions已经重新部署好。
                RegionServer就可以干干净净的结束，没有WAL日志需要切分。 </para>
            <note
                    xml:id="lb">
                <title>Load Balancer</title>
                <para>
                    当执行<command>graceful_stop</command>脚本时，
                    要将 Region Load Balancer 关掉（否则balancer和下线脚本会在region部署上存在冲突）
                    使用 shell 命令关闭 balancer：</para>
                <programlisting>hbase(main):001:0> balance_switch false
                    true
                    0 row(s) in 0.3590 seconds</programlisting>
                <para> 上面是将 balancer 关闭，开启命令是：</para>
                <programlisting>hbase(main):001:0> balance_switch true
                    false
                    0 row(s) in 0.3590 seconds</programlisting>
                <para> <command>graceful_stop</command>脚本会检查balancer，如果开启了，将关闭它。
                    如果脚本由于错误而过早退出，它不会重置balancer。因此，你最好在graceful_stop完成后，单独开启balancer。
                </para>
            </note>
            <section
                    xml:id="draining.servers">
                <title>同时下线多台Regions Servers</title>
                <para>如果你的集群很大，你可能想同时下线多台机器。
                    可以将这些RegionServers置为“draining”状态，可以在ZooKeeper的<filename>hbase_root/draining</filename>节点下
                    创建条目，标记相应RegionServer为draining节点。
                    这些znode的格式<code>name,port,startcode</code>与<filename>hbase_root/rs</filename>下的regionserver一样。

                </para>
                <para>如果没有这项措施，下线多个节点可能不太理想，因为一台regionserver上的drain regions，
                    可能会被移动到其它也处于draining状态的regionservers上。将RegionServers标记未draining状态能防止这种情况发生。<footnote>
                        <para> 更多细节，请参考 <link
                                xlink:href="http://inchoate-clatter.blogspot.com/2012/03/hbase-ops-automation.html">blog
                            post</link> </para>
                    </footnote>. </para>
            </section>

            <section
                    xml:id="bad.disk">
                <title>坏的或失效磁盘</title>
                <para>如果你的每台机器上都有充足的磁盘，为了预防磁盘坏死，最好还是设置下 <xref
                        linkend="dfs.datanode.failed.volumes.tolerated" /> 。
                    但通常磁盘都是"John Wayne"（美国硬汉） -- 需要一段时间才在 <filename>dmesg</filename> 输出错误信息
                    -- 或者由于某些原因，会比其它磁盘运行得慢。这种情况下，如果要下线磁盘，你有两个选择。你可以<link
                            xlink:href="http://wiki.apache.org/hadoop/FAQ#I_want_to_make_a_large_cluster_smaller_by_taking_out_a_bunch_of_nodes_simultaneously._How_can_this_be_done.3F">decommission
                        the datanode</link>，或重新复制坏磁盘的数据，停止该datanode，卸载坏卷（你不能卸载一个datanode正在使用的卷），
                    然后重启datanode（你必须先设置dfs.datanode.failed.volumes.tolerated > 0）。
                    Regionserver在重新调整从何获取数据时，会在它的日志里抛出一些错误-- 它可能会回滚WAL日志 --
                    但是通常来说，在一些延迟高峰情况下,它仍会“嘎吱嘎吱”地继续运转。 </para>
                <note>
                    <title>Short Circuit读取</title>
                    <para>如果你正在做short-circuit读取，在你停止datanode前，你必须先将regions移出regionserver。
                        当short-circuiting 读时, 尽管chmod'd， regionserver无法访问，因为文件已经打开了，就算datanode已经停止，
                        它仍会继续从坏磁盘中读取文件块。重启datanode后，重新将regions移回去。
                       </para>
                </note>
            </section>
        </section>
        <section
                xml:id="rolling">
            <title>滚动重启</title>

            <para>某些集群配置的改变，需要重启整个集群或Regionservers。另外，滚动重启可用来支持小的或维护发布升级,甚至是重要发布。
                查看你想升级到的版本的发布注意项，找出执行滚动升级的限制。
             </para>
            <para>重启集群节点有很多种方式，这取决于你的自身情况。下面详细介绍这些方法。</para>
            <section>
                <title>使用<command>rolling-restart.sh</command> 脚本</title>

                <para>HBase 推出了 <filename>bin/rolling-restart.sh</filename> 脚本, 可对整个集群、Master或RegionServers进行滚动重启。
                    这个脚本可作为一个模板，它还没有经过明确的测试。要使用这个脚本，你必须配置SSH无密码登录，并用tarball工具部署，
                    运行这个脚本前，你还需要设置一些环境变量。好好检查这个脚本，按你的需求修改它。</para>
                <example>
                    <title><filename>rolling-restart.sh</filename> General Usage</title>
                    <screen language="bourne">
                        $ <userinput>./bin/rolling-restart.sh --help</userinput><![CDATA[
Usage: rolling-restart.sh [--config <hbase-confdir>] [--rs-only] [--master-only] [--graceful] [--maxthreads xx]          
        ]]></screen>
                </example>
                <variablelist>
                    <varlistentry>
                        <term>单独对 RegionServers 滚动重启</term>
                        <listitem>
                            <para>使用  <code>--rs-only</code> 选项，可只滚动重启 RegionServers。如果你想重启单个RegionServer，
                                或者你修改的配置项，只会影响到RegionServers，那这个选项就很有必要了。
                               </para>
                            <para>如果你想重启单个RegionServer, 或者是在重启期间需要做一些其它操作，
                                可以使用<filename>bin/graceful_stop.sh</filename>命令。参考<xref linkend="rolling.restart.manual" />.</para>
                        </listitem>
                    </varlistentry>
                    <varlistentry>
                        <term>单独对 Masters 滚动重启</term>
                        <listitem>
                            <para>使用 <code>--master-only</code> 选项，可滚动重启主备 Masters。
                                如果你修改的配置项，只会影响到Master，或者你需要重启运行active Master的服务器时，你可以使用这个选项。
                               </para>
                            <para>如果你没有在运行backup Masters,  Master会简单重启。否则，为了避免因选择新的Master而出现竞争，
                                在重启前，它们都会被停止。首先启动主Master，然后再启动backup Masters。重启后，在开始工作前，Master会
                                立即检查并清除过渡期（ in transition ）的regions。
                              </para>
                        </listitem>
                    </varlistentry>
                    <varlistentry>
                        <term>平滑重启</term>
                        <listitem>
                            <para>如果你指定了 <code>--graceful</code> 选项, RegionServers 将使用 <filename>bin/graceful_stop.sh</filename>
                                脚本来重启，在重启前该脚本会将RegionServer上的regions移走，这更加安全，但会延缓重启。</para>
                        </listitem>
                    </varlistentry>
                    <varlistentry>
                        <term>限制线程数</term>
                        <listitem>
                            <para>使用 <code>--maxthreads</code>选项来指定线程数，可以限制滚动重启。
                               </para>
                        </listitem>
                    </varlistentry>
                </variablelist>
            </section>
            <section xml:id="rolling.restart.manual">
                <title>手动滚动重启</title>
                <para>为了拥有更多的控制权，你可能想对你的集群，手动进行滚动重启。
                     那么可以使用 <command>graceful-stop.sh</command> 命令 <xref
                            linkend="decommission" />。
                    在这个方法中，你可以单独重启每一个RegionServer，然后将它的原始regions移回之前保留的位置。
                    如果你也想重启 Master, 你必须分开做，通过这个方法，先重启Master再重启RegionServers。
                    下面有个使用示例，你可能需要按照你的环境调整它。该脚本只针对RegionServers做滚动重启，
                    在移动regions前，先关闭load balancer。
                    </para>
                <screen><![CDATA[
$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i; done &> /tmp/log.txt &;     
        ]]></screen>
                <para>监控 <filename>/tmp/log.txt</filename>文件的输出，以查看脚本的运行过程。
                     </para>
            </section>

            <section>
                <title>创建你自己的滚动重启脚本</title>
                <para>如果你想创建自己的滚动重启脚本，可以使用以下指南：</para>
                <orderedlist>
                    <listitem>
                        <para>解压缩新版本，验证它的配置，使用<command>rsync</command>, <command>scp</command>
                            或其它安全同步机制将其同步到整个集群。

                           </para></listitem>
                    <listitem><para>使用hbck工具确保集群的一致性。</para>
                        <screen>
                            $ ./bin/hbck
                        </screen>
                        <para>有需要的话就执行修复操作，参考<xref linkend="hbck" /> </para>
                    </listitem>
                    <listitem><para>先重启master。如果你的新HBase目录和之前的不同（比如升级），你可能需要修改这些命令。</para>
                        <screen>
                            $ ./bin/hbase-daemon.sh stop master; ./bin/hbase-daemon.sh start master
                        </screen>
                    </listitem>
                    <listitem><para>使用如下脚本，从master开始平滑重启每一台RegionServer。</para>
                        <screen><![CDATA[
$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i; done &> /tmp/log.txt &            
          ]]></screen>
                        <para>如果你正在运行Thrift 或 REST服务，还需使用 --thrift 或 --rest 选项。
                            查看其它可用选项，请运行<command>bin/graceful-stop.sh --help</command>命令。</para>
                        <para>在重启多台RegionServer的时候，慢慢drain HBase regions是很重要的。
                            否则，多个regions同时下线并重新分配给其它节点，而这些节点也可能很快下线，这对性能会造成很负面的影响。
                            你可以给上面的脚本加入延迟，例如，添加一个shell 命令<command>sleep</command>。让每台RegionServer重启后，等待5分钟，
                            修改脚本如下：</para>
                        <screen><![CDATA[
$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i & sleep 5m; done &> /tmp/log.txt &            
          ]]></screen>
                    </listitem>
                    <listitem><para>再次重启Master，这会清除dead servers列表，重新开启load balancer.</para></listitem>
                    <listitem><para>再次运行 <command>hbck</command> 工具，以确保集群是一致的。</para></listitem>
                </orderedlist>
            </section>
        </section>
        <section
                xml:id="adding.new.node">
            <title>添加新节点</title>
            <para>在HBase中添加一个新的regionserver基本是自由的，你只需简单地启动它：
                <command>$ ./bin/hbase-daemon.sh start regionserver</command> ，它就会将自己注册到master。
                理想情况下，在同一台机器上你也开启了一个DataNode，这样RS就可以开始获取本地文件了。
                如果你是依靠ssh开启你的daemons, 不要忘记在master的<filename>conf/regionservers</filename>文件中添加新的主机名。
              </para>
            <para>此时，这个 region server 还无法提供数据，因为还没有regions移动到它这里。如果开启了balancer，
                就会开始移动regions到新的RS。对于一个小的/中等集群，这对延迟会有不利的影响，
                因为在同一时间会有很多regions下线。因此，在下线节点时，建议禁用负载均衡器，并手动迁移regions（或者，
                使用脚本一个个迁移它们，这样更好）。
                </para>
            <para>迁移完的regions都只有0%的位置，在缓存中也不会有任何块，因此 region server不得不通过网络来服务请求。
                这除了会导致高延迟，还会占用你网卡的全部容量。在实际用途中，一个标准的1 GigE NIC 最多读取 <emphasis>100MB/s</emphasis>。
                这种情况下，或者你是在OLAP环境中请求获取位置，建议compact迁移来的regions。
               </para>

        </section>
    </section>
    <!--  node mgt -->

    <section
            xml:id="hbase_metrics">
        <title>HBase 指标</title>
        <section
                xml:id="metric_setup">
            <title>指标安装</title>
            <para>参考 <link
                    xlink:href="http://hbase.apache.org/metrics.html">Metrics</link> 中的介绍和启用Metrics emission的方法。
                这对 HBase 0.94.x依然有效。 </para>
            <para>对于 HBase 0.95.x 或以上版本, 参考 <link
                    xlink:href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/metrics2/package-summary.html" />
            </para>
        </section>
        <section
                xml:id="rs_metrics_ganglia">
            <title>给Ganglia用户的警告</title>
            <para>警告: 默认情况下，HBase会发出大量关于每台RegionServer的指标，这可能会使你的安装陷于困境。你可以选择
                增加Ganglia 服务器容量，或配置HBase少发出指标。
                 </para>
        </section>
        <section
                xml:id="rs_metrics">
            <title>RegionServer最重要的指标</title>
            <section
                    xml:id="hbase.regionserver.blockCacheHitCachingRatio">
                <title><varname>blockCacheExpressCachingRatio (formerly
                    blockCacheHitCachingRatio)</varname></title>
                <para>块缓存命中率(0 - 100)。配置缓存命中率用来查看缓存 (例如, cacheBlocks=true). </para>
            </section>
            <section
                    xml:id="hbase.regionserver.callQueueLength">
                <title><varname>callQueueLength</varname></title>
                <para>当前时间RegionServer调用队列的长度。如果请求到达的速度比RegionServer的处理速度更快，那么它们会被放置在callQueue中。
                    </para>
            </section>
            <section
                    xml:id="hbase.regionserver.compactionQueueSize">
                <title><varname>compactionQueueLength (formerly compactionQueueSize)</varname></title>
                <para>当前时间compaction队列的长度。这是RegionServer中已确定要做compaction的Stores的数量。
                    </para>
            </section>
            <section
                    xml:id="hbase.regionserver.flushQueueSize">
                <title><varname>flushQueueSize</varname></title>
                <para>当前时间MemStore中等待flush的enqueued regions的数量。</para>
            </section>
            <section
                    xml:id="hbase.regionserver.hdfsBlocksLocalityIndex">
                <title><varname>hdfsBlocksLocalityIndex</varname></title>
                <para>当前时间这个RegionServer的本地HDFS块所占的百分比，越高越好。 </para>
            </section>
            <section
                    xml:id="hbase.regionserver.memstoreSizeMB">
                <title><varname>memstoreSizeMB</varname></title>
                <para>当前时间这个RegionServer的所有memstore大小的总和（MB）。
                    看着点，当心它接近或超过RegionServer中为MemStore内存配置的high-watermark。 </para>
            </section>
            <section
                    xml:id="hbase.regionserver.regions">
                <title><varname>numberOfOnlineRegions</varname></title>
                <para>当前时间RegionServer提供服务的Regions数。这是个重要指标，用来追踪RegionServer-Region的密度。 </para>
            </section>
            <section
                    xml:id="hbase.regionserver.readRequestsCount">
                <title><varname>readRequestsCount</varname></title>
                <para>这个RegionServer在启动之后的读请求数。注意：这是个32位整数且能回滚。 </para>
            </section>
            <section
                    xml:id="hbase.regionserver.slowHLogAppendCount">
                <title><varname>slowHLogAppendCount</varname></title>
                <para>这个RegionServer在启动之后的写HLog过慢的次数, "过慢"指的是大于1秒。这个指标可用来判断HDFS状态的好坏。
                   </para>
            </section>
            <section
                    xml:id="hbase.regionserver.usedHeapMB">
                <title><varname>usedHeapMB</varname></title>
                <para>当前时间RegionServer使用的内存大小(MB).</para>
            </section>
            <section
                    xml:id="hbase.regionserver.writeRequestsCount">
                <title><varname>writeRequestsCount</varname></title>
                <para>这个RegionServer在启动之后的写请求数。 注意：这是个32位整数且能回滚。</para>
            </section>

        </section>
        <section
                xml:id="rs_metrics_other">
            <title>其它 RegionServer 指标</title>
            <section
                    xml:id="hbase.regionserver.blockCacheCount">
                <title><varname>blockCacheCount</varname></title>
                <para>当前时间内存中的缓存块数量，也是缓存中StoreFiles (HFiles)块的数量。 </para>
            </section>
            <section
                    xml:id="hbase.regionserver.blockCacheEvictedCount">
                <title><varname>blockCacheEvictedCount</varname></title>
                <para>RegionServer启动之后，由于堆大小限制，不得不从 block缓存中清除的块数目。
                   </para>
            </section>
            <section
                    xml:id="hbase.regionserver.blockCacheFree">
                <title><varname>blockCacheFreeMB</varname></title>
                <para>当前时间block cache的可用内存(MB)。</para>
            </section>
            <section
                    xml:id="hbase.regionserver.blockCacheHitCount">
                <title><varname>blockCacheHitCount</varname></title>
                <para>RegionServer在启动之后的从cache中读取StoreFiles (HFiles)的块数目。</para>
            </section>
            <section
                    xml:id="hbase.regionserver.blockCacheHitRatio">
                <title><varname>blockCacheHitRatio</varname></title>
                <para>RegionServer在启动之后的Block cache命中率(0 to 100)。包括所有的读请求，
                    尽管那些cacheBlocks=false的请求，会经常从磁盘读取，并被统计为"cache miss", 这意味着
                    full-scan MapReduce任务会大大的影响这个指标。</para>
            </section>
            <section
                    xml:id="hbase.regionserver.blockCacheMissCount">
                <title><varname>blockCacheMissCount</varname></title>
                <para>RegionServer在启动之后不是从缓存中读取到的 StoreFiles (HFiles)块数目。</para>
            </section>
            <section
                    xml:id="hbase.regionserver.blockCacheSize">
                <title><varname>blockCacheSizeMB</varname></title>
                <para>当前时间内存中的缓存块大小(MB)，比如，BlockCache使用的内存。</para>
            </section>
            <section
                    xml:id="hbase.regionserver.fsPreadLatency">
                <title><varname>fsPreadLatency*</varname></title>
                <para>RegionServer在启动之后统计的几种文件系统positional读取延迟（ms）指标。
                    </para>
            </section>
            <section
                    xml:id="hbase.regionserver.fsReadLatency">
                <title><varname>fsReadLatency*</varname></title>
                <para>RegionServer在启动之后统计的几种文件系统读取延迟（ms）指标。这个指标统计的所有读取（例如单条记录的Get操作，全部扫描操作），
                    包括compactions需要的读取。在与HBase的主要发布版本或你自己的代码比较时，这个指标才有意思。
                  </para>
            </section>
            <section
                    xml:id="hbase.regionserver.fsWriteLatency">
                <title><varname>fsWriteLatency*</varname></title>
                <para>RegionServer在启动之后统计的几种文件系统写延迟（ms）指标。 这个指标统计的所有写，
                    包括单条记录的Put操作，compaction导致的全部重写操作等。
                    在与HBase的主要发布版本或你自己的代码比较时，这个指标才有意思。
                  </para>
            </section>
            <section
                    xml:id="hbase.regionserver.stores">
                <title><varname>NumberOfStores</varname></title>
                <para>当前时间，RegionServer上打开的Store数。一个Store对应一个列族。例如，
                    如果一个表(包含列族)在一个RegionServer上有3个region，该列族就会打开3个store。 </para>
            </section>
            <section
                    xml:id="hbase.regionserver.storeFiles">
                <title><varname>NumberOfStorefiles</varname></title>
                <para>当前时间RegionServer上打开的Storefile数。一个store的StoreFile (HFile)可能不止一个。</para>
            </section>
            <section
                    xml:id="hbase.regionserver.requests">
                <title><varname>requestsPerSecond</varname></title>
                <para>当前时间读写请求数。请求与RegionServer的RPC调用相对应，因此一个单独的Get操作会产生一个请求，
                    但是一个Scan操作（caching设置为1000）的每一个‘next’调用（例如，非每一行）都会产生一个请求。
                    一个 bulk-load 请求的每个HFile会产生一个请求。由于这个指标的周期性，在衡量活跃性时， readRequestsCount和writeRequestsCount指标会更有意思。
                    </para>
            </section>
            <section
                    xml:id="hbase.regionserver.storeFileIndexSizeMB">
                <title><varname>storeFileIndexSizeMB</varname></title>
                <para>当前时间，这个RegionServer中所有StoreFile的索引大小的总和(MB)</para>
            </section>
        </section>
    </section>

    <section
            xml:id="ops.monitoring">
        <title>HBase 监控</title>
        <section
                xml:id="ops.monitoring.overview">
            <title>Overview</title>
            <para>对于每台RegionServer的宏观监控来说，下面的指标大概是最重要的，最好是与一个监控系统联系起来，比如<link
                    xlink:href="http://opentsdb.net/">OpenTSDB</link>。
              如果你的集群存在性能问题，你可能借此看出一些不寻常的东西。 </para>
            <itemizedlist>
                <title>HBase:</title>
                <listitem>
                    <para>参考 <xref
                            linkend="rs_metrics" /></para>
                </listitem>
            </itemizedlist>

            <itemizedlist>
                <title>OS:</title>
                <listitem>
                    <para>IO Wait</para>
                </listitem>
                <listitem>
                    <para>User CPU</para>
                </listitem>
            </itemizedlist>
            <itemizedlist>
                <title>Java:</title>
                <listitem>
                    <para>GC</para>
                </listitem>
            </itemizedlist>
            <para>HBase指标的更多信息，参考<xref
                    linkend="hbase_metrics" />. </para>
        </section>

        <section
                xml:id="ops.slow.query">
            <title>慢查询日志</title>
            <para>HBase慢查询日志由可解析的JSON数据组成，描述了客户端操作(Gets, Puts, Deletes等)的属性，
                这些操作要么是运行时间过长，要么是输出太多。 "too long to run"和"too much output"的
                阈值可以配置，下面会有描述。输出产生在主region server日志中，
                因此通过内容及其它日志事件，很容易发现更多的细节。它也预先设置了区分标签 <constant>(responseTooSlow)</constant>,
                <constant>(responseTooLarge)</constant>, <constant>(operationTooSlow)</constant>和
                <constant>(operationTooLarge)</constant>，以便用户只想看到慢查询时，可以用grep过滤。
              </para>

            <section>
                <title>配置</title>
                <para>有两个配置开关可用于调整慢查询日志的阈值。
                    </para>

                <itemizedlist>
                    <listitem>
                        <para><varname>hbase.ipc.warn.response.time</varname>查询不被记录到日志的最大执行毫秒数。
                             缺省为10000，即10秒。可以设为-1，以禁止通过时间长短来记日志。</para>
                    </listitem>
                    <listitem>
                        <para><varname>hbase.ipc.warn.response.size</varname> 查询不被记录到日志的最大返回字节数。
                            缺省为100Mb，可以设为-1，以禁止通过大小来记日志。 </para>
                    </listitem>
                </itemizedlist>
            </section>

            <section>
                <title>指标</title>
                <para>慢查询日志暴露给JMX的指标</para>
                <itemizedlist>
                    <listitem>
                        <para><varname>hadoop.regionserver_rpc_slowResponse</varname> 是一个全局指标，反映所有超时记入日志的响应。
                         </para>
                    </listitem>
                    <listitem>
                        <para><varname>hadoop.regionserver_rpc_methodName.aboveOneSec</varname> 指标反映所有超过1秒的响应。</para>
                    </listitem>
                </itemizedlist>

            </section>

            <section>
                <title>输出</title>
                <para>输出用操作做标签，如<constant>(operationTooSlow)</constant>。如果调用是客户端操作，如Put, Get或Delete，
                    会暴露详细的指纹信息。否则，标签为<constant>(responseTooSlow)</constant>，也同样提供可分析的JSON输出，
                    但具有较少的细节信息，这完全依赖于RPC自身的超时和超量设置。如果响应大小触发了日志记录，就用<constant>TooLarge</constant>
                    代替<constant>TooSlow</constant>。 在大小和时间都触发了日志记录时，也是<constant>TooLarge</constant>出现。 </para>
            </section>
            <section>
                <title>示例</title>
                <para>
                    <programlisting>2011-09-08 10:01:25,824 WARN org.apache.hadoop.ipc.HBaseServer: (operationTooSlow): {"tables":{"riley2":{"puts":[{"totalColumns":11,"families":{"actions":[{"timestamp":1315501284459,"qualifier":"0","vlen":9667580},{"timestamp":1315501284459,"qualifier":"1","vlen":10122412},{"timestamp":1315501284459,"qualifier":"2","vlen":11104617},{"timestamp":1315501284459,"qualifier":"3","vlen":13430635}]},"row":"cfcd208495d565ef66e7dff9f98764da:0"}],"families":["actions"]}},"processingtimems":956,"client":"10.47.34.63:33623","starttimems":1315501284456,"queuetimems":0,"totalPuts":1,"class":"HRegionServer","responsesize":0,"method":"multiPut"}</programlisting>
                </para>

                <para>注意， 在"tables"结构里的所有东西，是MultiPut的fingerprint的输出，其余的信息是RPC相关的，比如处理时间和客户端IP/port。
                    客户端的其他操作的模式和通用结构与此相同，但由于单个操作的性质会有一定的不同。 如果调用不是客户端操作，则fingerprint细节信息将完全没有。
                    </para>

                <para>对本示例而言，慢操作的原因可能仅仅是一个超大的(100MB) multiput，通过"vlen" 即value length得知， multiPut的每个Put的字段中有该信息。 </para>
            </section>
        </section>
        <section>
            <title>块缓存监控</title>
            <para>从HBase 0.98开始, HBase Web UI具有监控和报告块缓存性能的功能。点击<menuchoice>
                    <guimenu>Tasks</guimenu>
                    <guisubmenu>Show Non-RPC Tasks</guisubmenu>
                    <guimenuitem>Block Cache</guimenuitem>
                </menuchoice>，可以查看块缓存报表。 下面是报表功能的几个例子：</para>
            <figure>
                <title>基本信息</title>
                <mediaobject>
                    <imageobject>
                        <imagedata fileref="bc_basic.png" width="100%"/>
                    </imageobject>
                    <textobject>
                        <para>显示缓存的实现。</para>
                    </textobject>
                </mediaobject>
            </figure>
            <figure>
                <title>配置</title>
                <mediaobject>
                    <imageobject>
                        <imagedata fileref="bc_config.png" width="100%"/>
                    </imageobject>
                    <textobject>
                        <para>显示所有的缓存配置项。</para>
                    </textobject>
                </mediaobject>
            </figure>
            <figure>
                <title>统计</title>
                <mediaobject>
                    <imageobject>
                        <imagedata fileref="bc_stats.png" width="100%"/>
                    </imageobject>
                    <textobject>
                        <para>显示缓存性能的统计信息。</para>
                    </textobject>
                </mediaobject>
            </figure>
            <figure>
                <title>L1 and L2</title>
                <mediaobject>
                    <imageobject>
                        <imagedata fileref="bc_l1.png" width="100%"/>
                    </imageobject>
                    <imageobject>
                        <imagedata fileref="bc_l2_buckets.png" width="100%"/>
                    </imageobject>
                    <textobject>
                        <para>显示L1和L2缓存的信息。</para>
                    </textobject>
                </mediaobject>
            </figure>
            <para>这里没有展示全部的信息，到Web UI上去看一看吧。</para>
        </section>



    </section>

    <section
            xml:id="cluster_replication">
        <title>集群复制</title>
        <note>
            <para>参考 <link
                    xlink:href="http://hbase.apache.org/replication.html">Cluster Replication</link>. </para>
        </note>
        <para>HBase提供了在集群间拷贝数据的复制机制。集群复制可作为一种灾难恢复解决方案，也是一种高可用性机制。
            你也可用用它来分离web-facing操作和back-end任务，如MapReduce。</para>

        <para>HBase复制的架构模式是“主推送”(master-push)，因为每个region server都有自己的write-ahead log (WAL)。
            一个主集群可以复制到任意数量的从集群，每一个region server都复制自己的修改流。
            关于主/从复制的不同特性和其它类型的复制，更多信息请参考文章<link
                    xlink:href="http://highscalability.com/blog/2009/8/24/how-google-serves-data-from-multiple-datacenters.html">How
                Google Serves Data From Multiple Datacenters</link>.</para>

        <para>复制是异步进行的，允许集群在地理位置上相距甚远，或是存在可用性上的差异。这也意味着
            主从集群上的数据无法保证实时一致性。 插入到主集群的行，在从集群上不会立即可用或强一致。我们的最终目标是一致性。
             </para>

        <para>这个设计使用的复制格式，在理念上与MySQL使用的<firstterm><link
                xlink:href="http://dev.mysql.com/doc/refman/5.1/en/replication-formats.html">statement-based
            replication</link></firstterm>设计相同。 只不过复制的是整个
            WALEdits (由Put的多个单元格插入操作和客户端上的Delete操作组成)，而不是SQL语句，以保持原子性。</para>

        <para>每个region server的WALs会一直保存在HDFS中，以供其它从集群复制数据。
            每个region server从它需要复制的日志中读取数据，为了简单的故障恢复，会将当前位置保存在ZooKeeper中。
            那个位置，以及要处理的WALs队列，每个从集群的可能不一样。</para>

        <para>参与复制的集群大小可以不一样。主集群会通过随机分配以均衡从集群上的复制数据流。</para>

        <para>HBase支持主/主、循环复制和多个从集群复制。</para>

        <figure>
            <title>复制体系结构概述</title>
            <mediaobject>
                <imageobject>
                    <imagedata fileref="replication_overview.png" />
                </imageobject>
                <textobject>
                    <para>如之前所述，插图为HBase的复制体系结构。</para>
                </textobject>
            </mediaobject>
        </figure>

        <formalpara>
            <title>启用和配置复制</title>
            <para>参考 <link
                    xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/replication/package-summary.html#requirements">
                API documentation for replication</link> 中启用和配置复制的信息。</para>
        </formalpara>

        <section>
            <title>WAL的修改过程</title>
            <para>单个WAL的修改会经历几个步骤，以将其复制到从集群。</para>

            <orderedlist>
                <title>当从集群正确响应时：</title>
                <listitem>
                    <para>一个HBase客户端通过Put或Delete操作来修改HBase中的数据。</para>
                </listitem>
                <listitem>
                    <para>Region server以一种方式将请求写入WAL，如果写入失败，会进行replay。</para>
                </listitem>
                <listitem>
                    <para>如果列族中已变化的单元格在复制范围内，就将变更加入复制队列中。
                        </para>
                </listitem>
                <listitem>
                    <para>有一个单独的线程，会从日志中读取变更，作为批处理过程的一部分。只有那些符合复制的KeyValues会被保留。
                        在循环复制的情况下，可复制的KeyValues是一个模式为GLOBAL范围的列族的一部分，而不是catalog比如<code>hbase:meta</code>的一部分,
                         也不会来源于目标从集群。
                       </para>
                </listitem>
                <listitem>
                    <para>变更会标记上master的UUID，并添加到缓冲区。 当缓冲区满或者读取到文件的末尾时，该缓冲区会被发送到从集群的一个随机region server。
                        </para>
                </listitem>
                <listitem>
                    <para>Region server会顺序读取变更，并将它们分别放至不同的缓冲区中，每个表一个缓冲区。
                          读取完所有的变更后，使用 <link
                                xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html"
                                >HTable</link>flush每个缓冲区, HBase的标准客户端。Master的UUID保存在这些变更中，是为了允许循环复制。
                        </para>
                </listitem>
                <listitem>
                    <para>在Master中，当前正在复制的WAL的偏移量，注册在ZooKeeper中。</para>
                </listitem>
            </orderedlist>
            <orderedlist>
                <title>当从集群无响应时：</title>
                <listitem>
                    <para>关于插入变更的前三个步骤，完全一样。</para>
                </listitem>
                <listitem>
                    <para>同样是在一个单独的线程中，region server读取、过滤、修改日志变更的方式和上面的一样。
                          slave region server不会响应RPC调用。</para>
                </listitem>
                <listitem>
                    <para>master睡眠并重试多次（可配置）。</para>
                </listitem>
                <listitem>
                    <para>如果slave region server仍然不可用，master会选择一个新的region server子集进行复制，并重新发送变更缓冲区。</para>
                </listitem>
                <listitem>
                    <para>同时，WALs会回滚并存储到ZooKeeper中的一个队列里。region server <firstterm>归档</firstterm>的日志，
                        从region server的日志目录移动到了一个集中的日志目录， 将在进行复制的线程的内存队列中更新它们的路径信息。
                        </para>
                </listitem>
                <listitem>
                    <para>当从集群最终又可用时，缓冲区将按正常的方式处理。master region server随后将复制此过程中堆积的日志。
                        </para>
                </listitem>
            </orderedlist>


            <note xml:id="cluster.replication.preserving.tags">
                <title>在复制过程中保存标签</title>
                <para>默认情况下， 用于集群间复制的编解码器会剥除单元格的标签，比如单元格级别的ACL。
                    为了防止标签被剥除，你可以使用一个不同的codec。在进行复制的RegionServer的source和sink端，配置
                    <code>hbase.replication.rpc.codec</code> 以使用
                    <literal>org.apache.hadoop.hbase.codec.KeyValueCodecWithTags</literal>。 这个配置项在
                    <link xlink:href="https://issues.apache.org/jira/browse/HBASE-10322"
                            >HBASE-10322</link>中有介绍。</para>
            </note>
        </section>

        <section>
            <title>复制的内部要素</title>
            <variablelist>
                <varlistentry>
                    <term>ZooKeeper中的复制状态</term>
                    <listitem>
                        <para>HBase在ZooKeeper中维护它的复制状态。 默认情况下，状态信息保存在<filename>/hbase/replication</filename>节点下。
                            该节点有两个子节点，<code>Peers</code> 和 <code>RS</code> 。</para>
                        <warning>
                            <para>如果你删除了ZooKeeper中的“复制树”(<filename>/hbase/replication/</filename>) ，复制可能会
                                中断，数据可能会丢失。 这和 <xref
                                        linkend="design.invariants.zk.data"/>中的约束性信息无关。<link
                                        xlink:href="https://issues.apache.org/jira/browse/HBASE-10295"
                                        >HBASE-10295</link>中有关于这个问题的更多进展。</para>
                        </warning>
                    </listitem>
                </varlistentry>
                <varlistentry>
                    <term> <code>Peers</code> 节点（znode）</term>
                    <listitem>
                        <para> <code>peers</code> 节点（znode）缺省存储在<filename>/hbase/replication/peers</filename> 。
                            一个包含所有复制集群的列表，及每个集群的状态信息组成了该节点（znode）。每一个peer的Value是它的集群key，这在HBase Shell中有提供。
                            集群key包含一个集群的quorum中的Zookeeper节点列表，  ZooKeeper quorum的客户端端口，该集群的HDFS中的HBase根节点（znode）。
</para>
                        <screen>
                            /hbase/replication/peers
                            /1 [Value: zk1.host.com,zk2.host.com,zk3.host.com:2181:/hbase]
                            /2 [Value: zk5.host.com,zk6.host.com,zk7.host.com:2181:/hbase]
                        </screen>
                        <para>每个 peer的子节点（znode）表明了在那个集群上是否开启了复制。
                            这些 peer-state节点不会包含任何子节点，只会包含一个布尔值。
                            这个值由<code>ReplicationPeer.PeerStateTracker</code>类读取并维护。</para>
                        <screen>
                            /hbase/replication/peers
                            /1/peer-state [Value: ENABLED]
                            /2/peer-state [Value: DISABLED]
                        </screen>
                    </listitem>
                </varlistentry>
                <varlistentry>
                    <term> <code>RS</code> 节点（znode）</term>
                    <listitem>
                        <para> <code>rs</code> 节点（znode）包含了一个需要复制的WAL日志列表。

                            这个列表被分成一个由region server组织的队列集合，和日志正推往的一个同等集群。
                            对于集群中的每一个region server，在rs节点中都有一个子节点。
                            子节点名是region server的主机名，client端口和启动码（start code）。
                            这个列表包含活着的和死去的region servers。</para>
                        <screen>
                            /hbase/replication/rs
                            /hostname.example.org,6020,1234
                            /hostname2.example.org,6020,2856
                        </screen>
                        <para>每一个 <code>rs</code> 节点包含一个WAL复制队列的列表，
                            每个要复制的同等集群对应一个队列。这些队列由子节点代表，节点名就是相应的集群ID。
                            </para>
                        <screen>
                            /hbase/replication/rs
                            /hostname.example.org,6020,1234
                            /1
                            /2
                        </screen>
                        <para>每个队列有一个子节点，为每一份仍然需要复制的WAL日志。
                            这些子节点的值为复制的最后位置，该位置在每一次WAL日志复制时更新。
                            </para>
                        <screen>
                            /hbase/replication/rs
                            /hostname.example.org,6020,1234
                            /1
                            23522342.23422 [VALUE: 254]
                            12340993.22342 [VALUE: 0]
                        </screen>
                    </listitem>
                </varlistentry>
            </variablelist>
        </section>
        <section>
            <title>复制配置项</title>
            <informaltable>
                <tgroup cols="3">
                    <thead>
                        <row>
                            <entry>配置项</entry>
                            <entry>描述</entry>
                            <entry>缺省值</entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry><para><code>zookeeper.znode.parent</code></para></entry>
                            <entry><para>给HBase使用的ZooKeeper根节点名</para></entry>
                            <entry><para><literal>/hbase</literal></para></entry>
                        </row>
                        <row>
                            <entry><para><code>zookeeper.znode.replication</code></para></entry>
                            <entry><para>用于复制的ZooKeeper根节点名n</para></entry>
                            <entry><para><literal>replication</literal></para></entry>
                        </row>
                        <row>
                            <entry><para><code>zookeeper.znode.replication.peers</code></para></entry>
                            <entry><para><code>peer</code> 节点名</para></entry>
                            <entry><para><literal>peers</literal></para></entry>
                        </row>
                        <row>
                            <entry><para><code>zookeeper.znode.replication.peers.state</code></para></entry>
                            <entry><para><code>peer-state</code>节点名</para></entry>
                            <entry><para><literal>peer-state</literal></para></entry>
                        </row>
                        <row>
                            <entry><para><code>zookeeper.znode.replication.rs</code></para></entry>
                            <entry><para><code>rs</code>节点名</para></entry>
                            <entry><para><literal>rs</literal></para></entry>
                        </row>
                        <row>
                            <entry><para><code>hbase.replication</code></para></entry>
                            <entry><para>给定的集群是否启用复制</para></entry>
                            <entry><para><literal>false</literal></para></entry>
                        </row>
                        <row>
                            <entry><para><code>eplication.sleep.before.failover</code></para></entry>
                            <entry><para>region server宕机后，worker应当睡眠多少毫秒开始尝试复制其WAL队列                                </para></entry>
                            <entry><para><literal></literal></para></entry>
                        </row>
                        <row>
                            <entry><para><code>replication.executor.workers</code></para></entry>
                            <entry><para>给定的region server发生故障时，进行自动切换的region server数。
                                </para></entry>
                            <entry><para><literal>1</literal></para></entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
        </section>

        <section>
            <title>复制实现细节</title>
            <formalpara>
                <title>选择Region Server进行复制</title>
                <para>当一个主集群对从集群发起复制时，首先它会通过提供给它的集群key连接从集群的ZooKeeper ensemble，
                    然后扫描<filename>rs/</filename>目录找出所有可用的sinks（可以用来接收变更输入流进行复制的region servers），
                    按配置的比例（默认值为10%）挑选一部分机器进行复制。例如，如果从集群有150台机器，主集群region server则会从中选择15台发送变更。
                    因为这个挑选工作是由各自的主region server执行，因此使用到所有的从region server的概率还是非常高的，这个方法适用于任何规模的集群。
                    例如，10台机器的主集群，正往5台机器的从集群按10%的比例复制数据，主集群region server每次将随机选择一台机器。
                   </para>
            </formalpara>
            <para>主集群的region servers都会在从集群的 <filename>${<replaceable>zookeeper.znode.parent</replaceable>}/rs</filename>节点上
                放置一个监听器（watcher）。这个监听器用来监视从集群的结构变化。当从集群移除节点，或节点崩溃/恢复时，
                主集群的region servers将选择一些新的从region servers进行复制。
              </para>

            <formalpara>
                <title>跟踪日志</title>

                <para>主集群的每个region server在znode目录中都有它对应的znode.
                    每个从集群都有一个znode（5个从集群，就会有5个znode被创建）,每个znode包含一个需要处理的WAL队列。
                    每个队列都会跟踪region server创建的WAL，但是队列的大小可能不一样。
                    例如，如果一个从集群一段时间不可用，那么它的WAL不应被删除，而需要保存在队列中（而其它从集群的日志仍在处理）。
                    参考 <xref
                            linkend="rs.failover.details"/> </para>
            </formalpara>
            <para>当source初始化后，它便包含region server要写入的WAL。
                当日志滚动时，新文件在可用前会被添加到每个从集群的znode的队列中。这样的话，在region server往其追加变更前，
                可以保证所有的sources都已知道有新的日志出现，但这种操作十分消耗系统资源。当复制线程从日志文件中无法读取到新的条目
                （已经到达最后一个块），且在队列中还有其它文件时，这些队列项就会被丢弃。这意味着，当一个source是最新的，
                且通过region server正写入的日志文件复制时，读取到当前文件末尾的文件不会从队列中删除。
               </para>
            <para>A log can be archived if it is no longer used or if the number of logs exceeds
                <code>hbase.regionserver.maxlogs</code> because the insertion rate is faster than regions
                are flushed.
                当日志被归档时（由于日志不再被使用，或日志的数目超过了 <code>hbase.regionserver.maxlogs</code>的值，或插入速度比regions的刷写速度快），
                会通知source线程，日志路径已发生变化。如果source已经同步完了这个日志，便会忽略这条通知。
                如果这个文件在队列中，那么路径会在内存中更新。如果日志正在被复制，修改将会是原子性的，因此读进程在文件移动完成前
                不会尝试打开文件。因为移动文件是一种NameNode操作，如果日志正在被读取，也不会产生什么异常。
             </para>
            <formalpara>
                <title>读取、过滤以及发送变更</title>
                <para>默认情况下，一个source会尝试从WAL读取数据，并尽可能快地将它们推送到sink（从集群的接收服务器）。
                    推送速度首先会受到日志过滤的限制，只有那些GLOBAL类的且不属于catalog表的KeyValues会被保留，
                    也会受到每个从集群同步总大小的限制，默认为64MB。根据这个配置，带有3个从集群的主集群region server最多占用
                    192MB来存储要同步的数据，这还没计算被过滤掉的数据。
                   </para>
            </formalpara>
            <para>一旦缓冲的变更达到最大值，或读到了WAL文件的末尾，source线程将会停止读取并随机选择一个sink
                （从一个生成的从集群region server子集列表中挑选）来复制数据。它会直接给挑选的region server发送
                RPC请求，如果返回成功，source会判断当前文件是否读完。如果已读完，source会从队列中删除这个znode。否则，
                就在日志的znode中注册一个新的位移。如果RPC抛出异常，source会在重试10次之后重新选择一个新的sink。

               </para>
            <formalpara>
                <title>日志清理</title>
                <para>如果没有启用复制，master的日志清理线程会按配置的TTL（Time-To-Live）删除旧的日志。
                    这种机制在进行复制时表现不太好，因为归档日志超出了TTL后也有可能还在队列中。因此，这种默认行为被增强了一些，
                    即如果日志超过了TTL，清理线程会查找所有队列直到找到该日志，并缓存其找到的队列。如果没有找到该日志，
                    则日志会被删除。下次还需要查找日志时，它会先检查缓存。
                   </para>
            </formalpara>
            <formalpara xml:id="rs.failover.details">
                <title>Region Server 异常</title>
                <para>只要没有region server失效，在ZooKeeper中跟踪的日志就不会添加新值。
                    如果region server不幸失效了，因为ZooKeeper的高可用性，我们可以用它来管理队列的传输。
                 </para>
            </formalpara>
            <para>主集群的每一个region server都会为其它region服务器保留一个监听器（watcher），
                以便当其它服务器失效时收到通知（master也是这样做的）。当发生故障时，它们会竞争着在宕机服务器的znode（包含其队列）中创建一个叫
                <literal>lock</literal>的znode。创建成功的服务器会把所有的队列添加到自己的znode中去，由于ZooKeeper不支持对队列改名，
                所以必须逐个添加。在所有队列添加完成后，它将删除旧的队列。恢复过来的znode会用当前从集群的ID附加宕机服务器的名称来命名。

               </para>
            <para>接下来，主集群region server会为每个复制过来的队列创建一个新的source线程，
                并且每个线程会按照读取/过滤/推送的模式工作。最主要的不同是，这些队列不会接收新的数据，
                因为它们不属于新的region server，也就是说，当读进程读到日志的末尾时，队列的znode会被删除，
                并且主集群region server会关闭这个复制source。
             </para>
            <para>假如一个主集群有3个region server，在复制数据到ID为<literal>2</literal>的从集群。下面的目录结构代表了某个时间点的znode的布局。
                region server的znode都包含一个<literal>peers</literal> znode，其中包含一个队列。队列中的znode名称代表HDFS中的实际文件名，
                格式为：<literal><replaceable>address</replaceable>,<replaceable>port</replaceable>.<replaceable>timestamp</replaceable></literal>.</para>
            <screen>
                /hbase/replication/rs/
                1.1.1.1,60020,123456780/
                2/
                1.1.1.1,60020.1234  (Contains a position)
                1.1.1.1,60020.1265
                1.1.1.2,60020,123456790/
                2/
                1.1.1.2,60020.1214  (Contains a position)
                1.1.1.2,60020.1248
                1.1.1.2,60020.1312
                1.1.1.3,60020,    123456630/
                2/
                1.1.1.3,60020.1280  (Contains a position)
            </screen>
            <para>假设1.1.1.2丢失了它的ZooKeeper会话。其它幸存者将竞争着来创建锁，例子中，1.1.1.3创建成功，
                它会将所有队列转移到自己的对等znode下，并附加宕机服务器的名字。在1.1.1.3清理老的znode前，
                目录结构如下：
                </para>
            <screen>
                /hbase/replication/rs/
                1.1.1.1,60020,123456780/
                2/
                1.1.1.1,60020.1234  (Contains a position)
                1.1.1.1,60020.1265
                1.1.1.2,60020,123456790/
                lock
                2/
                1.1.1.2,60020.1214  (Contains a position)
                1.1.1.2,60020.1248
                1.1.1.2,60020.1312
                1.1.1.3,60020,123456630/
                2/
                1.1.1.3,60020.1280  (Contains a position)

                2-1.1.1.2,60020,123456790/
                1.1.1.2,60020.1214  (Contains a position)
                1.1.1.2,60020.1248
                1.1.1.2,60020.1312
            </screen>
            <para>之后，在1.1.1.3复制1.1.1.2的WAL的过程中，如果也宕机了 （一些新的日志还在正常队列中）...
                最后剩下的region server会试着锁定 1.1.1.3的znode，同时开始转移所有队列。
                新的目录结构如下：</para>
            <screen>
                /hbase/replication/rs/
                1.1.1.1,60020,123456780/
                2/
                1.1.1.1,60020.1378  (Contains a position)

                2-1.1.1.3,60020,123456630/
                1.1.1.3,60020.1325  (Contains a position)
                1.1.1.3,60020.1401

                2-1.1.1.2,60020,123456790-1.1.1.3,60020,123456630/
                1.1.1.2,60020.1312  (Contains a position)
                1.1.1.3,60020,123456630/
                lock
                2/
                1.1.1.3,60020.1325  (Contains a position)
                1.1.1.3,60020.1401

                2-1.1.1.2,60020,123456790/
                1.1.1.2,60020.1312  (Contains a position)
            </screen>
            <formalpara>
                <title>复制指标</title>
                <para>下面的指标是全局region server级和(从 HBase 0.95版本以后)peer级的:</para>
            </formalpara>
            <variablelist>
                <varlistentry>
                    <term><code>source.sizeOfLogQueue</code></term>
                    <listitem>
                        <para> 复制源要处理的WAL数 (不包括正在处理的) </para>
                    </listitem>
                </varlistentry>
                <varlistentry>
                    <term><code>source.shippedOps</code></term>
                    <listitem>
                        <para>推送的变更数</para>
                    </listitem>
                </varlistentry>
                <varlistentry>
                    <term><code>source.logEditsRead</code></term>
                    <listitem>
                        <para>复制源从HLogs中读取的变更数</para>
                    </listitem>
                </varlistentry>
                <varlistentry>
                    <term><code>source.ageOfLastShippedOp</code></term>
                    <listitem>
                        <para>复制源最后一次推送的时间</para>
                    </listitem>
                </varlistentry>
            </variablelist>

        </section>
    </section>
    <section
            xml:id="ops.backup">
        <title>HBase 备份</title>
        <para>HBase有两种广泛使用的备份策略：备份一个关闭的集群，备份一个在线的集群。每种方法有利有弊。

             </para>
        <para>   更多信息，参考Sematext博客上 <link
                xlink:href="http://blog.sematext.com/2011/03/11/hbase-backup-options/">HBase Backup
            Options</link> </para>
        <section
                xml:id="ops.backup.fullshutdown">
            <title>全封闭备份</title>
            <para>某些环境中可以容忍HBase集群定期全部关闭，例如，当集群用于后台分析而不服务于前端页面时。
                这样做的好处是，NameNode/Master，RegionServers都关闭了，因此StoreFiles和metadata都不会丢失任何变更。
                明显的缺点是集群被关闭了。步骤如下：
               </para>
            <section
                    xml:id="ops.backup.fullshutdown.stop">
                <title>停止 HBase</title>
                <para> </para>
            </section>
            <section
                    xml:id="ops.backup.fullshutdown.distcp">
                <title>Distcp</title>
                <para>Distcp could be used to either copy the contents of the HBase directory in HDFS to
                    either the same cluster in another directory, or to a different cluster. </para>
                <para>Note: Distcp works in this situation because the cluster is down and there are no
                    in-flight edits to files. Distcp-ing of files in the HBase directory is not generally
                    recommended on a live cluster. </para>
            </section>
            <section
                    xml:id="ops.backup.fullshutdown.restore">
                <title>Restore (if needed)</title>
                <para>The backup of the hbase directory from HDFS is copied onto the 'real' hbase directory
                    via distcp. The act of copying these files creates new HDFS metadata, which is why a
                    restore of the NameNode edits from the time of the HBase backup isn't required for this
                    kind of restore, because it's a restore (via distcp) of a specific HDFS directory (i.e.,
                    the HBase part) not the entire HDFS file-system. </para>
            </section>
        </section>
        <section
                xml:id="ops.backup.live.replication">
            <title>Live Cluster Backup - Replication</title>
            <para>This approach assumes that there is a second cluster. See the HBase page on <link
                    xlink:href="http://hbase.apache.org/replication.html">replication</link> for more
                information. </para>
        </section>
        <section
                xml:id="ops.backup.live.copytable">
            <title>Live Cluster Backup - CopyTable</title>
            <para>The <xref
                    linkend="copytable" /> utility could either be used to copy data from one table to another
                on the same cluster, or to copy data to another table on another cluster. </para>
            <para>Since the cluster is up, there is a risk that edits could be missed in the copy process.
            </para>
        </section>
        <section
                xml:id="ops.backup.live.export">
            <title>Live Cluster Backup - Export</title>
            <para>The <xref
                    linkend="export" /> approach dumps the content of a table to HDFS on the same cluster. To
                restore the data, the <xref
                        linkend="import" /> utility would be used. </para>
            <para>Since the cluster is up, there is a risk that edits could be missed in the export
                process. </para>
        </section>
    </section>
    <!--  backup -->

    <section
            xml:id="ops.snapshots">
        <title>HBase Snapshots</title>
        <para>HBase Snapshots allow you to take a snapshot of a table without too much impact on Region
            Servers. Snapshot, Clone and restore operations don't involve data copying. Also, Exporting
            the snapshot to another cluster doesn't have impact on the Region Servers. </para>
        <para>Prior to version 0.94.6, the only way to backup or to clone a table is to use
            CopyTable/ExportTable, or to copy all the hfiles in HDFS after disabling the table. The
            disadvantages of these methods are that you can degrade region server performance (Copy/Export
            Table) or you need to disable the table, that means no reads or writes; and this is usually
            unacceptable. </para>
        <section
                xml:id="ops.snapshots.configuration">
            <title>Configuration</title>
            <para>To turn on the snapshot support just set the <varname>hbase.snapshot.enabled</varname>
                property to true. (Snapshots are enabled by default in 0.95+ and off by default in
                0.94.6+)</para>
            <programlisting language="java">
                &lt;property>
                &lt;name>hbase.snapshot.enabled&lt;/name>
                &lt;value>true&lt;/value>
                &lt;/property>
            </programlisting>
        </section>
        <section
                xml:id="ops.snapshots.takeasnapshot">
            <title>Take a Snapshot</title>
            <para>You can take a snapshot of a table regardless of whether it is enabled or disabled. The
                snapshot operation doesn't involve any data copying.</para>
            <screen language="bourne">
                $ ./bin/hbase shell
                hbase> snapshot 'myTable', 'myTableSnapshot-122112'
            </screen>
        </section>
        <section
                xml:id="ops.snapshots.list">
            <title>Listing Snapshots</title>
            <para>List all snapshots taken (by printing the names and relative information).</para>
            <screen language="bourne">
                $ ./bin/hbase shell
                hbase> list_snapshots
            </screen>
        </section>
        <section
                xml:id="ops.snapshots.delete">
            <title>Deleting Snapshots</title>
            <para>You can remove a snapshot, and the files retained for that snapshot will be removed if
                no longer needed.</para>
            <screen language="bourne">
                $ ./bin/hbase shell
                hbase> delete_snapshot 'myTableSnapshot-122112'
            </screen>
        </section>
        <section
                xml:id="ops.snapshots.clone">
            <title>Clone a table from snapshot</title>
            <para>From a snapshot you can create a new table (clone operation) with the same data that you
                had when the snapshot was taken. The clone operation, doesn't involve data copies, and a
                change to the cloned table doesn't impact the snapshot or the original table.</para>
            <screen language="bourne">
                $ ./bin/hbase shell
                hbase> clone_snapshot 'myTableSnapshot-122112', 'myNewTestTable'
            </screen>
        </section>
        <section
                xml:id="ops.snapshots.restore">
            <title>Restore a snapshot</title>
            <para>The restore operation requires the table to be disabled, and the table will be restored
                to the state at the time when the snapshot was taken, changing both data and schema if
                required.</para>
            <screen language="bourne">
                $ ./bin/hbase shell
                hbase> disable 'myTable'
                hbase> restore_snapshot 'myTableSnapshot-122112'
            </screen>
            <note>
                <para>Since Replication works at log level and snapshots at file-system level, after a
                    restore, the replicas will be in a different state from the master. If you want to use
                    restore, you need to stop replication and redo the bootstrap. </para>
            </note>
            <para>In case of partial data-loss due to misbehaving client, instead of a full restore that
                requires the table to be disabled, you can clone the table from the snapshot and use a
                Map-Reduce job to copy the data that you need, from the clone to the main one. </para>
        </section>
        <section
                xml:id="ops.snapshots.acls">
            <title>Snapshots operations and ACLs</title>
            <para>If you are using security with the AccessController Coprocessor (See <xref
                    linkend="hbase.accesscontrol.configuration" />), only a global administrator can take,
                clone, or restore a snapshot, and these actions do not capture the ACL rights. This means
                that restoring a table preserves the ACL rights of the existing table, while cloning a table
                creates a new table that has no ACL rights until the administrator adds them.</para>
        </section>
        <section
                xml:id="ops.snapshots.export">
            <title>Export to another cluster</title>
            <para>The ExportSnapshot tool copies all the data related to a snapshot (hfiles, logs,
                snapshot metadata) to another cluster. The tool executes a Map-Reduce job, similar to
                distcp, to copy files between the two clusters, and since it works at file-system level the
                hbase cluster does not have to be online.</para>
            <para>To copy a snapshot called MySnapshot to an HBase cluster srv2 (hdfs:///srv2:8082/hbase)
                using 16 mappers:</para>
            <programlisting language="bourne">$ bin/hbase class org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot MySnapshot -copy-to hdfs://srv2:8082/hbase -mappers 16</programlisting>
            <formalpara>
                <title>Limiting Bandwidth Consumption</title>
                <para>You can limit the bandwidth consumption when exporting a snapshot, by specifying the
                    <code>-bandwidth</code> parameter, which expects an integer representing megabytes per
                    second. The following example limits the above example to 200 MB/sec.</para>
            </formalpara>
            <programlisting language="bourne">$ bin/hbase class org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot MySnapshot -copy-to hdfs://srv2:8082/hbase -mappers 16 -bandwidth 200</programlisting>
        </section>
    </section>
    <!--  snapshots -->

    <section
            xml:id="ops.capacity">
        <title>Capacity Planning and Region Sizing</title>
        <para>There are several considerations when planning the capacity for an HBase cluster and
            performing the initial configuration. Start with a solid understanding of how HBase handles
            data internally.</para>
        <section
                xml:id="ops.capacity.nodes">
            <title>Node count and hardware/VM configuration</title>
            <section
                    xml:id="ops.capacity.nodes.datasize">
                <title>Physical data size</title>
                <para>Physical data size on disk is distinct from logical size of your data and is affected
                    by the following: </para>
                <itemizedlist>
                    <listitem>
                        <para>Increased by HBase overhead</para>
                        <itemizedlist>
                            <listitem>
                                <para>See <xref
                                        linkend="keyvalue" /> and <xref
                                        linkend="keysize" />. At least 24 bytes per key-value (cell), can be more. Small
                                    keys/values means more relative overhead.</para>
                            </listitem>
                            <listitem>
                                <para>KeyValue instances are aggregated into blocks, which are indexed. Indexes also
                                    have to be stored. Blocksize is configurable on a per-ColumnFamily basis. See <xref
                                            linkend="regions.arch" />.</para>
                            </listitem>
                        </itemizedlist>
                    </listitem>
                    <listitem>
                        <para>Decreased by <xref
                                linkend="compression"
                                xrefstyle="template:compression" /> and data block encoding, depending on data. See
                            also <link
                                    xlink:href="http://search-hadoop.com/m/lL12B1PFVhp1">this thread</link>. You might
                            want to test what compression and encoding (if any) make sense for your data.</para>
                    </listitem>
                    <listitem>
                        <para>Increased by size of region server <xref
                                linkend="wal"
                                xrefstyle="template:WAL" /> (usually fixed and negligible - less than half of RS
                            memory size, per RS).</para>
                    </listitem>
                    <listitem>
                        <para>Increased by HDFS replication - usually x3.</para>
                    </listitem>
                </itemizedlist>
                <para>Aside from the disk space necessary to store the data, one RS may not be able to serve
                    arbitrarily large amounts of data due to some practical limits on region count and size
                    (see <xref
                            linkend="ops.capacity.regions"
                            xrefstyle="template:below" />).</para>
            </section>
            <!-- ops.capacity.nodes.datasize -->
            <section
                    xml:id="ops.capacity.nodes.throughput">
                <title>Read/Write throughput</title>
                <para>Number of nodes can also be driven by required thoughput for reads and/or writes. The
                    throughput one can get per node depends a lot on data (esp. key/value sizes) and request
                    patterns, as well as node and system configuration. Planning should be done for peak load
                    if it is likely that the load would be the main driver of the increase of the node count.
                    PerformanceEvaluation and <xref
                            linkend="ycsb"
                            xrefstyle="template:YCSB" /> tools can be used to test single node or a test
                    cluster.</para>
                <para>For write, usually 5-15Mb/s per RS can be expected, since every region server has only
                    one active WAL. There's no good estimate for reads, as it depends vastly on data,
                    requests, and cache hit rate. <xref
                            linkend="perf.casestudy" /> might be helpful.</para>
            </section>
            <!-- ops.capacity.nodes.throughput -->
            <section
                    xml:id="ops.capacity.nodes.gc">
                <title>JVM GC limitations</title>
                <para>RS cannot currently utilize very large heap due to cost of GC. There's also no good
                    way of running multiple RS-es per server (other than running several VMs per machine).
                    Thus, ~20-24Gb or less memory dedicated to one RS is recommended. GC tuning is required
                    for large heap sizes. See <xref
                            linkend="gcpause" />, <xref
                            linkend="trouble.log.gc" /> and elsewhere (TODO: where?)</para>
            </section>
            <!-- ops.capacity.nodes.gc -->
        </section>
        <!-- ops.capacity.nodes -->
        <section
                xml:id="ops.capacity.regions">
            <title>Determining region count and size</title>
            <para>Generally less regions makes for a smoother running cluster (you can always manually
                split the big regions later (if necessary) to spread the data, or request load, over the
                cluster); 20-200 regions per RS is a reasonable range. The number of regions cannot be
                configured directly (unless you go for fully <xref
                        linkend="disable.splitting"
                        xrefstyle="template:manual splitting" />); adjust the region size to achieve the target
                region size given table size.</para>
            <para>When configuring regions for multiple tables, note that most region settings can be set
                on a per-table basis via <link
                        xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HTableDescriptor.html">HTableDescriptor</link>,
                as well as shell commands. These settings will override the ones in
                <varname>hbase-site.xml</varname>. That is useful if your tables have different
                workloads/use cases.</para>
            <para>Also note that in the discussion of region sizes here, <emphasis
                    role="bold">HDFS replication factor is not (and should not be) taken into account, whereas
                other factors <xref
                        linkend="ops.capacity.nodes.datasize"
                        xrefstyle="template:above" /> should be.</emphasis> So, if your data is compressed and
                replicated 3 ways by HDFS, "9 Gb region" means 9 Gb of compressed data. HDFS replication
                factor only affects your disk usage and is invisible to most HBase code.</para>
            <section
                    xml:id="ops.capacity.regions.count">
                <title>Number of regions per RS - upper bound</title>
                <para>In production scenarios, where you have a lot of data, you are normally concerned with
                    the maximum number of regions you can have per server. <xref
                            linkend="too_many_regions" /> has technical discussion on the subject; in short, maximum
                    number of regions is mostly determined by memstore memory usage. Each region has its own
                    memstores; these grow up to a configurable size; usually in 128-256Mb range, see <xref
                            linkend="hbase.hregion.memstore.flush.size" />. There's one memstore per column family
                    (so there's only one per region if there's one CF in the table). RS dedicates some
                    fraction of total memory (see <xref
                            linkend="hbase.regionserver.global.memstore.size" />) to region memstores. If this
                    memory is exceeded (too much memstore usage), undesirable consequences such as
                    unresponsive server, or later compaction storms, can result. Thus, a good starting point
                    for the number of regions per RS (assuming one table) is:</para>

                <programlisting>(RS memory)*(total memstore fraction)/((memstore size)*(# column families))</programlisting>
                <para> E.g. if RS has 16Gb RAM, with default settings, it is 16384*0.4/128 ~ 51 regions per
                    RS is a starting point. The formula can be extended to multiple tables; if they all have
                    the same configuration, just use total number of families.</para>
                <para>This number can be adjusted; the formula above assumes all your regions are filled at
                    approximately the same rate. If only a fraction of your regions are going to be actively
                    written to, you can divide the result by that fraction to get a larger region count. Then,
                    even if all regions are written to, all region memstores are not filled evenly, and
                    eventually jitter appears even if they are (due to limited number of concurrent flushes).
                    Thus, one can have as many as 2-3 times more regions than the starting point; however,
                    increased numbers carry increased risk.</para>
                <para>For write-heavy workload, memstore fraction can be increased in configuration at the
                    expense of block cache; this will also allow one to have more regions.</para>
            </section>
            <!-- ops.capacity.regions.count -->
            <section
                    xml:id="ops.capacity.regions.mincount">
                <title>Number of regions per RS - lower bound</title>
                <para>HBase scales by having regions across many servers. Thus if you have 2 regions for
                    16GB data, on a 20 node machine your data will be concentrated on just a few machines -
                    nearly the entire cluster will be idle. This really can't be stressed enough, since a
                    common problem is loading 200MB data into HBase and then wondering why your awesome 10
                    node cluster isn't doing anything.</para>
                <para>On the other hand, if you have a very large amount of data, you may also want to go
                    for a larger number of regions to avoid having regions that are too large.</para>
            </section>
            <!-- ops.capacity.regions.mincount -->
            <section
                    xml:id="ops.capacity.regions.size">
                <title>Maximum region size</title>
                <para>For large tables in production scenarios, maximum region size is mostly limited by
                    compactions - very large compactions, esp. major, can degrade cluster performance.
                    Currently, the recommended maximum region size is 10-20Gb, and 5-10Gb is optimal. For
                    older 0.90.x codebase, the upper-bound of regionsize is about 4Gb, with a default of
                    256Mb.</para>
                <para>The size at which the region is split into two is generally configured via <xref
                        linkend="hbase.hregion.max.filesize" />; for details, see <xref
                        linkend="arch.region.splits" />.</para>
                <para>If you cannot estimate the size of your tables well, when starting off, it's probably
                    best to stick to the default region size, perhaps going smaller for hot tables (or
                    manually split hot regions to spread the load over the cluster), or go with larger region
                    sizes if your cell sizes tend to be largish (100k and up).</para>
                <para>In HBase 0.98, experimental stripe compactions feature was added that would allow for
                    larger regions, especially for log data. See <xref
                            linkend="ops.stripe" />.</para>
            </section>
            <!-- ops.capacity.regions.size -->
            <section
                    xml:id="ops.capacity.regions.total">
                <title>Total data size per region server</title>
                <para>According to above numbers for region size and number of regions per region server, in
                    an optimistic estimate 10 GB x 100 regions per RS will give up to 1TB served per region
                    server, which is in line with some of the reported multi-PB use cases. However, it is
                    important to think about the data vs cache size ratio at the RS level. With 1TB of data
                    per server and 10 GB block cache, only 1% of the data will be cached, which may barely
                    cover all block indices.</para>
            </section>
            <!-- ops.capacity.regions.total -->
        </section>
        <!-- ops.capacity.regions -->
        <section
                xml:id="ops.capacity.config">
            <title>Initial configuration and tuning</title>
            <para>First, see <xref
                    linkend="important_configurations" />. Note that some configurations, more than others,
                depend on specific scenarios. Pay special attention to:</para>
            <itemizedlist>
                <listitem>
                    <para><xref
                            linkend="hbase.regionserver.handler.count" /> - request handler thread count, vital
                        for high-throughput workloads.</para>
                </listitem>
                <listitem>
                    <para><xref
                            linkend="config.wals" /> - the blocking number of WAL files depends on your memstore
                        configuration and should be set accordingly to prevent potential blocking when doing
                        high volume of writes.</para>
                </listitem>
            </itemizedlist>
            <para>Then, there are some considerations when setting up your cluster and tables.</para>
            <section
                    xml:id="ops.capacity.config.compactions">
                <title>Compactions</title>
                <para>Depending on read/write volume and latency requirements, optimal compaction settings
                    may be different. See <xref
                            linkend="compaction" /> for some details.</para>
                <para>When provisioning for large data sizes, however, it's good to keep in mind that
                    compactions can affect write throughput. Thus, for write-intensive workloads, you may opt
                    for less frequent compactions and more store files per regions. Minimum number of files
                    for compactions (<varname>hbase.hstore.compaction.min</varname>) can be set to higher
                    value; <xref
                            linkend="hbase.hstore.blockingStoreFiles" /> should also be increased, as more files
                    might accumulate in such case. You may also consider manually managing compactions: <xref
                            linkend="managed.compactions" /></para>
            </section>
            <!-- ops.capacity.config.compactions -->
            <section
                    xml:id="ops.capacity.config.presplit">
                <title>Pre-splitting the table</title>
                <para>Based on the target number of the regions per RS (see <xref
                        linkend="ops.capacity.regions.count"
                        xrefstyle="template:above" />) and number of RSes, one can pre-split the table at
                    creation time. This would both avoid some costly splitting as the table starts to fill up,
                    and ensure that the table starts out already distributed across many servers.</para>
                <para>If the table is expected to grow large enough to justify that, at least one region per
                    RS should be created. It is not recommended to split immediately into the full target
                    number of regions (e.g. 50 * number of RSes), but a low intermediate value can be chosen.
                    For multiple tables, it is recommended to be conservative with presplitting (e.g.
                    pre-split 1 region per RS at most), especially if you don't know how much each table will
                    grow. If you split too much, you may end up with too many regions, with some tables having
                    too many small regions.</para>
                <para>For pre-splitting howto, see <xref
                        linkend="precreate.regions" />.</para>
            </section>
            <!-- ops.capacity.config.presplit -->
        </section>
        <!-- ops.capacity.config -->
    </section>
    <!-- ops.capacity -->
    <section
            xml:id="table.rename">
        <title>Table Rename</title>
        <para>In versions 0.90.x of hbase and earlier, we had a simple script that would rename the hdfs
            table directory and then do an edit of the hbase:meta table replacing all mentions of the old
            table name with the new. The script was called <command>./bin/rename_table.rb</command>. The
            script was deprecated and removed mostly because it was unmaintained and the operation
            performed by the script was brutal. </para>
        <para> As of hbase 0.94.x, you can use the snapshot facility renaming a table. Here is how you
            would do it using the hbase shell:</para>
        <screen><![CDATA[hbase shell> disable 'tableName'
hbase shell> snapshot 'tableName', 'tableSnapshot'
hbase shell> clone_snapshot 'tableSnapshot', 'newTableName'
hbase shell> delete_snapshot 'tableSnapshot'
hbase shell> drop 'tableName']]></screen>
        <para>or in code it would be as follows:</para>
        <programlisting language="Java">void rename(HBaseAdmin admin, String oldTableName, String newTableName) {
            String snapshotName = randomName();
            admin.disableTable(oldTableName);
            admin.snapshot(snapshotName, oldTableName);
            admin.cloneSnapshot(snapshotName, newTableName);
            admin.deleteSnapshot(snapshotName);
            admin.deleteTable(oldTableName);
            }</programlisting>

    </section>

</chapter>
